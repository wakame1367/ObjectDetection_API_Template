{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dog_face_detector_gpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakamezake/ObjectDetection_API_Template/blob/master/dog_face_detector_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uz_dHPY5p8H",
        "colab_type": "text"
      },
      "source": [
        "See https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-mRrjkd5tpd",
        "colab_type": "code",
        "outputId": "fb8fba2d-0017-4fb0-e4dd-6e5069cd429b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2258
        }
      },
      "source": [
        "!sudo apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install --user Cython\n",
        "!pip install --user contextlib2\n",
        "!pip install --user jupyter\n",
        "!pip install --user matplotlib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-tk is already the newest version (2.7.15~rc1-1).\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-olefile\n",
            "  python-pkg-resources python-six python-webencodings\n",
            "Suggested packages:\n",
            "  python-genshi python-lxml-dbg python-lxml-doc python-pil-doc python-pil-dbg\n",
            "  python-setuptools\n",
            "The following NEW packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-lxml python-olefile\n",
            "  python-pil python-pkg-resources python-six python-webencodings\n",
            "0 upgraded, 9 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,818 kB of archives.\n",
            "After this operation, 7,688 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-bs4 all 4.6.0-1 [67.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-webencodings all 0.5-2 [10.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-html5lib all 0.999999999-1 [83.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-lxml amd64 4.2.1-1ubuntu0.1 [1,075 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-olefile all 0.45.1-1 [33.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pil amd64 5.1.0-1 [328 kB]\n",
            "Fetched 1,818 kB in 3s (696 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.10)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (0.5.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter) (6.0.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.5.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter) (7.4.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.5.1)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (5.2.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (2.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (4.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (4.3.2)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.8.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (4.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (2.10.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (3.1.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter) (3.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter) (0.1.7)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter) (2.5.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter) (4.4.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter) (41.0.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter) (0.6.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "Successfully installed prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc55eL2J53ML",
        "colab_type": "code",
        "outputId": "45442236-f91f-4dfb-abef-d9ebdfbc8fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# COCO API install\n",
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 953, done.\u001b[K\n",
            "remote: Total 953 (delta 0), reused 0 (delta 0), pack-reused 953\u001b[K\n",
            "Receiving objects: 100% (953/953), 11.70 MiB | 1.27 MiB/s, done.\n",
            "Resolving deltas: 100% (565/565), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj93dQ3j6Mxq",
        "colab_type": "code",
        "outputId": "d2bd9b0a-cc0f-4912-980c-9176d4c7092b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd cocoapi/PythonAPI"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cocoapi/PythonAPI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_lfPi2J6Qts",
        "colab_type": "code",
        "outputId": "2ad33d5c-6a4f-48b5-e368-05a54aac929b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1062
        }
      },
      "source": [
        "!make"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.6/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/../common/maskApi.o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -o build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAx2VaTq6SYo",
        "colab_type": "code",
        "outputId": "940760b5-9cb0-4713-93c5-4fbf03a9dcbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "%cd ../../\n",
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 27787 (delta 0), reused 0 (delta 0), pack-reused 27785\u001b[K\n",
            "Receiving objects: 100% (27787/27787), 509.14 MiB | 16.66 MiB/s, done.\n",
            "Resolving deltas: 100% (17036/17036), done.\n",
            "Checking out files: 100% (2998/2998), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ4YSUbg6h90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r cocoapi/PythonAPI/pycocotools ./models/research"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMryQHSg7ktU",
        "colab_type": "code",
        "outputId": "2169740c-8367-43f4-827b-1c548e2b6cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd models/research"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0RdUgVU73du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDKzDtNe8Dtv",
        "colab_type": "code",
        "outputId": "1f710011-d0e2-4f2c-ba91-fa8f0a3408cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%set_env PYTHONPATH=/content/models/research:/content/models/research/slim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/models/research:/content/models/research/slim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9x_zRu78lQ",
        "colab_type": "code",
        "outputId": "ae644b84-bf48-4446-a23d-d832df128587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 15:15:52.568540 140015655364480 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0619 15:15:52.786838 140015655364480 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0619 15:15:52.835028 140015655364480 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 3.6.7: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.263s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XYPax0D896B",
        "colab_type": "code",
        "outputId": "8f962bd0-2dce-4c9e-b58a-acaa0012d715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!curl \"http://download.tensorflow.org/models/object_detection/pet_faces_tfrecord.tar.gz\" | tar xzf -"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  357M  100  357M    0     0  94.3M      0  0:00:03  0:00:03 --:--:-- 94.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EQZAJjV9O5B",
        "colab_type": "code",
        "outputId": "1b6a09be-f86d-4bcd-f05b-2060eeb7a610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz\n",
        "!tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 46 44.4M   46 20.5M    0     0  43.2M      0  0:00:01 --:--:--  0:00:01 43.1M\r100 44.4M  100 44.4M    0     0  82.8M      0 --:--:-- --:--:-- --:--:-- 82.6M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybMX76O39whL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF7Omaz--1ct",
        "colab_type": "code",
        "outputId": "c45eb5d7-2f98-48a9-b1a6-76b27852844f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd object_detection/samples/configs/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection/samples/configs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTl9_CaqBfFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -i \"s|PATH_TO_BE_CONFIGURED|/content/models/research|g\" ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config\n",
        "!sed -i \"s|batch_size: 128|batch_size: 256|g\" ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config\n",
        "!sed -i \"s|/content/models/research/pet_label_map.pbtxt|/content/models/research/object_detection/data/pet_label_map.pbtxt|g\" ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHPVAdPOCjFd",
        "colab_type": "code",
        "outputId": "b552b340-8605-4e29-ba38-50f9eb829c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3444
        }
      },
      "source": [
        "!cat ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSD with Mobilenet v1 0.75 depth multiplied feature extractor, focal loss and\n",
            "# quantized training.\n",
            "# Trained on IIIT-Oxford pets, initialized from COCO detection checkpoint\n",
            "\n",
            "# This config is TPU compatible\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 37\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v1'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 0.75\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.75,\n",
            "          gamma: 2.0\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "          delta: 1.0\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint: \"/content/models/research/model.ckpt\"\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  load_all_detection_checkpoint_vars: true\n",
            "  batch_size: 256\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  num_steps: 2000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 0.2\n",
            "          total_steps: 2000\n",
            "          warmup_steps: 0\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/models/research/pet_faces_train.record-?????-of-00010\"\n",
            "  }\n",
            "  label_map_path: \"/content/models/research/object_detection/data/pet_label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  num_examples: 1100\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/models/research/pet_faces_val.record-?????-of-00010\"\n",
            "  }\n",
            "  label_map_path: \"/content/models/research/object_detection/data/pet_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n",
            "\n",
            "graph_rewriter {\n",
            "  quantization {\n",
            "    delay: 1800\n",
            "    activation_bits: 8\n",
            "    weight_bits: 8\n",
            "  }\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfejZDleU8w_",
        "colab_type": "code",
        "outputId": "e5128eda-1a48-4ec9-980c-e659077a1395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd ../"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POvBXwdFC5NX",
        "colab_type": "code",
        "outputId": "eefa5acc-18f0-4059-e494-b32c60843471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159687
        }
      },
      "source": [
        "!python legacy/train.py --logtostderr  --pipeline_config_path=samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config --train_dir=training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 15:29:33.099700 139649090267008 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0619 15:29:33.235805 139649090267008 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0619 15:29:33.245740 139649090267008 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0619 15:29:33.256233 139649090267008 deprecation_wrapper.py:119] From legacy/train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0619 15:29:33.256397 139649090267008 deprecation_wrapper.py:119] From legacy/train.py:55: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0619 15:29:33.256863 139649090267008 deprecation_wrapper.py:119] From legacy/train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0619 15:29:33.257318 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0619 15:29:33.257437 139649090267008 deprecation_wrapper.py:119] From legacy/train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0619 15:29:33.257807 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:98: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0619 15:29:33.261889 139649090267008 deprecation_wrapper.py:119] From legacy/train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0619 15:29:33.264577 139649090267008 deprecation.py:323] From /content/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0619 15:29:33.268882 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:177: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0619 15:29:33.269080 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:192: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0619 15:29:33.290605 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0619 15:29:33.296168 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0619 15:29:33.296266 139649090267008 dataset_builder.py:72] num_readers has been reduced to 10 to match input file shards.\n",
            "W0619 15:29:33.302103 139649090267008 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0619 15:29:33.302236 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0619 15:29:33.328032 139649090267008 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0619 15:29:33.634072 139649090267008 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0619 15:29:33.641030 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0619 15:29:33.645246 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:512: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0619 15:29:33.694545 139649090267008 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:188: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0619 15:29:33.705234 139649090267008 deprecation.py:323] From /content/models/research/object_detection/core/box_list_ops.py:201: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0619 15:29:34.389097 139649090267008 deprecation.py:323] From /content/models/research/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0619 15:29:34.393155 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0619 15:29:34.394191 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0619 15:29:34.402997 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0619 15:29:35.555402 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2515: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0619 15:29:38.356684 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0619 15:29:38.356913 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 15:29:38.391567 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 15:29:38.425103 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 15:29:38.460105 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 15:29:38.493120 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 15:29:38.526887 139649090267008 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0619 15:29:45.653610 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:172: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0619 15:29:45.654933 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:178: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0619 15:29:45.694485 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "I0619 15:29:52.059851 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
            "I0619 15:29:52.060405 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
            "I0619 15:29:52.060786 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
            "I0619 15:29:52.061166 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
            "I0619 15:29:52.061536 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
            "I0619 15:29:52.061899 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
            "I0619 15:29:52.062246 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
            "I0619 15:29:52.062604 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
            "I0619 15:29:52.062947 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
            "I0619 15:29:52.063335 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
            "I0619 15:29:52.063715 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
            "I0619 15:29:52.064069 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
            "I0619 15:29:52.064384 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
            "I0619 15:29:52.064751 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
            "I0619 15:29:52.065107 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
            "I0619 15:29:52.065476 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
            "I0619 15:29:52.065813 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
            "I0619 15:29:52.066187 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
            "I0619 15:29:52.066522 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
            "I0619 15:29:52.066883 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
            "I0619 15:29:52.067231 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
            "I0619 15:29:52.067595 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
            "I0619 15:29:52.067974 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
            "I0619 15:29:52.068329 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
            "I0619 15:29:52.068681 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
            "I0619 15:29:52.069042 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
            "I0619 15:29:52.069377 139649090267008 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n",
            "W0619 15:29:52.085093 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0619 15:29:52.094416 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "W0619 15:29:52.095748 139649090267008 deprecation.py:323] From /content/models/research/object_detection/legacy/trainer.py:313: SyncReplicasOptimizer.__init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `SyncReplicaOptimizer` class is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\n",
            "I0619 15:29:52.095877 139649090267008 sync_replicas_optimizer.py:188] SyncReplicasV2: replicas_to_aggregate=8; total_num_replicas=1\n",
            "W0619 15:29:59.193535 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0619 15:29:59.387349 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0619 15:29:59.389575 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0619 15:29:59.392874 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0619 15:29:59.398766 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0619 15:29:59.957727 139649090267008 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/variables_helper.py:134: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0619 15:29:59.960198 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/BoxPredictor_0/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.960363 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/BoxPredictor_0/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.960428 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/BoxPredictor_0/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.960494 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/BoxPredictor_0/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.960546 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.960597 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.960661 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.960720 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.960769 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.960816 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.960863 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/BoxPredictor_0/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.960910 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/BoxPredictor_0/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.961003 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/BoxPredictor_0/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.961053 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/BoxPredictor_0/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.961099 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.961169 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.961229 139649090267008 variables_helper.py:149] Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[114]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.961282 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.961342 139649090267008 variables_helper.py:149] Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 384, 273]], model variable shape: [[1, 1, 384, 114]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.961393 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.961442 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.961491 139649090267008 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.961539 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/BoxPredictor_1/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.961587 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/BoxPredictor_1/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.961636 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/BoxPredictor_1/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.961685 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/BoxPredictor_1/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.961733 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.961782 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.961835 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.961891 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.961940 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.962002 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.962051 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/BoxPredictor_1/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.962100 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/BoxPredictor_1/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.962153 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/BoxPredictor_1/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.962202 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/BoxPredictor_1/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.962249 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.962297 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.962352 139649090267008 variables_helper.py:149] Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.962401 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.962460 139649090267008 variables_helper.py:149] Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 768, 546]], model variable shape: [[1, 1, 768, 228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.962509 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.962558 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.962606 139649090267008 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.962655 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/BoxPredictor_2/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.962703 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/BoxPredictor_2/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.962753 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/BoxPredictor_2/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.962801 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/BoxPredictor_2/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.962850 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.962898 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.962951 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.963025 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.963087 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.963138 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.963185 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/BoxPredictor_2/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.963232 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/BoxPredictor_2/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.963279 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/BoxPredictor_2/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.963333 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/BoxPredictor_2/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.963384 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.963433 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.963489 139649090267008 variables_helper.py:149] Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.963538 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.963595 139649090267008 variables_helper.py:149] Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 384, 546]], model variable shape: [[1, 1, 384, 228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.963643 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.963708 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.963756 139649090267008 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.963804 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/BoxPredictor_3/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.963852 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/BoxPredictor_3/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.963899 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/BoxPredictor_3/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.963947 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/BoxPredictor_3/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.964016 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.964066 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.964121 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.964184 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.964233 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.964281 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.964329 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/BoxPredictor_3/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.964377 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/BoxPredictor_3/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.964433 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/BoxPredictor_3/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:29:59.964481 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/BoxPredictor_3/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:29:59.964528 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.964577 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.964638 139649090267008 variables_helper.py:149] Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.964688 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.964746 139649090267008 variables_helper.py:149] Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 192, 546]], model variable shape: [[1, 1, 192, 228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:29:59.964795 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:29:59.964844 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:29:59.964894 139649090267008 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:29:59.964941 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/BoxPredictor_4/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:29:59.965006 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/BoxPredictor_4/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.006812 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/BoxPredictor_4/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.006925 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/BoxPredictor_4/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.007028 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.007112 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.007207 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.007287 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.007370 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.007451 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.007516 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/BoxPredictor_4/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.007601 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/BoxPredictor_4/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.007669 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/BoxPredictor_4/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.007735 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/BoxPredictor_4/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.007801 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.007868 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.007945 139649090267008 variables_helper.py:149] Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:30:00.008037 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.008138 139649090267008 variables_helper.py:149] Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 192, 546]], model variable shape: [[1, 1, 192, 228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:30:00.008214 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.008285 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.008353 139649090267008 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.008420 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/BoxPredictor_5/BoxEncodingPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.008487 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/BoxPredictor_5/BoxEncodingPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.008554 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/BoxPredictor_5/BoxEncodingPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.008620 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/BoxPredictor_5/BoxEncodingPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.008687 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.008751 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.008834 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.008910 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.008993 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.009061 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.009133 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/BoxPredictor_5/ClassPredictor/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.009199 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/BoxPredictor_5/ClassPredictor/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.009264 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/BoxPredictor_5/ClassPredictor/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.009337 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/BoxPredictor_5/ClassPredictor/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.009413 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.009476 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.009549 139649090267008 variables_helper.py:149] Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:30:00.009614 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/biases/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.009694 139649090267008 variables_helper.py:149] Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 96, 546]], model variable shape: [[1, 1, 96, 228]]. This variable will not be initialized from the checkpoint.\n",
            "W0619 15:30:00.009761 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.009826 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.009910 139649090267008 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.010002 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010082 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010180 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010259 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010336 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010426 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010505 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010580 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010665 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010745 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010820 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.010906 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011000 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011080 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011178 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011257 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011334 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011417 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011497 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011572 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011657 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011737 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011811 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011896 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.011990 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.012072 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.012172 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.012253 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.012331 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.012407 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.012479 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.012547 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.012653 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.012736 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.012808 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.012920 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.013001 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.013065 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.013146 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.013223 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.013300 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.013368 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.013434 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.013499 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.013566 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.013630 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.013708 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.013774 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.013839 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.013914 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.014006 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.014088 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.014170 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.014241 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.014308 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.014390 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.014497 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.014587 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.014668 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.014747 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.014835 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.014921 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.015042 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.015140 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.015224 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.015304 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.015385 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.015462 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.015569 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.015637 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.015704 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.015778 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.015854 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.015932 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.016024 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.016095 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.016175 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.016244 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.016310 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.016387 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.016457 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.016524 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.016598 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.016674 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.016755 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.016824 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.016893 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.016980 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.017054 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.017129 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.017210 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.017278 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.017344 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.017420 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.017496 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.017573 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.017645 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.017714 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.017792 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.017857 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.017921 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.018031 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.018102 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.018179 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.018255 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.018332 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.018411 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.018482 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.018549 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.018617 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.018684 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.018749 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.018825 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.018894 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.018976 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.019056 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019142 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019240 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019314 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019388 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019472 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019547 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019620 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019701 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019775 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019847 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.019937 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020030 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020101 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020189 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020262 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020348 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020430 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020505 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020578 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020689 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020764 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020837 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.020919 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021032 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021108 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021204 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021281 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021357 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021439 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021517 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021591 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021675 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021753 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021828 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.021912 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022008 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022088 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022186 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022264 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022340 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022423 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022501 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022586 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022675 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022750 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022822 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022903 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.022997 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023075 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023169 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023246 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023335 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023429 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/Momentum] is not available in checkpoint\n",
            "W0619 15:30:00.023498 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.023579 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.023647 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.023714 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.023781 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.023847 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.023912 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.023994 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.024062 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.024140 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.024211 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.024277 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.024342 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.024409 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.024475 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.024540 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.024606 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.024683 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.024747 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.024811 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.024876 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.024955 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.025044 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.025109 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.025186 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.025252 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.025320 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.025385 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.025450 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.025515 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.025582 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.025649 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.025714 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.025780 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.025846 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.025911 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.025997 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.026077 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.026150 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.026216 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.026280 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.026345 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.026426 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.026492 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.026567 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.026631 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.026695 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.026759 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.026822 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.026886 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.026951 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.027036 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.027101 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.027177 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.027241 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.027306 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.027386 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.027453 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.027519 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.027585 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.027651 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.027717 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.027783 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.027848 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.027914 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.028007 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.028078 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.028156 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.028226 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.028290 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.028357 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.028421 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.028487 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.028553 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.028643 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.028703 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.028782 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.028846 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.028909 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.028989 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.029057 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.029129 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.029197 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.029262 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.029326 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.029389 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.029453 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.029515 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.029594 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.029665 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.029731 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.029797 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.029863 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.029927 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.030013 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.030080 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.030156 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.030224 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.030292 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.030357 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.030422 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.030487 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.030554 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.030619 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.030684 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.030750 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.030817 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.030883 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.030948 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.031036 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.031103 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.031169 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.031223 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.031281 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.031337 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.031389 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.031440 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.031492 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.031546 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.031601 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.031661 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.031723 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.031787 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.031851 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.031918 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.031996 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.032063 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.032137 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.032205 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.032271 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.032337 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.032403 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.032469 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.032534 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.032599 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.032665 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.032740 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.032804 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.032885 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.032952 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.033041 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.033106 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.033183 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.033249 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.033315 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.033381 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.033447 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.033513 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.033579 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.033646 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.033711 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.033775 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.033840 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.033905 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.033990 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.034062 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.034139 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.034209 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.034275 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.034340 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.034469 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.034548 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.034626 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.034706 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.034784 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.034862 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.034938 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.035047 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.035136 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.035219 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.035298 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.035376 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.035455 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.035557 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.035622 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.035689 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.035753 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.035819 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.035886 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.035952 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.036042 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.036110 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.036188 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.036254 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.036319 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.036384 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.036450 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.036515 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.036581 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.036647 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.036713 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.036777 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.036841 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.036906 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.036992 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.037063 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.037141 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.037209 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.037275 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.037339 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.037403 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.037469 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.037536 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.037603 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.037669 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.037736 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.037801 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.037867 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.037932 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max/biased] is not available in checkpoint\n",
            "W0619 15:30:00.038024 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.038094 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min/biased] is not available in checkpoint\n",
            "W0619 15:30:00.038173 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min/local_step] is not available in checkpoint\n",
            "W0619 15:30:00.038241 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.038306 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.038371 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/max] is not available in checkpoint\n",
            "W0619 15:30:00.038436 139649090267008 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/min] is not available in checkpoint\n",
            "W0619 15:30:00.038501 139649090267008 variables_helper.py:152] Variable [fake_quantization_step] is not available in checkpoint\n",
            "W0619 15:30:00.038568 139649090267008 variables_helper.py:152] Variable [global_step] is not available in checkpoint\n",
            "W0619 15:30:00.652907 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-06-19 15:30:02.718769: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-19 15:30:02.720642: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18d1b800 executing computations on platform Host. Devices:\n",
            "2019-06-19 15:30:02.720678: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-19 15:30:02.726241: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-19 15:30:02.980184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:02.980736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18d1bd40 executing computations on platform CUDA. Devices:\n",
            "2019-06-19 15:30:02.980765: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-19 15:30:02.981040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:02.981405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-19 15:30:02.988282: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 15:30:03.151473: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-19 15:30:03.222653: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-19 15:30:03.243287: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-19 15:30:03.425575: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-19 15:30:03.522140: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-19 15:30:03.850510: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-19 15:30:03.850756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:03.851263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:03.851623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-19 15:30:03.855197: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 15:30:03.857553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-19 15:30:03.857585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-19 15:30:03.857596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-19 15:30:03.859788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:03.860248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 15:30:03.860600: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-19 15:30:03.860654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-06-19 15:30:05.941080: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0619 15:30:06.127343 139649090267008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0619 15:30:06.129094 139649090267008 saver.py:1280] Restoring parameters from /content/models/research/model.ckpt\n",
            "I0619 15:30:07.576414 139649090267008 session_manager.py:500] Running local_init_op.\n",
            "I0619 15:30:08.239321 139649090267008 session_manager.py:502] Done running local_init_op.\n",
            "I0619 15:30:20.497265 139649090267008 learning.py:754] Starting Session.\n",
            "I0619 15:30:20.931147 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 15:30:20.935312 139649090267008 learning.py:768] Starting Queues.\n",
            "2019-06-19 15:30:31.877065: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "I0619 15:30:34.771788 139646026082048 supervisor.py:1099] global_step/sec: 0\n",
            "2019-06-19 15:30:51.870709: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:31:02.093017 139646017689344 supervisor.py:1050] Recording summary at step 0.\n",
            "I0619 15:31:06.713706 139649090267008 learning.py:507] global step 0: loss = 3.8076 (32.196 sec/step)\n",
            "I0619 15:31:10.086900 139649090267008 learning.py:507] global step 0: loss = 3.7956 (2.907 sec/step)\n",
            "I0619 15:31:13.043754 139649090267008 learning.py:507] global step 0: loss = 3.7668 (2.955 sec/step)\n",
            "I0619 15:31:16.040792 139649090267008 learning.py:507] global step 0: loss = 3.7691 (2.995 sec/step)\n",
            "I0619 15:31:19.024146 139649090267008 learning.py:507] global step 0: loss = 3.7823 (2.981 sec/step)\n",
            "I0619 15:31:21.933725 139649090267008 learning.py:507] global step 0: loss = 3.7977 (2.908 sec/step)\n",
            "I0619 15:31:24.904146 139649090267008 learning.py:507] global step 0: loss = 3.7868 (2.969 sec/step)\n",
            "I0619 15:31:27.835682 139649090267008 learning.py:507] global step 1: loss = 3.8152 (2.928 sec/step)\n",
            "I0619 15:31:30.734680 139649090267008 learning.py:507] global step 1: loss = 3.6537 (2.897 sec/step)\n",
            "I0619 15:31:33.611352 139649090267008 learning.py:507] global step 1: loss = 3.6450 (2.875 sec/step)\n",
            "I0619 15:31:36.557509 139649090267008 learning.py:507] global step 1: loss = 3.7137 (2.945 sec/step)\n",
            "I0619 15:31:39.457621 139649090267008 learning.py:507] global step 1: loss = 3.6654 (2.898 sec/step)\n",
            "I0619 15:31:42.402887 139649090267008 learning.py:507] global step 1: loss = 3.6145 (2.944 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:31:45.320719 139649090267008 learning.py:507] global step 1: loss = 3.6352 (2.916 sec/step)\n",
            "I0619 15:31:48.268888 139649090267008 learning.py:507] global step 1: loss = 3.6948 (2.946 sec/step)\n",
            "I0619 15:31:51.181990 139649090267008 learning.py:507] global step 1: loss = 3.6442 (2.911 sec/step)\n",
            "2019-06-19 15:31:51.201877: W tensorflow/core/framework/allocator.cc:107] Allocation of 737280000 exceeds 10% of system memory.\n",
            "I0619 15:31:54.606385 139649090267008 learning.py:507] global step 2: loss = 3.6907 (3.422 sec/step)\n",
            "2019-06-19 15:31:54.653099: W tensorflow/core/framework/allocator.cc:107] Allocation of 858218496 exceeds 10% of system memory.\n",
            "I0619 15:31:58.095214 139649090267008 learning.py:507] global step 2: loss = 3.4222 (3.487 sec/step)\n",
            "I0619 15:32:01.045166 139649090267008 learning.py:507] global step 2: loss = 3.5221 (2.948 sec/step)\n",
            "I0619 15:32:03.958353 139649090267008 learning.py:507] global step 2: loss = 3.4471 (2.911 sec/step)\n",
            "I0619 15:32:06.860532 139649090267008 learning.py:507] global step 2: loss = 3.4717 (2.900 sec/step)\n",
            "I0619 15:32:09.905581 139649090267008 learning.py:507] global step 2: loss = 3.4636 (3.043 sec/step)\n",
            "I0619 15:32:12.950525 139649090267008 learning.py:507] global step 2: loss = 3.4004 (3.043 sec/step)\n",
            "I0619 15:32:16.096428 139649090267008 learning.py:507] global step 2: loss = 3.5369 (3.144 sec/step)\n",
            "I0619 15:32:19.087929 139649090267008 learning.py:507] global step 3: loss = 3.5216 (2.989 sec/step)\n",
            "I0619 15:32:23.416800 139649090267008 learning.py:507] global step 3: loss = 3.2037 (4.315 sec/step)\n",
            "I0619 15:32:26.072269 139646017689344 supervisor.py:1050] Recording summary at step 3.\n",
            "I0619 15:32:27.123983 139649090267008 learning.py:507] global step 3: loss = 3.2322 (3.690 sec/step)\n",
            "I0619 15:32:29.127182 139646026082048 supervisor.py:1099] global_step/sec: 0.026234\n",
            "I0619 15:32:30.163912 139649090267008 learning.py:507] global step 3: loss = 3.2214 (3.038 sec/step)\n",
            "2019-06-19 15:32:30.204666: W tensorflow/core/framework/allocator.cc:107] Allocation of 1059840000 exceeds 10% of system memory.\n",
            "I0619 15:32:33.311467 139649090267008 learning.py:507] global step 3: loss = 3.3186 (3.146 sec/step)\n",
            "I0619 15:32:36.261946 139649090267008 learning.py:507] global step 3: loss = 3.2357 (2.949 sec/step)\n",
            "I0619 15:32:39.238283 139649090267008 learning.py:507] global step 3: loss = 3.2771 (2.974 sec/step)\n",
            "I0619 15:32:42.246685 139649090267008 learning.py:507] global step 3: loss = 3.2628 (3.006 sec/step)\n",
            "I0619 15:32:45.182098 139649090267008 learning.py:507] global step 4: loss = 3.2368 (2.933 sec/step)\n",
            "I0619 15:32:48.160091 139649090267008 learning.py:507] global step 4: loss = 3.0053 (2.976 sec/step)\n",
            "I0619 15:32:51.286127 139649090267008 learning.py:507] global step 4: loss = 3.0068 (3.124 sec/step)\n",
            "I0619 15:32:54.213670 139649090267008 learning.py:507] global step 4: loss = 2.7310 (2.926 sec/step)\n",
            "I0619 15:32:57.184526 139649090267008 learning.py:507] global step 4: loss = 2.7623 (2.969 sec/step)\n",
            "I0619 15:33:00.135337 139649090267008 learning.py:507] global step 4: loss = 2.8345 (2.949 sec/step)\n",
            "I0619 15:33:03.177002 139649090267008 learning.py:507] global step 4: loss = 2.7139 (3.040 sec/step)\n",
            "I0619 15:33:06.129144 139649090267008 learning.py:507] global step 4: loss = 2.7969 (2.950 sec/step)\n",
            "I0619 15:33:09.175667 139649090267008 learning.py:507] global step 5: loss = 2.8497 (3.045 sec/step)\n",
            "I0619 15:33:12.181615 139649090267008 learning.py:507] global step 5: loss = 3.1691 (3.004 sec/step)\n",
            "I0619 15:33:15.086565 139649090267008 learning.py:507] global step 5: loss = 2.9755 (2.903 sec/step)\n",
            "I0619 15:33:17.927034 139649090267008 learning.py:507] global step 5: loss = 3.1408 (2.839 sec/step)\n",
            "I0619 15:33:20.797223 139649090267008 learning.py:507] global step 5: loss = 3.2674 (2.868 sec/step)\n",
            "I0619 15:33:23.788158 139649090267008 learning.py:507] global step 5: loss = 3.0905 (2.989 sec/step)\n",
            "I0619 15:33:26.681305 139649090267008 learning.py:507] global step 5: loss = 3.0734 (2.892 sec/step)\n",
            "I0619 15:33:29.534899 139649090267008 learning.py:507] global step 5: loss = 3.2150 (2.852 sec/step)\n",
            "I0619 15:33:32.493605 139649090267008 learning.py:507] global step 6: loss = 3.2855 (2.956 sec/step)\n",
            "I0619 15:33:35.379817 139649090267008 learning.py:507] global step 6: loss = 3.0776 (2.884 sec/step)\n",
            "I0619 15:33:38.275279 139649090267008 learning.py:507] global step 6: loss = 2.9923 (2.894 sec/step)\n",
            "I0619 15:33:41.306816 139649090267008 learning.py:507] global step 6: loss = 3.0262 (3.030 sec/step)\n",
            "I0619 15:33:44.326557 139649090267008 learning.py:507] global step 6: loss = 3.1533 (3.018 sec/step)\n",
            "I0619 15:33:47.205793 139649090267008 learning.py:507] global step 6: loss = 3.0416 (2.877 sec/step)\n",
            "I0619 15:33:50.152299 139649090267008 learning.py:507] global step 6: loss = 3.0455 (2.945 sec/step)\n",
            "I0619 15:33:53.057545 139649090267008 learning.py:507] global step 6: loss = 2.9519 (2.904 sec/step)\n",
            "I0619 15:33:55.966788 139649090267008 learning.py:507] global step 7: loss = 3.0676 (2.907 sec/step)\n",
            "I0619 15:33:58.948206 139649090267008 learning.py:507] global step 7: loss = 2.8263 (2.979 sec/step)\n",
            "I0619 15:34:01.914388 139649090267008 learning.py:507] global step 7: loss = 2.6811 (2.964 sec/step)\n",
            "2019-06-19 15:34:01.958298: W tensorflow/core/framework/allocator.cc:107] Allocation of 1725239040 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1725243392 bytes == 0x7efe3a118000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:34:05.862899 139649090267008 learning.py:507] global step 7: loss = 2.8350 (3.947 sec/step)\n",
            "I0619 15:34:08.947901 139649090267008 learning.py:507] global step 7: loss = 2.8501 (3.083 sec/step)\n",
            "I0619 15:34:11.888102 139649090267008 learning.py:507] global step 7: loss = 2.8236 (2.938 sec/step)\n",
            "I0619 15:34:14.856668 139649090267008 learning.py:507] global step 7: loss = 2.7532 (2.967 sec/step)\n",
            "I0619 15:34:17.745075 139649090267008 learning.py:507] global step 7: loss = 2.7478 (2.886 sec/step)\n",
            "I0619 15:34:20.678677 139649090267008 learning.py:507] global step 8: loss = 2.8076 (2.932 sec/step)\n",
            "I0619 15:34:26.283392 139649090267008 learning.py:507] global step 8: loss = 2.6805 (5.598 sec/step)\n",
            "I0619 15:34:26.563079 139646017689344 supervisor.py:1050] Recording summary at step 8.\n",
            "I0619 15:34:29.127137 139646026082048 supervisor.py:1099] global_step/sec: 0.0416667\n",
            "I0619 15:34:29.315631 139649090267008 learning.py:507] global step 8: loss = 2.6885 (3.025 sec/step)\n",
            "I0619 15:34:32.253251 139649090267008 learning.py:507] global step 8: loss = 2.7023 (2.936 sec/step)\n",
            "I0619 15:34:35.194826 139649090267008 learning.py:507] global step 8: loss = 2.7295 (2.940 sec/step)\n",
            "I0619 15:34:38.129729 139649090267008 learning.py:507] global step 8: loss = 2.6144 (2.932 sec/step)\n",
            "2019-06-19 15:34:38.169184: W tensorflow/core/framework/allocator.cc:107] Allocation of 1887327744 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1887330304 bytes == 0x7efe3a118000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:34:41.304793 139649090267008 learning.py:507] global step 8: loss = 2.8646 (3.173 sec/step)\n",
            "tcmalloc: large alloc 2791251968 bytes == 0x7efd93b26000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:34:46.114776 139649090267008 learning.py:507] global step 8: loss = 2.5319 (4.808 sec/step)\n",
            "I0619 15:34:49.156301 139649090267008 learning.py:507] global step 9: loss = 2.6367 (3.039 sec/step)\n",
            "I0619 15:34:52.078737 139649090267008 learning.py:507] global step 9: loss = 2.4262 (2.921 sec/step)\n",
            "I0619 15:34:55.055833 139649090267008 learning.py:507] global step 9: loss = 2.6908 (2.975 sec/step)\n",
            "I0619 15:34:58.223453 139649090267008 learning.py:507] global step 9: loss = 2.5751 (3.166 sec/step)\n",
            "I0619 15:35:01.841729 139649090267008 learning.py:507] global step 9: loss = 2.6091 (3.617 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:35:05.591273 139649090267008 learning.py:507] global step 9: loss = 2.5340 (3.748 sec/step)\n",
            "I0619 15:35:08.524095 139649090267008 learning.py:507] global step 9: loss = 2.6180 (2.931 sec/step)\n",
            "I0619 15:35:11.626264 139649090267008 learning.py:507] global step 9: loss = 2.9932 (3.100 sec/step)\n",
            "I0619 15:35:14.567193 139649090267008 learning.py:507] global step 10: loss = 2.7547 (2.938 sec/step)\n",
            "I0619 15:35:17.629787 139649090267008 learning.py:507] global step 10: loss = 2.4369 (3.061 sec/step)\n",
            "I0619 15:35:20.530751 139649090267008 learning.py:507] global step 10: loss = 2.7328 (2.899 sec/step)\n",
            "I0619 15:35:23.607176 139649090267008 learning.py:507] global step 10: loss = 2.4320 (3.074 sec/step)\n",
            "I0619 15:35:26.631046 139649090267008 learning.py:507] global step 10: loss = 2.3495 (3.022 sec/step)\n",
            "I0619 15:35:29.734355 139649090267008 learning.py:507] global step 10: loss = 2.4108 (3.102 sec/step)\n",
            "I0619 15:35:32.843213 139649090267008 learning.py:507] global step 10: loss = 2.4907 (3.107 sec/step)\n",
            "I0619 15:35:35.875801 139649090267008 learning.py:507] global step 10: loss = 2.4456 (3.031 sec/step)\n",
            "I0619 15:35:39.180855 139649090267008 learning.py:507] global step 11: loss = 2.5865 (3.303 sec/step)\n",
            "I0619 15:35:42.155070 139649090267008 learning.py:507] global step 11: loss = 2.0105 (2.972 sec/step)\n",
            "I0619 15:35:45.151283 139649090267008 learning.py:507] global step 11: loss = 2.2352 (2.994 sec/step)\n",
            "I0619 15:35:48.193804 139649090267008 learning.py:507] global step 11: loss = 2.3755 (3.041 sec/step)\n",
            "I0619 15:35:51.449851 139649090267008 learning.py:507] global step 11: loss = 2.2929 (3.254 sec/step)\n",
            "I0619 15:35:54.382909 139649090267008 learning.py:507] global step 11: loss = 2.1686 (2.931 sec/step)\n",
            "I0619 15:35:57.444124 139649090267008 learning.py:507] global step 11: loss = 2.2403 (3.060 sec/step)\n",
            "I0619 15:36:00.460386 139649090267008 learning.py:507] global step 11: loss = 2.3343 (3.015 sec/step)\n",
            "I0619 15:36:03.580258 139649090267008 learning.py:507] global step 12: loss = 2.3352 (3.117 sec/step)\n",
            "I0619 15:36:06.549863 139649090267008 learning.py:507] global step 12: loss = 2.2083 (2.968 sec/step)\n",
            "I0619 15:36:09.589834 139649090267008 learning.py:507] global step 12: loss = 2.1400 (3.038 sec/step)\n",
            "I0619 15:36:12.550911 139649090267008 learning.py:507] global step 12: loss = 2.3040 (2.959 sec/step)\n",
            "I0619 15:36:15.563285 139649090267008 learning.py:507] global step 12: loss = 2.0129 (3.011 sec/step)\n",
            "I0619 15:36:18.543298 139649090267008 learning.py:507] global step 12: loss = 2.2188 (2.978 sec/step)\n",
            "I0619 15:36:22.149637 139649090267008 learning.py:507] global step 12: loss = 2.0265 (3.600 sec/step)\n",
            "I0619 15:36:26.079168 139646017689344 supervisor.py:1050] Recording summary at step 12.\n",
            "I0619 15:36:26.690408 139649090267008 learning.py:507] global step 12: loss = 2.2803 (4.512 sec/step)\n",
            "I0619 15:36:29.127337 139646026082048 supervisor.py:1099] global_step/sec: 0.0333333\n",
            "I0619 15:36:29.703443 139649090267008 learning.py:507] global step 13: loss = 2.1859 (3.011 sec/step)\n",
            "I0619 15:36:32.729877 139649090267008 learning.py:507] global step 13: loss = 2.0665 (3.025 sec/step)\n",
            "I0619 15:36:35.727656 139649090267008 learning.py:507] global step 13: loss = 2.1032 (2.996 sec/step)\n",
            "I0619 15:36:38.773246 139649090267008 learning.py:507] global step 13: loss = 2.0414 (3.044 sec/step)\n",
            "I0619 15:36:41.767922 139649090267008 learning.py:507] global step 13: loss = 2.1824 (2.993 sec/step)\n",
            "I0619 15:36:44.814486 139649090267008 learning.py:507] global step 13: loss = 2.2363 (3.045 sec/step)\n",
            "I0619 15:36:47.806771 139649090267008 learning.py:507] global step 13: loss = 1.9567 (2.990 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:36:50.820590 139649090267008 learning.py:507] global step 13: loss = 2.3182 (3.012 sec/step)\n",
            "I0619 15:36:53.764087 139649090267008 learning.py:507] global step 14: loss = 1.9789 (2.941 sec/step)\n",
            "I0619 15:36:56.781062 139649090267008 learning.py:507] global step 14: loss = 1.9702 (3.015 sec/step)\n",
            "I0619 15:36:59.802267 139649090267008 learning.py:507] global step 14: loss = 1.8125 (3.019 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:37:02.843976 139649090267008 learning.py:507] global step 14: loss = 1.8132 (3.039 sec/step)\n",
            "I0619 15:37:05.830106 139649090267008 learning.py:507] global step 14: loss = 1.9098 (2.984 sec/step)\n",
            "I0619 15:37:08.842017 139649090267008 learning.py:507] global step 14: loss = 1.9717 (3.010 sec/step)\n",
            "I0619 15:37:11.911623 139649090267008 learning.py:507] global step 14: loss = 1.9480 (3.068 sec/step)\n",
            "I0619 15:37:14.888205 139649090267008 learning.py:507] global step 14: loss = 2.0202 (2.975 sec/step)\n",
            "I0619 15:37:17.925852 139649090267008 learning.py:507] global step 15: loss = 1.7872 (3.035 sec/step)\n",
            "I0619 15:37:20.884453 139649090267008 learning.py:507] global step 15: loss = 2.3329 (2.957 sec/step)\n",
            "I0619 15:37:23.841419 139649090267008 learning.py:507] global step 15: loss = 2.3664 (2.955 sec/step)\n",
            "I0619 15:37:26.835912 139649090267008 learning.py:507] global step 15: loss = 2.0241 (2.993 sec/step)\n",
            "I0619 15:37:29.872896 139649090267008 learning.py:507] global step 15: loss = 1.8431 (3.035 sec/step)\n",
            "I0619 15:37:32.906796 139649090267008 learning.py:507] global step 15: loss = 2.0070 (3.032 sec/step)\n",
            "I0619 15:37:35.956071 139649090267008 learning.py:507] global step 15: loss = 1.9103 (3.048 sec/step)\n",
            "I0619 15:37:38.997335 139649090267008 learning.py:507] global step 15: loss = 2.1642 (3.039 sec/step)\n",
            "I0619 15:37:42.035571 139649090267008 learning.py:507] global step 16: loss = 2.1173 (3.036 sec/step)\n",
            "I0619 15:37:44.979628 139649090267008 learning.py:507] global step 16: loss = 1.8667 (2.942 sec/step)\n",
            "I0619 15:37:48.020955 139649090267008 learning.py:507] global step 16: loss = 1.8800 (3.039 sec/step)\n",
            "I0619 15:37:51.199218 139649090267008 learning.py:507] global step 16: loss = 2.0262 (3.176 sec/step)\n",
            "I0619 15:37:54.183107 139649090267008 learning.py:507] global step 16: loss = 1.6997 (2.982 sec/step)\n",
            "I0619 15:37:57.235601 139649090267008 learning.py:507] global step 16: loss = 2.0761 (3.051 sec/step)\n",
            "I0619 15:38:00.277360 139649090267008 learning.py:507] global step 16: loss = 1.9571 (3.040 sec/step)\n",
            "I0619 15:38:03.378423 139649090267008 learning.py:507] global step 16: loss = 1.8935 (3.099 sec/step)\n",
            "I0619 15:38:06.486053 139649090267008 learning.py:507] global step 17: loss = 1.5646 (3.105 sec/step)\n",
            "I0619 15:38:09.749722 139649090267008 learning.py:507] global step 17: loss = 1.9067 (3.262 sec/step)\n",
            "I0619 15:38:12.834775 139649090267008 learning.py:507] global step 17: loss = 1.7177 (3.083 sec/step)\n",
            "I0619 15:38:15.963743 139649090267008 learning.py:507] global step 17: loss = 1.6504 (3.127 sec/step)\n",
            "I0619 15:38:19.007893 139649090267008 learning.py:507] global step 17: loss = 1.7836 (3.042 sec/step)\n",
            "I0619 15:38:23.582148 139649090267008 learning.py:507] global step 17: loss = 1.6766 (4.571 sec/step)\n",
            "I0619 15:38:25.985926 139646017689344 supervisor.py:1050] Recording summary at step 17.\n",
            "I0619 15:38:27.303700 139649090267008 learning.py:507] global step 17: loss = 1.6162 (3.708 sec/step)\n",
            "I0619 15:38:29.127213 139646026082048 supervisor.py:1099] global_step/sec: 0.0416667\n",
            "I0619 15:38:30.358064 139649090267008 learning.py:507] global step 17: loss = 1.7895 (3.052 sec/step)\n",
            "I0619 15:38:33.399425 139649090267008 learning.py:507] global step 18: loss = 1.7422 (3.039 sec/step)\n",
            "I0619 15:38:36.592686 139649090267008 learning.py:507] global step 18: loss = 1.7221 (3.191 sec/step)\n",
            "I0619 15:38:39.580982 139649090267008 learning.py:507] global step 18: loss = 1.8303 (2.987 sec/step)\n",
            "I0619 15:38:42.581199 139649090267008 learning.py:507] global step 18: loss = 1.7617 (2.999 sec/step)\n",
            "I0619 15:38:45.626726 139649090267008 learning.py:507] global step 18: loss = 1.5604 (3.044 sec/step)\n",
            "I0619 15:38:48.663943 139649090267008 learning.py:507] global step 18: loss = 1.7594 (3.035 sec/step)\n",
            "I0619 15:38:51.840614 139649090267008 learning.py:507] global step 18: loss = 1.5463 (3.175 sec/step)\n",
            "I0619 15:38:54.851125 139649090267008 learning.py:507] global step 18: loss = 1.6601 (3.009 sec/step)\n",
            "I0619 15:38:57.999840 139649090267008 learning.py:507] global step 19: loss = 1.7020 (3.146 sec/step)\n",
            "I0619 15:39:00.979774 139649090267008 learning.py:507] global step 19: loss = 1.5702 (2.978 sec/step)\n",
            "I0619 15:39:04.119092 139649090267008 learning.py:507] global step 19: loss = 1.4560 (3.137 sec/step)\n",
            "I0619 15:39:07.330621 139649090267008 learning.py:507] global step 19: loss = 1.5642 (3.210 sec/step)\n",
            "I0619 15:39:10.472892 139649090267008 learning.py:507] global step 19: loss = 1.6172 (3.140 sec/step)\n",
            "I0619 15:39:13.559003 139649090267008 learning.py:507] global step 19: loss = 1.7478 (3.084 sec/step)\n",
            "I0619 15:39:16.780386 139649090267008 learning.py:507] global step 19: loss = 1.5355 (3.220 sec/step)\n",
            "I0619 15:39:19.846417 139649090267008 learning.py:507] global step 19: loss = 1.5452 (3.064 sec/step)\n",
            "I0619 15:39:22.968810 139649090267008 learning.py:507] global step 20: loss = 1.5089 (3.120 sec/step)\n",
            "I0619 15:39:26.227058 139649090267008 learning.py:507] global step 20: loss = 1.4895 (3.256 sec/step)\n",
            "I0619 15:39:29.288051 139649090267008 learning.py:507] global step 20: loss = 1.6434 (3.059 sec/step)\n",
            "I0619 15:39:32.283131 139649090267008 learning.py:507] global step 20: loss = 1.4438 (2.993 sec/step)\n",
            "I0619 15:39:35.304589 139649090267008 learning.py:507] global step 20: loss = 1.6906 (3.020 sec/step)\n",
            "I0619 15:39:38.421882 139649090267008 learning.py:507] global step 20: loss = 1.4417 (3.115 sec/step)\n",
            "I0619 15:39:41.472412 139649090267008 learning.py:507] global step 20: loss = 1.3387 (3.049 sec/step)\n",
            "I0619 15:39:44.537751 139649090267008 learning.py:507] global step 20: loss = 1.3112 (3.064 sec/step)\n",
            "I0619 15:39:47.590135 139649090267008 learning.py:507] global step 21: loss = 1.5148 (3.050 sec/step)\n",
            "I0619 15:39:50.542464 139649090267008 learning.py:507] global step 21: loss = 1.5982 (2.951 sec/step)\n",
            "I0619 15:39:53.570271 139649090267008 learning.py:507] global step 21: loss = 1.4274 (3.026 sec/step)\n",
            "I0619 15:39:56.638483 139649090267008 learning.py:507] global step 21: loss = 1.6920 (3.066 sec/step)\n",
            "I0619 15:39:59.746807 139649090267008 learning.py:507] global step 21: loss = 1.5428 (3.106 sec/step)\n",
            "I0619 15:40:02.757863 139649090267008 learning.py:507] global step 21: loss = 1.5295 (3.009 sec/step)\n",
            "I0619 15:40:05.779871 139649090267008 learning.py:507] global step 21: loss = 1.5183 (3.020 sec/step)\n",
            "I0619 15:40:08.969932 139649090267008 learning.py:507] global step 21: loss = 1.4950 (3.188 sec/step)\n",
            "I0619 15:40:11.963625 139649090267008 learning.py:507] global step 22: loss = 1.5933 (2.992 sec/step)\n",
            "I0619 15:40:15.007827 139649090267008 learning.py:507] global step 22: loss = 1.4327 (3.042 sec/step)\n",
            "I0619 15:40:18.007438 139649090267008 learning.py:507] global step 22: loss = 1.3884 (2.997 sec/step)\n",
            "I0619 15:40:20.931288 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 15:40:21.047116 139649090267008 learning.py:507] global step 22: loss = 1.4113 (3.033 sec/step)\n",
            "I0619 15:40:27.504056 139646017689344 supervisor.py:1050] Recording summary at step 22.\n",
            "I0619 15:40:28.214992 139649090267008 learning.py:507] global step 22: loss = 1.3911 (7.104 sec/step)\n",
            "I0619 15:40:29.158199 139646026082048 supervisor.py:1099] global_step/sec: 0.0416559\n",
            "I0619 15:40:31.272353 139649090267008 learning.py:507] global step 22: loss = 1.3242 (3.056 sec/step)\n",
            "I0619 15:40:34.274713 139649090267008 learning.py:507] global step 22: loss = 1.4248 (3.001 sec/step)\n",
            "I0619 15:40:37.352292 139649090267008 learning.py:507] global step 22: loss = 1.3423 (3.076 sec/step)\n",
            "I0619 15:40:40.508615 139649090267008 learning.py:507] global step 23: loss = 1.4950 (3.154 sec/step)\n",
            "I0619 15:40:43.611321 139649090267008 learning.py:507] global step 23: loss = 1.4842 (3.101 sec/step)\n",
            "I0619 15:40:46.849310 139649090267008 learning.py:507] global step 23: loss = 1.2700 (3.236 sec/step)\n",
            "I0619 15:40:50.034830 139649090267008 learning.py:507] global step 23: loss = 1.4915 (3.183 sec/step)\n",
            "I0619 15:40:53.096254 139649090267008 learning.py:507] global step 23: loss = 1.4719 (3.059 sec/step)\n",
            "I0619 15:40:56.162362 139649090267008 learning.py:507] global step 23: loss = 1.4245 (3.064 sec/step)\n",
            "I0619 15:40:59.241466 139649090267008 learning.py:507] global step 23: loss = 1.6106 (3.077 sec/step)\n",
            "I0619 15:41:02.270433 139649090267008 learning.py:507] global step 23: loss = 1.4200 (3.027 sec/step)\n",
            "I0619 15:41:05.301787 139649090267008 learning.py:507] global step 24: loss = 1.3886 (3.029 sec/step)\n",
            "I0619 15:41:08.345080 139649090267008 learning.py:507] global step 24: loss = 1.4044 (3.041 sec/step)\n",
            "I0619 15:41:11.485894 139649090267008 learning.py:507] global step 24: loss = 1.4887 (3.139 sec/step)\n",
            "I0619 15:41:14.552998 139649090267008 learning.py:507] global step 24: loss = 1.4075 (3.065 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:41:17.651137 139649090267008 learning.py:507] global step 24: loss = 1.1900 (3.096 sec/step)\n",
            "I0619 15:41:20.649475 139649090267008 learning.py:507] global step 24: loss = 1.2222 (2.996 sec/step)\n",
            "I0619 15:41:23.708588 139649090267008 learning.py:507] global step 24: loss = 1.4182 (3.057 sec/step)\n",
            "I0619 15:41:26.755869 139649090267008 learning.py:507] global step 24: loss = 1.4555 (3.046 sec/step)\n",
            "I0619 15:41:29.810114 139649090267008 learning.py:507] global step 25: loss = 1.3905 (3.052 sec/step)\n",
            "I0619 15:41:32.887472 139649090267008 learning.py:507] global step 25: loss = 1.3832 (3.076 sec/step)\n",
            "I0619 15:41:35.932494 139649090267008 learning.py:507] global step 25: loss = 1.2048 (3.043 sec/step)\n",
            "I0619 15:41:39.039782 139649090267008 learning.py:507] global step 25: loss = 1.2026 (3.106 sec/step)\n",
            "I0619 15:41:42.172373 139649090267008 learning.py:507] global step 25: loss = 1.4350 (3.131 sec/step)\n",
            "I0619 15:41:45.163594 139649090267008 learning.py:507] global step 25: loss = 1.2415 (2.989 sec/step)\n",
            "I0619 15:41:48.222878 139649090267008 learning.py:507] global step 25: loss = 1.3066 (3.057 sec/step)\n",
            "I0619 15:41:51.239247 139649090267008 learning.py:507] global step 25: loss = 1.3970 (3.014 sec/step)\n",
            "I0619 15:41:54.465350 139649090267008 learning.py:507] global step 26: loss = 1.3720 (3.224 sec/step)\n",
            "I0619 15:41:57.507459 139649090267008 learning.py:507] global step 26: loss = 1.1525 (3.040 sec/step)\n",
            "I0619 15:42:00.485956 139649090267008 learning.py:507] global step 26: loss = 1.3213 (2.977 sec/step)\n",
            "I0619 15:42:03.566748 139649090267008 learning.py:507] global step 26: loss = 1.3465 (3.079 sec/step)\n",
            "I0619 15:42:06.650166 139649090267008 learning.py:507] global step 26: loss = 1.3680 (3.081 sec/step)\n",
            "I0619 15:42:09.670763 139649090267008 learning.py:507] global step 26: loss = 1.2424 (3.019 sec/step)\n",
            "I0619 15:42:13.000629 139649090267008 learning.py:507] global step 26: loss = 1.3520 (3.328 sec/step)\n",
            "I0619 15:42:16.011196 139649090267008 learning.py:507] global step 26: loss = 1.4480 (3.009 sec/step)\n",
            "I0619 15:42:19.105991 139649090267008 learning.py:507] global step 27: loss = 1.3141 (3.092 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:42:23.799252 139649090267008 learning.py:507] global step 27: loss = 1.2357 (4.685 sec/step)\n",
            "I0619 15:42:26.241475 139646017689344 supervisor.py:1050] Recording summary at step 27.\n",
            "I0619 15:42:27.524315 139649090267008 learning.py:507] global step 27: loss = 1.1869 (3.723 sec/step)\n",
            "I0619 15:42:29.127243 139646026082048 supervisor.py:1099] global_step/sec: 0.0416774\n",
            "I0619 15:42:30.597224 139649090267008 learning.py:507] global step 27: loss = 1.2030 (3.071 sec/step)\n",
            "I0619 15:42:33.619349 139649090267008 learning.py:507] global step 27: loss = 1.0712 (3.020 sec/step)\n",
            "I0619 15:42:36.687020 139649090267008 learning.py:507] global step 27: loss = 1.3303 (3.066 sec/step)\n",
            "I0619 15:42:39.750195 139649090267008 learning.py:507] global step 27: loss = 1.2367 (3.061 sec/step)\n",
            "I0619 15:42:42.858982 139649090267008 learning.py:507] global step 27: loss = 1.3158 (3.107 sec/step)\n",
            "I0619 15:42:45.952054 139649090267008 learning.py:507] global step 28: loss = 1.2047 (3.091 sec/step)\n",
            "I0619 15:42:49.083114 139649090267008 learning.py:507] global step 28: loss = 1.2938 (3.129 sec/step)\n",
            "I0619 15:42:52.161522 139649090267008 learning.py:507] global step 28: loss = 1.4281 (3.077 sec/step)\n",
            "I0619 15:42:55.214159 139649090267008 learning.py:507] global step 28: loss = 1.3045 (3.051 sec/step)\n",
            "I0619 15:42:58.272187 139649090267008 learning.py:507] global step 28: loss = 1.2443 (3.056 sec/step)\n",
            "I0619 15:43:01.385407 139649090267008 learning.py:507] global step 28: loss = 1.3021 (3.111 sec/step)\n",
            "I0619 15:43:04.533487 139649090267008 learning.py:507] global step 28: loss = 1.2803 (3.146 sec/step)\n",
            "I0619 15:43:07.733334 139649090267008 learning.py:507] global step 28: loss = 1.1407 (3.198 sec/step)\n",
            "I0619 15:43:10.847348 139649090267008 learning.py:507] global step 29: loss = 1.2184 (3.112 sec/step)\n",
            "I0619 15:43:14.020710 139649090267008 learning.py:507] global step 29: loss = 1.2638 (3.171 sec/step)\n",
            "I0619 15:43:17.173731 139649090267008 learning.py:507] global step 29: loss = 1.2379 (3.151 sec/step)\n",
            "I0619 15:43:20.368178 139649090267008 learning.py:507] global step 29: loss = 1.1502 (3.192 sec/step)\n",
            "I0619 15:43:23.553324 139649090267008 learning.py:507] global step 29: loss = 1.2483 (3.183 sec/step)\n",
            "I0619 15:43:26.909075 139649090267008 learning.py:507] global step 29: loss = 1.1794 (3.354 sec/step)\n",
            "I0619 15:43:30.103447 139649090267008 learning.py:507] global step 29: loss = 1.1885 (3.193 sec/step)\n",
            "I0619 15:43:33.243283 139649090267008 learning.py:507] global step 29: loss = 1.2827 (3.138 sec/step)\n",
            "I0619 15:43:36.371128 139649090267008 learning.py:507] global step 30: loss = 1.1388 (3.125 sec/step)\n",
            "I0619 15:43:39.471757 139649090267008 learning.py:507] global step 30: loss = 1.2764 (3.099 sec/step)\n",
            "I0619 15:43:42.575821 139649090267008 learning.py:507] global step 30: loss = 1.2602 (3.102 sec/step)\n",
            "I0619 15:43:45.796055 139649090267008 learning.py:507] global step 30: loss = 1.1075 (3.218 sec/step)\n",
            "I0619 15:43:48.881813 139649090267008 learning.py:507] global step 30: loss = 1.1396 (3.084 sec/step)\n",
            "I0619 15:43:51.943762 139649090267008 learning.py:507] global step 30: loss = 1.2373 (3.060 sec/step)\n",
            "I0619 15:43:55.006309 139649090267008 learning.py:507] global step 30: loss = 1.2546 (3.061 sec/step)\n",
            "I0619 15:43:58.051608 139649090267008 learning.py:507] global step 30: loss = 1.2967 (3.043 sec/step)\n",
            "I0619 15:44:01.089746 139649090267008 learning.py:507] global step 31: loss = 1.2213 (3.036 sec/step)\n",
            "I0619 15:44:04.270389 139649090267008 learning.py:507] global step 31: loss = 1.1132 (3.179 sec/step)\n",
            "I0619 15:44:07.350455 139649090267008 learning.py:507] global step 31: loss = 1.0872 (3.078 sec/step)\n",
            "I0619 15:44:10.409219 139649090267008 learning.py:507] global step 31: loss = 1.3436 (3.057 sec/step)\n",
            "I0619 15:44:13.565102 139649090267008 learning.py:507] global step 31: loss = 1.1374 (3.154 sec/step)\n",
            "I0619 15:44:16.680737 139649090267008 learning.py:507] global step 31: loss = 1.2469 (3.114 sec/step)\n",
            "I0619 15:44:19.777423 139649090267008 learning.py:507] global step 31: loss = 1.1817 (3.095 sec/step)\n",
            "I0619 15:44:24.717604 139649090267008 learning.py:507] global step 31: loss = 1.2111 (4.931 sec/step)\n",
            "I0619 15:44:26.375324 139646017689344 supervisor.py:1050] Recording summary at step 31.\n",
            "I0619 15:44:28.274946 139649090267008 learning.py:507] global step 32: loss = 1.1238 (3.541 sec/step)\n",
            "I0619 15:44:29.128621 139646026082048 supervisor.py:1099] global_step/sec: 0.0416662\n",
            "I0619 15:44:31.428164 139649090267008 learning.py:507] global step 32: loss = 1.0054 (3.151 sec/step)\n",
            "I0619 15:44:34.538751 139649090267008 learning.py:507] global step 32: loss = 1.2431 (3.109 sec/step)\n",
            "I0619 15:44:37.618888 139649090267008 learning.py:507] global step 32: loss = 1.2132 (3.078 sec/step)\n",
            "I0619 15:44:40.747293 139649090267008 learning.py:507] global step 32: loss = 1.2933 (3.127 sec/step)\n",
            "I0619 15:44:43.847544 139649090267008 learning.py:507] global step 32: loss = 0.9985 (3.098 sec/step)\n",
            "I0619 15:44:47.035677 139649090267008 learning.py:507] global step 32: loss = 1.1717 (3.186 sec/step)\n",
            "I0619 15:44:50.239878 139649090267008 learning.py:507] global step 32: loss = 1.1734 (3.203 sec/step)\n",
            "I0619 15:44:53.261606 139649090267008 learning.py:507] global step 33: loss = 1.0191 (3.019 sec/step)\n",
            "I0619 15:44:56.298564 139649090267008 learning.py:507] global step 33: loss = 1.1854 (3.035 sec/step)\n",
            "I0619 15:44:59.365777 139649090267008 learning.py:507] global step 33: loss = 1.2637 (3.065 sec/step)\n",
            "I0619 15:45:02.431555 139649090267008 learning.py:507] global step 33: loss = 1.3369 (3.064 sec/step)\n",
            "I0619 15:45:05.742940 139649090267008 learning.py:507] global step 33: loss = 1.0832 (3.309 sec/step)\n",
            "I0619 15:45:08.825026 139649090267008 learning.py:507] global step 33: loss = 1.0616 (3.080 sec/step)\n",
            "I0619 15:45:11.860125 139649090267008 learning.py:507] global step 33: loss = 1.0943 (3.033 sec/step)\n",
            "I0619 15:45:14.932364 139649090267008 learning.py:507] global step 33: loss = 1.0060 (3.070 sec/step)\n",
            "I0619 15:45:17.971889 139649090267008 learning.py:507] global step 34: loss = 1.0428 (3.038 sec/step)\n",
            "I0619 15:45:21.062386 139649090267008 learning.py:507] global step 34: loss = 1.2233 (3.088 sec/step)\n",
            "I0619 15:45:24.225146 139649090267008 learning.py:507] global step 34: loss = 1.0994 (3.161 sec/step)\n",
            "I0619 15:45:27.256186 139649090267008 learning.py:507] global step 34: loss = 1.3340 (3.029 sec/step)\n",
            "I0619 15:45:30.326193 139649090267008 learning.py:507] global step 34: loss = 1.1040 (3.068 sec/step)\n",
            "tcmalloc: large alloc 1627873280 bytes == 0x7efd93b26000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:45:33.618436 139649090267008 learning.py:507] global step 34: loss = 1.1478 (3.290 sec/step)\n",
            "I0619 15:45:36.746439 139649090267008 learning.py:507] global step 34: loss = 1.3146 (3.126 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:45:39.756407 139649090267008 learning.py:507] global step 34: loss = 1.1363 (3.008 sec/step)\n",
            "I0619 15:45:42.882149 139649090267008 learning.py:507] global step 35: loss = 1.1645 (3.123 sec/step)\n",
            "I0619 15:45:45.964183 139649090267008 learning.py:507] global step 35: loss = 1.1035 (3.080 sec/step)\n",
            "I0619 15:45:49.034513 139649090267008 learning.py:507] global step 35: loss = 1.0065 (3.069 sec/step)\n",
            "I0619 15:45:52.416382 139649090267008 learning.py:507] global step 35: loss = 1.1399 (3.380 sec/step)\n",
            "I0619 15:45:55.481526 139649090267008 learning.py:507] global step 35: loss = 0.9914 (3.063 sec/step)\n",
            "tcmalloc: large alloc 1787822080 bytes == 0x7efd93b26000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:45:58.781952 139649090267008 learning.py:507] global step 35: loss = 1.1808 (3.299 sec/step)\n",
            "I0619 15:46:01.919023 139649090267008 learning.py:507] global step 35: loss = 1.0741 (3.135 sec/step)\n",
            "I0619 15:46:05.052027 139649090267008 learning.py:507] global step 35: loss = 1.1277 (3.131 sec/step)\n",
            "I0619 15:46:08.156085 139649090267008 learning.py:507] global step 36: loss = 1.1600 (3.102 sec/step)\n",
            "I0619 15:46:11.256638 139649090267008 learning.py:507] global step 36: loss = 1.2985 (3.099 sec/step)\n",
            "I0619 15:46:14.298510 139649090267008 learning.py:507] global step 36: loss = 1.0170 (3.040 sec/step)\n",
            "I0619 15:46:17.650007 139649090267008 learning.py:507] global step 36: loss = 1.0838 (3.349 sec/step)\n",
            "I0619 15:46:20.673682 139649090267008 learning.py:507] global step 36: loss = 1.0889 (3.022 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:46:25.980108 139649090267008 learning.py:507] global step 36: loss = 1.0241 (5.304 sec/step)\n",
            "I0619 15:46:26.739222 139646017689344 supervisor.py:1050] Recording summary at step 36.\n",
            "I0619 15:46:28.984738 139649090267008 learning.py:507] global step 36: loss = 1.0899 (2.999 sec/step)\n",
            "I0619 15:46:29.127805 139646026082048 supervisor.py:1099] global_step/sec: 0.0333335\n",
            "I0619 15:46:32.029170 139649090267008 learning.py:507] global step 36: loss = 1.0420 (3.043 sec/step)\n",
            "I0619 15:46:35.041004 139649090267008 learning.py:507] global step 37: loss = 0.9992 (3.010 sec/step)\n",
            "I0619 15:46:38.146368 139649090267008 learning.py:507] global step 37: loss = 0.9270 (3.104 sec/step)\n",
            "I0619 15:46:41.109033 139649090267008 learning.py:507] global step 37: loss = 0.9466 (2.961 sec/step)\n",
            "I0619 15:46:44.102746 139649090267008 learning.py:507] global step 37: loss = 1.0778 (2.992 sec/step)\n",
            "I0619 15:46:47.071341 139649090267008 learning.py:507] global step 37: loss = 1.0631 (2.967 sec/step)\n",
            "I0619 15:46:50.043468 139649090267008 learning.py:507] global step 37: loss = 1.0363 (2.970 sec/step)\n",
            "I0619 15:46:53.096314 139649090267008 learning.py:507] global step 37: loss = 0.9883 (3.051 sec/step)\n",
            "I0619 15:46:56.178918 139649090267008 learning.py:507] global step 37: loss = 0.9861 (3.081 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:46:59.201037 139649090267008 learning.py:507] global step 38: loss = 0.9102 (3.020 sec/step)\n",
            "I0619 15:47:02.188087 139649090267008 learning.py:507] global step 38: loss = 1.0129 (2.985 sec/step)\n",
            "I0619 15:47:05.220920 139649090267008 learning.py:507] global step 38: loss = 1.0804 (3.031 sec/step)\n",
            "I0619 15:47:08.180495 139649090267008 learning.py:507] global step 38: loss = 0.9662 (2.957 sec/step)\n",
            "I0619 15:47:11.184197 139649090267008 learning.py:507] global step 38: loss = 0.8901 (3.002 sec/step)\n",
            "I0619 15:47:14.219255 139649090267008 learning.py:507] global step 38: loss = 1.0683 (3.033 sec/step)\n",
            "I0619 15:47:17.211985 139649090267008 learning.py:507] global step 38: loss = 1.0619 (2.990 sec/step)\n",
            "I0619 15:47:20.162902 139649090267008 learning.py:507] global step 38: loss = 0.9835 (2.949 sec/step)\n",
            "I0619 15:47:23.117904 139649090267008 learning.py:507] global step 39: loss = 0.9269 (2.952 sec/step)\n",
            "I0619 15:47:26.083710 139649090267008 learning.py:507] global step 39: loss = 1.0567 (2.964 sec/step)\n",
            "I0619 15:47:29.100026 139649090267008 learning.py:507] global step 39: loss = 1.0676 (3.015 sec/step)\n",
            "I0619 15:47:32.340616 139649090267008 learning.py:507] global step 39: loss = 0.9665 (3.239 sec/step)\n",
            "I0619 15:47:35.307320 139649090267008 learning.py:507] global step 39: loss = 0.9701 (2.965 sec/step)\n",
            "I0619 15:47:38.234313 139649090267008 learning.py:507] global step 39: loss = 1.1703 (2.925 sec/step)\n",
            "I0619 15:47:41.201986 139649090267008 learning.py:507] global step 39: loss = 1.0320 (2.966 sec/step)\n",
            "I0619 15:47:44.159699 139649090267008 learning.py:507] global step 39: loss = 1.1212 (2.956 sec/step)\n",
            "I0619 15:47:47.152322 139649090267008 learning.py:507] global step 40: loss = 1.0916 (2.990 sec/step)\n",
            "I0619 15:47:50.258769 139649090267008 learning.py:507] global step 40: loss = 0.9867 (3.105 sec/step)\n",
            "I0619 15:47:53.250474 139649090267008 learning.py:507] global step 40: loss = 1.0704 (2.990 sec/step)\n",
            "I0619 15:47:56.236203 139649090267008 learning.py:507] global step 40: loss = 0.9519 (2.984 sec/step)\n",
            "I0619 15:47:59.235738 139649090267008 learning.py:507] global step 40: loss = 1.3325 (2.998 sec/step)\n",
            "I0619 15:48:02.177830 139649090267008 learning.py:507] global step 40: loss = 0.9592 (2.940 sec/step)\n",
            "I0619 15:48:05.235279 139649090267008 learning.py:507] global step 40: loss = 1.1491 (3.055 sec/step)\n",
            "I0619 15:48:08.213733 139649090267008 learning.py:507] global step 40: loss = 1.0987 (2.977 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:48:11.189808 139649090267008 learning.py:507] global step 41: loss = 0.9595 (2.973 sec/step)\n",
            "I0619 15:48:14.134445 139649090267008 learning.py:507] global step 41: loss = 0.9703 (2.943 sec/step)\n",
            "I0619 15:48:17.116941 139649090267008 learning.py:507] global step 41: loss = 0.9525 (2.981 sec/step)\n",
            "I0619 15:48:20.094299 139649090267008 learning.py:507] global step 41: loss = 0.9028 (2.976 sec/step)\n",
            "I0619 15:48:25.393672 139649090267008 learning.py:507] global step 41: loss = 1.1215 (5.290 sec/step)\n",
            "I0619 15:48:26.411311 139646017689344 supervisor.py:1050] Recording summary at step 41.\n",
            "I0619 15:48:28.647879 139649090267008 learning.py:507] global step 41: loss = 1.0741 (3.236 sec/step)\n",
            "I0619 15:48:29.143133 139646026082048 supervisor.py:1099] global_step/sec: 0.0416613\n",
            "I0619 15:48:31.667349 139649090267008 learning.py:507] global step 41: loss = 0.8867 (3.017 sec/step)\n",
            "I0619 15:48:34.704458 139649090267008 learning.py:507] global step 41: loss = 1.0076 (3.035 sec/step)\n",
            "I0619 15:48:37.727510 139649090267008 learning.py:507] global step 42: loss = 0.9280 (3.020 sec/step)\n",
            "I0619 15:48:40.713719 139649090267008 learning.py:507] global step 42: loss = 1.0446 (2.984 sec/step)\n",
            "tcmalloc: large alloc 3068264448 bytes == 0x7efd93b26000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 15:48:44.301529 139649090267008 learning.py:507] global step 42: loss = 1.0105 (3.586 sec/step)\n",
            "I0619 15:48:47.302890 139649090267008 learning.py:507] global step 42: loss = 1.0310 (2.999 sec/step)\n",
            "I0619 15:48:50.502517 139649090267008 learning.py:507] global step 42: loss = 0.9527 (3.198 sec/step)\n",
            "I0619 15:48:53.503864 139649090267008 learning.py:507] global step 42: loss = 0.9880 (2.999 sec/step)\n",
            "I0619 15:48:56.438158 139649090267008 learning.py:507] global step 42: loss = 0.9418 (2.933 sec/step)\n",
            "I0619 15:48:59.454713 139649090267008 learning.py:507] global step 42: loss = 0.9313 (3.015 sec/step)\n",
            "I0619 15:49:03.489447 139649090267008 learning.py:507] global step 43: loss = 0.8939 (4.032 sec/step)\n",
            "I0619 15:49:06.436015 139649090267008 learning.py:507] global step 43: loss = 0.9510 (2.945 sec/step)\n",
            "I0619 15:49:09.497946 139649090267008 learning.py:507] global step 43: loss = 0.9702 (3.060 sec/step)\n",
            "I0619 15:49:12.513052 139649090267008 learning.py:507] global step 43: loss = 1.0554 (3.013 sec/step)\n",
            "I0619 15:49:15.557971 139649090267008 learning.py:507] global step 43: loss = 1.0595 (3.043 sec/step)\n",
            "I0619 15:49:18.522566 139649090267008 learning.py:507] global step 43: loss = 0.9654 (2.963 sec/step)\n",
            "I0619 15:49:21.581603 139649090267008 learning.py:507] global step 43: loss = 0.8670 (3.057 sec/step)\n",
            "I0619 15:49:24.570785 139649090267008 learning.py:507] global step 43: loss = 0.9002 (2.987 sec/step)\n",
            "I0619 15:49:27.582031 139649090267008 learning.py:507] global step 44: loss = 0.9394 (3.010 sec/step)\n",
            "I0619 15:49:30.600695 139649090267008 learning.py:507] global step 44: loss = 0.9677 (3.017 sec/step)\n",
            "I0619 15:49:33.565221 139649090267008 learning.py:507] global step 44: loss = 1.0673 (2.963 sec/step)\n",
            "I0619 15:49:36.565046 139649090267008 learning.py:507] global step 44: loss = 0.9334 (2.998 sec/step)\n",
            "I0619 15:49:39.505497 139649090267008 learning.py:507] global step 44: loss = 0.8749 (2.939 sec/step)\n",
            "I0619 15:49:42.505671 139649090267008 learning.py:507] global step 44: loss = 0.9538 (2.998 sec/step)\n",
            "I0619 15:49:45.539515 139649090267008 learning.py:507] global step 44: loss = 1.1170 (3.032 sec/step)\n",
            "I0619 15:49:48.575654 139649090267008 learning.py:507] global step 44: loss = 0.8842 (3.034 sec/step)\n",
            "I0619 15:49:51.618347 139649090267008 learning.py:507] global step 45: loss = 1.0388 (3.040 sec/step)\n",
            "I0619 15:49:54.607209 139649090267008 learning.py:507] global step 45: loss = 1.0660 (2.987 sec/step)\n",
            "I0619 15:49:57.580237 139649090267008 learning.py:507] global step 45: loss = 1.0894 (2.971 sec/step)\n",
            "I0619 15:50:00.526428 139649090267008 learning.py:507] global step 45: loss = 0.8666 (2.944 sec/step)\n",
            "I0619 15:50:03.556090 139649090267008 learning.py:507] global step 45: loss = 0.9423 (3.028 sec/step)\n",
            "I0619 15:50:06.647328 139649090267008 learning.py:507] global step 45: loss = 1.0301 (3.089 sec/step)\n",
            "I0619 15:50:09.643045 139649090267008 learning.py:507] global step 45: loss = 0.9746 (2.994 sec/step)\n",
            "I0619 15:50:12.711859 139649090267008 learning.py:507] global step 45: loss = 1.1041 (3.067 sec/step)\n",
            "I0619 15:50:15.757507 139649090267008 learning.py:507] global step 46: loss = 1.0402 (3.043 sec/step)\n",
            "I0619 15:50:18.703271 139649090267008 learning.py:507] global step 46: loss = 0.9857 (2.944 sec/step)\n",
            "I0619 15:50:20.931303 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 15:50:23.889421 139649090267008 learning.py:507] global step 46: loss = 1.0272 (5.132 sec/step)\n",
            "I0619 15:50:27.283054 139646017689344 supervisor.py:1050] Recording summary at step 46.\n",
            "I0619 15:50:28.470012 139649090267008 learning.py:507] global step 46: loss = 0.9106 (4.572 sec/step)\n",
            "I0619 15:50:31.408175 139649090267008 learning.py:507] global step 46: loss = 0.9143 (2.936 sec/step)\n",
            "I0619 15:50:34.433110 139649090267008 learning.py:507] global step 46: loss = 0.9400 (3.023 sec/step)\n",
            "I0619 15:50:37.449186 139649090267008 learning.py:507] global step 46: loss = 0.9211 (3.014 sec/step)\n",
            "I0619 15:50:40.422684 139649090267008 learning.py:507] global step 46: loss = 0.8704 (2.972 sec/step)\n",
            "I0619 15:50:43.465134 139649090267008 learning.py:507] global step 47: loss = 1.1170 (3.040 sec/step)\n",
            "I0619 15:50:46.427784 139649090267008 learning.py:507] global step 47: loss = 0.9967 (2.961 sec/step)\n",
            "I0619 15:50:49.376874 139649090267008 learning.py:507] global step 47: loss = 0.9699 (2.947 sec/step)\n",
            "I0619 15:50:52.426411 139649090267008 learning.py:507] global step 47: loss = 1.1759 (3.048 sec/step)\n",
            "I0619 15:50:55.592838 139649090267008 learning.py:507] global step 47: loss = 0.9333 (3.165 sec/step)\n",
            "I0619 15:50:58.645440 139649090267008 learning.py:507] global step 47: loss = 0.8839 (3.051 sec/step)\n",
            "I0619 15:51:01.655346 139649090267008 learning.py:507] global step 47: loss = 0.7950 (3.008 sec/step)\n",
            "I0619 15:51:04.766405 139649090267008 learning.py:507] global step 47: loss = 0.8946 (3.109 sec/step)\n",
            "I0619 15:51:07.723636 139649090267008 learning.py:507] global step 48: loss = 0.9569 (2.954 sec/step)\n",
            "I0619 15:51:10.866929 139649090267008 learning.py:507] global step 48: loss = 0.8702 (3.141 sec/step)\n",
            "I0619 15:51:13.863929 139649090267008 learning.py:507] global step 48: loss = 1.0715 (2.995 sec/step)\n",
            "I0619 15:51:16.865432 139649090267008 learning.py:507] global step 48: loss = 0.9588 (3.000 sec/step)\n",
            "I0619 15:51:19.948188 139649090267008 learning.py:507] global step 48: loss = 0.8984 (3.081 sec/step)\n",
            "I0619 15:51:22.942196 139649090267008 learning.py:507] global step 48: loss = 1.0168 (2.992 sec/step)\n",
            "I0619 15:51:25.944654 139649090267008 learning.py:507] global step 48: loss = 0.9715 (3.000 sec/step)\n",
            "I0619 15:51:29.116841 139649090267008 learning.py:507] global step 48: loss = 1.0698 (3.170 sec/step)\n",
            "I0619 15:51:32.128294 139649090267008 learning.py:507] global step 49: loss = 0.8526 (3.009 sec/step)\n",
            "I0619 15:51:35.185606 139649090267008 learning.py:507] global step 49: loss = 0.8997 (3.055 sec/step)\n",
            "I0619 15:51:38.327717 139649090267008 learning.py:507] global step 49: loss = 0.9049 (3.140 sec/step)\n",
            "I0619 15:51:41.413468 139649090267008 learning.py:507] global step 49: loss = 0.8165 (3.084 sec/step)\n",
            "I0619 15:51:44.422853 139649090267008 learning.py:507] global step 49: loss = 0.9175 (3.007 sec/step)\n",
            "I0619 15:51:47.536381 139649090267008 learning.py:507] global step 49: loss = 0.8113 (3.112 sec/step)\n",
            "I0619 15:51:50.581797 139649090267008 learning.py:507] global step 49: loss = 0.7934 (3.043 sec/step)\n",
            "I0619 15:51:53.707077 139649090267008 learning.py:507] global step 49: loss = 0.9027 (3.123 sec/step)\n",
            "I0619 15:51:56.725200 139649090267008 learning.py:507] global step 50: loss = 0.8360 (3.016 sec/step)\n",
            "I0619 15:51:59.687819 139649090267008 learning.py:507] global step 50: loss = 0.9165 (2.961 sec/step)\n",
            "I0619 15:52:02.956116 139649090267008 learning.py:507] global step 50: loss = 0.9101 (3.267 sec/step)\n",
            "I0619 15:52:05.940797 139649090267008 learning.py:507] global step 50: loss = 0.9212 (2.983 sec/step)\n",
            "I0619 15:52:08.958320 139649090267008 learning.py:507] global step 50: loss = 0.8885 (3.016 sec/step)\n",
            "I0619 15:52:12.084805 139649090267008 learning.py:507] global step 50: loss = 1.0120 (3.125 sec/step)\n",
            "I0619 15:52:15.076059 139649090267008 learning.py:507] global step 50: loss = 0.9718 (2.989 sec/step)\n",
            "I0619 15:52:18.140316 139649090267008 learning.py:507] global step 50: loss = 0.9097 (3.063 sec/step)\n",
            "I0619 15:52:22.392846 139649090267008 learning.py:507] global step 51: loss = 0.9874 (4.210 sec/step)\n",
            "I0619 15:52:26.139516 139646017689344 supervisor.py:1050] Recording summary at step 51.\n",
            "I0619 15:52:26.783139 139649090267008 learning.py:507] global step 51: loss = 0.9080 (4.279 sec/step)\n",
            "I0619 15:52:29.751788 139649090267008 learning.py:507] global step 51: loss = 1.0235 (2.967 sec/step)\n",
            "I0619 15:52:32.671577 139649090267008 learning.py:507] global step 51: loss = 0.8557 (2.918 sec/step)\n",
            "I0619 15:52:35.725438 139649090267008 learning.py:507] global step 51: loss = 0.8870 (3.052 sec/step)\n",
            "I0619 15:52:38.727105 139649090267008 learning.py:507] global step 51: loss = 0.9377 (3.000 sec/step)\n",
            "I0619 15:52:41.674322 139649090267008 learning.py:507] global step 51: loss = 1.0158 (2.945 sec/step)\n",
            "I0619 15:52:44.721041 139649090267008 learning.py:507] global step 51: loss = 0.9104 (3.045 sec/step)\n",
            "I0619 15:52:47.783948 139649090267008 learning.py:507] global step 52: loss = 0.8832 (3.061 sec/step)\n",
            "I0619 15:52:50.804620 139649090267008 learning.py:507] global step 52: loss = 0.9132 (3.019 sec/step)\n",
            "I0619 15:52:53.774042 139649090267008 learning.py:507] global step 52: loss = 0.8902 (2.967 sec/step)\n",
            "I0619 15:52:56.756951 139649090267008 learning.py:507] global step 52: loss = 0.8435 (2.981 sec/step)\n",
            "I0619 15:52:59.754732 139649090267008 learning.py:507] global step 52: loss = 0.8690 (2.996 sec/step)\n",
            "I0619 15:53:02.686870 139649090267008 learning.py:507] global step 52: loss = 0.9555 (2.930 sec/step)\n",
            "I0619 15:53:05.818655 139649090267008 learning.py:507] global step 52: loss = 0.9569 (3.130 sec/step)\n",
            "I0619 15:53:08.798443 139649090267008 learning.py:507] global step 52: loss = 0.9432 (2.978 sec/step)\n",
            "I0619 15:53:11.855928 139649090267008 learning.py:507] global step 53: loss = 0.8768 (3.055 sec/step)\n",
            "I0619 15:53:14.847193 139649090267008 learning.py:507] global step 53: loss = 0.8300 (2.989 sec/step)\n",
            "I0619 15:53:17.839123 139649090267008 learning.py:507] global step 53: loss = 0.7964 (2.990 sec/step)\n",
            "I0619 15:53:20.871186 139649090267008 learning.py:507] global step 53: loss = 0.8353 (3.030 sec/step)\n",
            "I0619 15:53:23.840084 139649090267008 learning.py:507] global step 53: loss = 0.9526 (2.967 sec/step)\n",
            "I0619 15:53:26.774299 139649090267008 learning.py:507] global step 53: loss = 0.9533 (2.932 sec/step)\n",
            "I0619 15:53:29.885388 139649090267008 learning.py:507] global step 53: loss = 0.7966 (3.109 sec/step)\n",
            "I0619 15:53:32.857700 139649090267008 learning.py:507] global step 53: loss = 0.8498 (2.970 sec/step)\n",
            "I0619 15:53:35.899353 139649090267008 learning.py:507] global step 54: loss = 0.9358 (3.039 sec/step)\n",
            "I0619 15:53:38.965928 139649090267008 learning.py:507] global step 54: loss = 0.8934 (3.064 sec/step)\n",
            "I0619 15:53:41.996402 139649090267008 learning.py:507] global step 54: loss = 0.8185 (3.029 sec/step)\n",
            "I0619 15:53:44.976202 139649090267008 learning.py:507] global step 54: loss = 0.8374 (2.978 sec/step)\n",
            "I0619 15:53:48.003661 139649090267008 learning.py:507] global step 54: loss = 0.7889 (3.025 sec/step)\n",
            "I0619 15:53:51.013198 139649090267008 learning.py:507] global step 54: loss = 0.8112 (3.008 sec/step)\n",
            "I0619 15:53:53.976125 139649090267008 learning.py:507] global step 54: loss = 0.9948 (2.961 sec/step)\n",
            "I0619 15:53:56.971835 139649090267008 learning.py:507] global step 54: loss = 0.8263 (2.994 sec/step)\n",
            "I0619 15:53:59.951512 139649090267008 learning.py:507] global step 55: loss = 0.9321 (2.977 sec/step)\n",
            "I0619 15:54:02.911318 139649090267008 learning.py:507] global step 55: loss = 0.8226 (2.958 sec/step)\n",
            "I0619 15:54:05.889106 139649090267008 learning.py:507] global step 55: loss = 0.9447 (2.976 sec/step)\n",
            "I0619 15:54:08.915953 139649090267008 learning.py:507] global step 55: loss = 1.0437 (3.025 sec/step)\n",
            "I0619 15:54:12.012645 139649090267008 learning.py:507] global step 55: loss = 0.8157 (3.095 sec/step)\n",
            "I0619 15:54:14.968046 139649090267008 learning.py:507] global step 55: loss = 0.8373 (2.954 sec/step)\n",
            "I0619 15:54:18.045160 139649090267008 learning.py:507] global step 55: loss = 0.7965 (3.075 sec/step)\n",
            "I0619 15:54:21.006459 139649090267008 learning.py:507] global step 55: loss = 0.7929 (2.959 sec/step)\n",
            "I0619 15:54:25.658192 139646017689344 supervisor.py:1050] Recording summary at step 55.\n",
            "I0619 15:54:26.247464 139649090267008 learning.py:507] global step 56: loss = 1.0276 (5.017 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 15:54:29.186393 139649090267008 learning.py:507] global step 56: loss = 0.9557 (2.937 sec/step)\n",
            "I0619 15:54:32.187680 139649090267008 learning.py:507] global step 56: loss = 0.8456 (3.000 sec/step)\n",
            "I0619 15:54:35.127688 139649090267008 learning.py:507] global step 56: loss = 0.9580 (2.938 sec/step)\n",
            "I0619 15:54:38.054621 139649090267008 learning.py:507] global step 56: loss = 1.0392 (2.925 sec/step)\n",
            "I0619 15:54:41.006849 139649090267008 learning.py:507] global step 56: loss = 0.8585 (2.950 sec/step)\n",
            "I0619 15:54:44.040130 139649090267008 learning.py:507] global step 56: loss = 0.9502 (3.031 sec/step)\n",
            "I0619 15:54:47.058623 139649090267008 learning.py:507] global step 56: loss = 1.0057 (3.017 sec/step)\n",
            "I0619 15:54:50.038234 139649090267008 learning.py:507] global step 57: loss = 0.9442 (2.976 sec/step)\n",
            "I0619 15:54:53.077836 139649090267008 learning.py:507] global step 57: loss = 0.8262 (3.038 sec/step)\n",
            "I0619 15:54:56.129204 139649090267008 learning.py:507] global step 57: loss = 0.8864 (3.049 sec/step)\n",
            "I0619 15:54:59.076628 139649090267008 learning.py:507] global step 57: loss = 0.8898 (2.946 sec/step)\n",
            "I0619 15:55:02.070926 139649090267008 learning.py:507] global step 57: loss = 0.8377 (2.993 sec/step)\n",
            "I0619 15:55:05.126271 139649090267008 learning.py:507] global step 57: loss = 0.8100 (3.053 sec/step)\n",
            "I0619 15:55:08.174458 139649090267008 learning.py:507] global step 57: loss = 0.8085 (3.046 sec/step)\n",
            "I0619 15:55:11.184839 139649090267008 learning.py:507] global step 57: loss = 1.0360 (3.009 sec/step)\n",
            "I0619 15:55:14.242424 139649090267008 learning.py:507] global step 58: loss = 0.8246 (3.055 sec/step)\n",
            "I0619 15:55:17.211737 139649090267008 learning.py:507] global step 58: loss = 0.8493 (2.967 sec/step)\n",
            "I0619 15:55:20.188074 139649090267008 learning.py:507] global step 58: loss = 0.7848 (2.975 sec/step)\n",
            "I0619 15:55:23.267028 139649090267008 learning.py:507] global step 58: loss = 0.8017 (3.077 sec/step)\n",
            "I0619 15:55:26.307101 139649090267008 learning.py:507] global step 58: loss = 0.8994 (3.038 sec/step)\n",
            "I0619 15:55:29.365157 139649090267008 learning.py:507] global step 58: loss = 0.8604 (3.056 sec/step)\n",
            "I0619 15:55:32.528802 139649090267008 learning.py:507] global step 58: loss = 0.7530 (3.162 sec/step)\n",
            "I0619 15:55:35.629735 139649090267008 learning.py:507] global step 58: loss = 0.7598 (3.099 sec/step)\n",
            "I0619 15:55:38.638015 139649090267008 learning.py:507] global step 59: loss = 0.7706 (3.006 sec/step)\n",
            "I0619 15:55:41.665354 139649090267008 learning.py:507] global step 59: loss = 0.8628 (3.025 sec/step)\n",
            "I0619 15:55:44.680102 139649090267008 learning.py:507] global step 59: loss = 0.8747 (3.013 sec/step)\n",
            "I0619 15:55:47.879536 139649090267008 learning.py:507] global step 59: loss = 0.8784 (3.198 sec/step)\n",
            "I0619 15:55:50.909731 139649090267008 learning.py:507] global step 59: loss = 0.8772 (3.028 sec/step)\n",
            "I0619 15:55:53.887856 139649090267008 learning.py:507] global step 59: loss = 0.8725 (2.976 sec/step)\n",
            "I0619 15:55:56.899262 139649090267008 learning.py:507] global step 59: loss = 0.7867 (3.010 sec/step)\n",
            "I0619 15:55:59.835942 139649090267008 learning.py:507] global step 59: loss = 0.9600 (2.935 sec/step)\n",
            "I0619 15:56:02.860412 139649090267008 learning.py:507] global step 60: loss = 0.8591 (3.022 sec/step)\n",
            "I0619 15:56:06.104422 139649090267008 learning.py:507] global step 60: loss = 0.8661 (3.242 sec/step)\n",
            "I0619 15:56:09.067248 139649090267008 learning.py:507] global step 60: loss = 0.7628 (2.961 sec/step)\n",
            "I0619 15:56:12.090878 139649090267008 learning.py:507] global step 60: loss = 0.7390 (3.022 sec/step)\n",
            "I0619 15:56:15.068893 139649090267008 learning.py:507] global step 60: loss = 0.9478 (2.976 sec/step)\n",
            "I0619 15:56:18.065025 139649090267008 learning.py:507] global step 60: loss = 0.8247 (2.994 sec/step)\n",
            "I0619 15:56:20.993260 139649090267008 learning.py:507] global step 60: loss = 0.9353 (2.927 sec/step)\n",
            "I0619 15:56:26.235411 139649090267008 learning.py:507] global step 60: loss = 0.9802 (5.240 sec/step)\n",
            "I0619 15:56:26.251443 139646017689344 supervisor.py:1050] Recording summary at step 60.\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:56:29.256042 139649090267008 learning.py:507] global step 61: loss = 0.9808 (3.018 sec/step)\n",
            "I0619 15:56:32.212613 139649090267008 learning.py:507] global step 61: loss = 0.8895 (2.955 sec/step)\n",
            "I0619 15:56:35.199169 139649090267008 learning.py:507] global step 61: loss = 0.7826 (2.985 sec/step)\n",
            "I0619 15:56:38.216058 139649090267008 learning.py:507] global step 61: loss = 0.8063 (3.015 sec/step)\n",
            "I0619 15:56:41.195206 139649090267008 learning.py:507] global step 61: loss = 0.8049 (2.977 sec/step)\n",
            "I0619 15:56:44.329465 139649090267008 learning.py:507] global step 61: loss = 0.8312 (3.132 sec/step)\n",
            "I0619 15:56:47.407772 139649090267008 learning.py:507] global step 61: loss = 0.9298 (3.076 sec/step)\n",
            "I0619 15:56:50.420432 139649090267008 learning.py:507] global step 61: loss = 0.8740 (3.011 sec/step)\n",
            "I0619 15:56:53.450448 139649090267008 learning.py:507] global step 62: loss = 0.8969 (3.027 sec/step)\n",
            "I0619 15:56:56.657695 139649090267008 learning.py:507] global step 62: loss = 0.8446 (3.205 sec/step)\n",
            "I0619 15:56:59.698363 139649090267008 learning.py:507] global step 62: loss = 0.7500 (3.039 sec/step)\n",
            "I0619 15:57:02.686752 139649090267008 learning.py:507] global step 62: loss = 0.8040 (2.986 sec/step)\n",
            "I0619 15:57:05.697007 139649090267008 learning.py:507] global step 62: loss = 0.8058 (3.008 sec/step)\n",
            "I0619 15:57:08.722353 139649090267008 learning.py:507] global step 62: loss = 0.8448 (3.023 sec/step)\n",
            "I0619 15:57:11.657558 139649090267008 learning.py:507] global step 62: loss = 0.8092 (2.933 sec/step)\n",
            "I0619 15:57:14.927041 139649090267008 learning.py:507] global step 62: loss = 0.9235 (3.268 sec/step)\n",
            "I0619 15:57:17.935217 139649090267008 learning.py:507] global step 63: loss = 0.8255 (3.005 sec/step)\n",
            "I0619 15:57:20.964183 139649090267008 learning.py:507] global step 63: loss = 0.8887 (3.026 sec/step)\n",
            "I0619 15:57:23.969993 139649090267008 learning.py:507] global step 63: loss = 0.6383 (3.004 sec/step)\n",
            "I0619 15:57:26.982869 139649090267008 learning.py:507] global step 63: loss = 0.7485 (3.011 sec/step)\n",
            "I0619 15:57:30.009540 139649090267008 learning.py:507] global step 63: loss = 0.9415 (3.025 sec/step)\n",
            "I0619 15:57:33.001049 139649090267008 learning.py:507] global step 63: loss = 0.7011 (2.990 sec/step)\n",
            "I0619 15:57:35.979002 139649090267008 learning.py:507] global step 63: loss = 0.7560 (2.976 sec/step)\n",
            "I0619 15:57:38.927089 139649090267008 learning.py:507] global step 63: loss = 0.8100 (2.946 sec/step)\n",
            "I0619 15:57:41.921649 139649090267008 learning.py:507] global step 64: loss = 0.9902 (2.992 sec/step)\n",
            "I0619 15:57:44.904713 139649090267008 learning.py:507] global step 64: loss = 1.0012 (2.981 sec/step)\n",
            "I0619 15:57:47.881445 139649090267008 learning.py:507] global step 64: loss = 0.7839 (2.974 sec/step)\n",
            "I0619 15:57:50.924937 139649090267008 learning.py:507] global step 64: loss = 0.8163 (3.041 sec/step)\n",
            "I0619 15:57:53.860386 139649090267008 learning.py:507] global step 64: loss = 0.8455 (2.934 sec/step)\n",
            "I0619 15:57:56.891915 139649090267008 learning.py:507] global step 64: loss = 0.8005 (3.030 sec/step)\n",
            "I0619 15:57:59.925683 139649090267008 learning.py:507] global step 64: loss = 0.8256 (3.032 sec/step)\n",
            "I0619 15:58:02.894110 139649090267008 learning.py:507] global step 64: loss = 0.8650 (2.967 sec/step)\n",
            "I0619 15:58:05.933470 139649090267008 learning.py:507] global step 65: loss = 0.8571 (3.036 sec/step)\n",
            "I0619 15:58:08.870245 139649090267008 learning.py:507] global step 65: loss = 0.8459 (2.935 sec/step)\n",
            "I0619 15:58:11.914160 139649090267008 learning.py:507] global step 65: loss = 1.0271 (3.042 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 15:58:14.869260 139649090267008 learning.py:507] global step 65: loss = 0.7359 (2.953 sec/step)\n",
            "I0619 15:58:17.907995 139649090267008 learning.py:507] global step 65: loss = 0.9173 (3.037 sec/step)\n",
            "I0619 15:58:20.919805 139649090267008 learning.py:507] global step 65: loss = 0.8354 (3.010 sec/step)\n",
            "I0619 15:58:26.084183 139649090267008 learning.py:507] global step 65: loss = 0.7236 (5.148 sec/step)\n",
            "I0619 15:58:26.866929 139646017689344 supervisor.py:1050] Recording summary at step 65.\n",
            "I0619 15:58:29.248103 139649090267008 learning.py:507] global step 65: loss = 0.7830 (3.162 sec/step)\n",
            "I0619 15:58:32.214917 139649090267008 learning.py:507] global step 66: loss = 0.8371 (2.965 sec/step)\n",
            "I0619 15:58:35.305214 139649090267008 learning.py:507] global step 66: loss = 0.7891 (3.088 sec/step)\n",
            "I0619 15:58:38.419882 139649090267008 learning.py:507] global step 66: loss = 0.8010 (3.113 sec/step)\n",
            "I0619 15:58:41.411079 139649090267008 learning.py:507] global step 66: loss = 0.9422 (2.989 sec/step)\n",
            "I0619 15:58:44.447235 139649090267008 learning.py:507] global step 66: loss = 0.8001 (3.034 sec/step)\n",
            "I0619 15:58:47.430042 139649090267008 learning.py:507] global step 66: loss = 0.7352 (2.981 sec/step)\n",
            "I0619 15:58:50.406712 139649090267008 learning.py:507] global step 66: loss = 0.7735 (2.975 sec/step)\n",
            "I0619 15:58:53.466701 139649090267008 learning.py:507] global step 66: loss = 0.7870 (3.058 sec/step)\n",
            "I0619 15:58:56.704499 139649090267008 learning.py:507] global step 67: loss = 0.8084 (3.235 sec/step)\n",
            "I0619 15:58:59.698279 139649090267008 learning.py:507] global step 67: loss = 0.9013 (2.992 sec/step)\n",
            "I0619 15:59:02.722658 139649090267008 learning.py:507] global step 67: loss = 0.8908 (3.022 sec/step)\n",
            "I0619 15:59:05.694041 139649090267008 learning.py:507] global step 67: loss = 0.9788 (2.969 sec/step)\n",
            "I0619 15:59:08.661246 139649090267008 learning.py:507] global step 67: loss = 0.8649 (2.965 sec/step)\n",
            "I0619 15:59:11.672117 139649090267008 learning.py:507] global step 67: loss = 0.8034 (3.009 sec/step)\n",
            "I0619 15:59:14.740131 139649090267008 learning.py:507] global step 67: loss = 0.8369 (3.066 sec/step)\n",
            "I0619 15:59:17.771634 139649090267008 learning.py:507] global step 67: loss = 0.7496 (3.030 sec/step)\n",
            "I0619 15:59:20.748226 139649090267008 learning.py:507] global step 68: loss = 0.7348 (2.974 sec/step)\n",
            "I0619 15:59:23.738793 139649090267008 learning.py:507] global step 68: loss = 0.9125 (2.989 sec/step)\n",
            "I0619 15:59:26.748765 139649090267008 learning.py:507] global step 68: loss = 0.7482 (3.008 sec/step)\n",
            "I0619 15:59:29.774937 139649090267008 learning.py:507] global step 68: loss = 0.7432 (3.024 sec/step)\n",
            "I0619 15:59:32.759812 139649090267008 learning.py:507] global step 68: loss = 0.7976 (2.983 sec/step)\n",
            "I0619 15:59:35.860370 139649090267008 learning.py:507] global step 68: loss = 0.7937 (3.099 sec/step)\n",
            "I0619 15:59:38.817328 139649090267008 learning.py:507] global step 68: loss = 0.6961 (2.955 sec/step)\n",
            "I0619 15:59:41.819137 139649090267008 learning.py:507] global step 68: loss = 0.8375 (3.000 sec/step)\n",
            "I0619 15:59:44.844879 139649090267008 learning.py:507] global step 69: loss = 0.6728 (3.023 sec/step)\n",
            "I0619 15:59:47.853658 139649090267008 learning.py:507] global step 69: loss = 0.8002 (3.007 sec/step)\n",
            "I0619 15:59:50.847516 139649090267008 learning.py:507] global step 69: loss = 0.7524 (2.992 sec/step)\n",
            "I0619 15:59:53.801032 139649090267008 learning.py:507] global step 69: loss = 0.8790 (2.952 sec/step)\n",
            "I0619 15:59:56.869070 139649090267008 learning.py:507] global step 69: loss = 0.9037 (3.063 sec/step)\n",
            "I0619 15:59:59.847696 139649090267008 learning.py:507] global step 69: loss = 0.7393 (2.977 sec/step)\n",
            "I0619 16:00:02.933042 139649090267008 learning.py:507] global step 69: loss = 0.7559 (3.083 sec/step)\n",
            "I0619 16:00:05.986408 139649090267008 learning.py:507] global step 69: loss = 0.7565 (3.051 sec/step)\n",
            "I0619 16:00:09.025599 139649090267008 learning.py:507] global step 70: loss = 0.9031 (3.037 sec/step)\n",
            "I0619 16:00:12.177231 139649090267008 learning.py:507] global step 70: loss = 0.8646 (3.150 sec/step)\n",
            "I0619 16:00:15.142313 139649090267008 learning.py:507] global step 70: loss = 0.6973 (2.963 sec/step)\n",
            "I0619 16:00:18.284986 139649090267008 learning.py:507] global step 70: loss = 0.9303 (3.141 sec/step)\n",
            "I0619 16:00:20.931303 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "tcmalloc: large alloc 3068264448 bytes == 0x7efe0bd94000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 16:00:21.852674 139649090267008 learning.py:507] global step 70: loss = 0.8887 (3.213 sec/step)\n",
            "I0619 16:00:30.432398 139649090267008 learning.py:507] global step 70: loss = 0.8402 (8.571 sec/step)\n",
            "I0619 16:00:31.565598 139646017689344 supervisor.py:1050] Recording summary at step 70.\n",
            "I0619 16:00:33.942735 139649090267008 learning.py:507] global step 70: loss = 0.7619 (3.504 sec/step)\n",
            "I0619 16:00:37.159175 139649090267008 learning.py:507] global step 70: loss = 0.7602 (3.215 sec/step)\n",
            "I0619 16:00:40.651883 139649090267008 learning.py:507] global step 71: loss = 0.6921 (3.490 sec/step)\n",
            "I0619 16:00:43.819517 139649090267008 learning.py:507] global step 71: loss = 0.7930 (3.166 sec/step)\n",
            "I0619 16:00:47.831631 139649090267008 learning.py:507] global step 71: loss = 0.7752 (4.010 sec/step)\n",
            "I0619 16:00:51.007424 139649090267008 learning.py:507] global step 71: loss = 0.8148 (3.174 sec/step)\n",
            "I0619 16:00:54.161068 139649090267008 learning.py:507] global step 71: loss = 0.7735 (3.152 sec/step)\n",
            "I0619 16:00:57.254751 139649090267008 learning.py:507] global step 71: loss = 0.7775 (3.092 sec/step)\n",
            "I0619 16:01:00.364768 139649090267008 learning.py:507] global step 71: loss = 0.7262 (3.108 sec/step)\n",
            "I0619 16:01:03.480207 139649090267008 learning.py:507] global step 71: loss = 0.6725 (3.113 sec/step)\n",
            "I0619 16:01:06.475937 139649090267008 learning.py:507] global step 72: loss = 0.7442 (2.993 sec/step)\n",
            "I0619 16:01:09.535519 139649090267008 learning.py:507] global step 72: loss = 0.7731 (3.058 sec/step)\n",
            "I0619 16:01:12.595360 139649090267008 learning.py:507] global step 72: loss = 0.7620 (3.058 sec/step)\n",
            "I0619 16:01:15.571316 139649090267008 learning.py:507] global step 72: loss = 0.7189 (2.974 sec/step)\n",
            "I0619 16:01:18.575385 139649090267008 learning.py:507] global step 72: loss = 0.8478 (3.002 sec/step)\n",
            "I0619 16:01:21.564222 139649090267008 learning.py:507] global step 72: loss = 0.7105 (2.987 sec/step)\n",
            "I0619 16:01:24.501768 139649090267008 learning.py:507] global step 72: loss = 0.7398 (2.936 sec/step)\n",
            "I0619 16:01:27.477010 139649090267008 learning.py:507] global step 72: loss = 0.7229 (2.973 sec/step)\n",
            "I0619 16:01:30.538812 139649090267008 learning.py:507] global step 73: loss = 0.7461 (3.059 sec/step)\n",
            "I0619 16:01:33.692169 139649090267008 learning.py:507] global step 73: loss = 0.7246 (3.152 sec/step)\n",
            "I0619 16:01:36.743438 139649090267008 learning.py:507] global step 73: loss = 0.7499 (3.050 sec/step)\n",
            "I0619 16:01:39.854378 139649090267008 learning.py:507] global step 73: loss = 1.0557 (3.109 sec/step)\n",
            "I0619 16:01:42.873873 139649090267008 learning.py:507] global step 73: loss = 0.8093 (3.018 sec/step)\n",
            "I0619 16:01:46.099683 139649090267008 learning.py:507] global step 73: loss = 0.8122 (3.224 sec/step)\n",
            "I0619 16:01:49.148540 139649090267008 learning.py:507] global step 73: loss = 0.6657 (3.047 sec/step)\n",
            "I0619 16:01:52.247854 139649090267008 learning.py:507] global step 73: loss = 0.8040 (3.098 sec/step)\n",
            "I0619 16:01:55.269399 139649090267008 learning.py:507] global step 74: loss = 0.8116 (3.019 sec/step)\n",
            "I0619 16:01:58.327536 139649090267008 learning.py:507] global step 74: loss = 0.7452 (3.056 sec/step)\n",
            "I0619 16:02:01.369604 139649090267008 learning.py:507] global step 74: loss = 0.8833 (3.040 sec/step)\n",
            "I0619 16:02:04.665107 139649090267008 learning.py:507] global step 74: loss = 0.7081 (3.294 sec/step)\n",
            "I0619 16:02:07.909921 139649090267008 learning.py:507] global step 74: loss = 0.7203 (3.243 sec/step)\n",
            "I0619 16:02:11.027470 139649090267008 learning.py:507] global step 74: loss = 0.7317 (3.116 sec/step)\n",
            "I0619 16:02:14.139695 139649090267008 learning.py:507] global step 74: loss = 0.7643 (3.110 sec/step)\n",
            "I0619 16:02:17.243195 139649090267008 learning.py:507] global step 74: loss = 0.8278 (3.102 sec/step)\n",
            "I0619 16:02:20.300061 139649090267008 learning.py:507] global step 75: loss = 0.8479 (3.055 sec/step)\n",
            "I0619 16:02:25.922914 139649090267008 learning.py:507] global step 75: loss = 0.8137 (5.612 sec/step)\n",
            "I0619 16:02:26.498395 139646017689344 supervisor.py:1050] Recording summary at step 75.\n",
            "I0619 16:02:29.011273 139649090267008 learning.py:507] global step 75: loss = 0.7087 (3.086 sec/step)\n",
            "I0619 16:02:32.123061 139649090267008 learning.py:507] global step 75: loss = 0.7406 (3.110 sec/step)\n",
            "I0619 16:02:35.192157 139649090267008 learning.py:507] global step 75: loss = 0.6897 (3.067 sec/step)\n",
            "I0619 16:02:38.268098 139649090267008 learning.py:507] global step 75: loss = 0.7730 (3.074 sec/step)\n",
            "I0619 16:02:41.279362 139649090267008 learning.py:507] global step 75: loss = 0.7620 (3.009 sec/step)\n",
            "I0619 16:02:44.312814 139649090267008 learning.py:507] global step 75: loss = 0.7093 (3.032 sec/step)\n",
            "I0619 16:02:47.347806 139649090267008 learning.py:507] global step 76: loss = 0.7202 (3.033 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:02:50.567792 139649090267008 learning.py:507] global step 76: loss = 0.6954 (3.218 sec/step)\n",
            "I0619 16:02:53.783952 139649090267008 learning.py:507] global step 76: loss = 0.6972 (3.214 sec/step)\n",
            "I0619 16:02:56.883177 139649090267008 learning.py:507] global step 76: loss = 0.6937 (3.097 sec/step)\n",
            "tcmalloc: large alloc 3068264448 bytes == 0x7efdbd294000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 16:03:00.758167 139649090267008 learning.py:507] global step 76: loss = 0.6632 (3.873 sec/step)\n",
            "I0619 16:03:03.810016 139649090267008 learning.py:507] global step 76: loss = 0.6342 (3.050 sec/step)\n",
            "I0619 16:03:06.804671 139649090267008 learning.py:507] global step 76: loss = 0.7744 (2.993 sec/step)\n",
            "I0619 16:03:10.147341 139649090267008 learning.py:507] global step 76: loss = 0.8010 (3.341 sec/step)\n",
            "I0619 16:03:13.167484 139649090267008 learning.py:507] global step 77: loss = 0.6601 (3.017 sec/step)\n",
            "I0619 16:03:16.369477 139649090267008 learning.py:507] global step 77: loss = 0.7094 (3.200 sec/step)\n",
            "I0619 16:03:20.121524 139649090267008 learning.py:507] global step 77: loss = 0.8093 (3.750 sec/step)\n",
            "I0619 16:03:23.164681 139649090267008 learning.py:507] global step 77: loss = 0.7287 (3.041 sec/step)\n",
            "I0619 16:03:26.275016 139649090267008 learning.py:507] global step 77: loss = 0.6830 (3.108 sec/step)\n",
            "I0619 16:03:29.350006 139649090267008 learning.py:507] global step 77: loss = 0.7867 (3.073 sec/step)\n",
            "I0619 16:03:32.502662 139649090267008 learning.py:507] global step 77: loss = 0.8906 (3.151 sec/step)\n",
            "I0619 16:03:35.618909 139649090267008 learning.py:507] global step 77: loss = 0.7523 (3.115 sec/step)\n",
            "I0619 16:03:38.693746 139649090267008 learning.py:507] global step 78: loss = 0.6732 (3.072 sec/step)\n",
            "I0619 16:03:41.694413 139649090267008 learning.py:507] global step 78: loss = 0.8572 (2.999 sec/step)\n",
            "I0619 16:03:44.812401 139649090267008 learning.py:507] global step 78: loss = 0.7337 (3.116 sec/step)\n",
            "I0619 16:03:48.052649 139649090267008 learning.py:507] global step 78: loss = 0.8043 (3.239 sec/step)\n",
            "I0619 16:03:51.254681 139649090267008 learning.py:507] global step 78: loss = 0.7933 (3.200 sec/step)\n",
            "I0619 16:03:54.304751 139649090267008 learning.py:507] global step 78: loss = 0.7404 (3.048 sec/step)\n",
            "I0619 16:03:57.450237 139649090267008 learning.py:507] global step 78: loss = 0.7383 (3.144 sec/step)\n",
            "I0619 16:04:00.472623 139649090267008 learning.py:507] global step 78: loss = 0.7213 (3.020 sec/step)\n",
            "I0619 16:04:03.479380 139649090267008 learning.py:507] global step 79: loss = 0.6699 (3.004 sec/step)\n",
            "I0619 16:04:06.683630 139649090267008 learning.py:507] global step 79: loss = 0.7217 (3.202 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:04:09.718489 139649090267008 learning.py:507] global step 79: loss = 0.7595 (3.033 sec/step)\n",
            "I0619 16:04:12.692190 139649090267008 learning.py:507] global step 79: loss = 0.7455 (2.972 sec/step)\n",
            "I0619 16:04:15.656446 139649090267008 learning.py:507] global step 79: loss = 0.7370 (2.962 sec/step)\n",
            "I0619 16:04:18.686062 139649090267008 learning.py:507] global step 79: loss = 0.8038 (3.028 sec/step)\n",
            "I0619 16:04:22.715903 139649090267008 learning.py:507] global step 79: loss = 0.7732 (4.009 sec/step)\n",
            "I0619 16:04:26.098448 139646017689344 supervisor.py:1050] Recording summary at step 79.\n",
            "I0619 16:04:27.033204 139649090267008 learning.py:507] global step 79: loss = 0.6634 (4.310 sec/step)\n",
            "I0619 16:04:30.071434 139649090267008 learning.py:507] global step 80: loss = 0.7944 (3.035 sec/step)\n",
            "I0619 16:04:33.073533 139649090267008 learning.py:507] global step 80: loss = 0.6831 (3.000 sec/step)\n",
            "I0619 16:04:36.025614 139649090267008 learning.py:507] global step 80: loss = 0.6817 (2.950 sec/step)\n",
            "I0619 16:04:39.101152 139649090267008 learning.py:507] global step 80: loss = 0.7806 (3.074 sec/step)\n",
            "I0619 16:04:42.395026 139649090267008 learning.py:507] global step 80: loss = 0.7483 (3.292 sec/step)\n",
            "I0619 16:04:45.444227 139649090267008 learning.py:507] global step 80: loss = 0.6698 (3.047 sec/step)\n",
            "I0619 16:04:48.400102 139649090267008 learning.py:507] global step 80: loss = 0.8158 (2.954 sec/step)\n",
            "I0619 16:04:51.429609 139649090267008 learning.py:507] global step 80: loss = 0.7364 (3.028 sec/step)\n",
            "I0619 16:04:54.490937 139649090267008 learning.py:507] global step 81: loss = 0.6468 (3.059 sec/step)\n",
            "I0619 16:04:57.520785 139649090267008 learning.py:507] global step 81: loss = 0.7312 (3.026 sec/step)\n",
            "I0619 16:05:00.826587 139649090267008 learning.py:507] global step 81: loss = 0.6475 (3.304 sec/step)\n",
            "I0619 16:05:03.906208 139649090267008 learning.py:507] global step 81: loss = 0.6900 (3.078 sec/step)\n",
            "I0619 16:05:06.906659 139649090267008 learning.py:507] global step 81: loss = 0.6667 (2.999 sec/step)\n",
            "I0619 16:05:09.940890 139649090267008 learning.py:507] global step 81: loss = 0.7938 (3.032 sec/step)\n",
            "I0619 16:05:12.909010 139649090267008 learning.py:507] global step 81: loss = 0.7526 (2.966 sec/step)\n",
            "I0619 16:05:15.885555 139649090267008 learning.py:507] global step 81: loss = 0.7417 (2.975 sec/step)\n",
            "I0619 16:05:18.969255 139649090267008 learning.py:507] global step 82: loss = 0.6742 (3.081 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:05:21.939957 139649090267008 learning.py:507] global step 82: loss = 0.6975 (2.969 sec/step)\n",
            "I0619 16:05:24.949297 139649090267008 learning.py:507] global step 82: loss = 0.8761 (3.007 sec/step)\n",
            "I0619 16:05:27.905458 139649090267008 learning.py:507] global step 82: loss = 0.6926 (2.954 sec/step)\n",
            "I0619 16:05:30.950607 139649090267008 learning.py:507] global step 82: loss = 0.6937 (3.043 sec/step)\n",
            "I0619 16:05:34.037872 139649090267008 learning.py:507] global step 82: loss = 0.7755 (3.085 sec/step)\n",
            "I0619 16:05:37.035875 139649090267008 learning.py:507] global step 82: loss = 0.6711 (2.996 sec/step)\n",
            "I0619 16:05:40.066605 139649090267008 learning.py:507] global step 82: loss = 0.8366 (3.029 sec/step)\n",
            "I0619 16:05:43.019282 139649090267008 learning.py:507] global step 83: loss = 0.8165 (2.950 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:05:46.036347 139649090267008 learning.py:507] global step 83: loss = 0.6591 (3.015 sec/step)\n",
            "I0619 16:05:49.064935 139649090267008 learning.py:507] global step 83: loss = 0.7022 (3.027 sec/step)\n",
            "I0619 16:05:52.098197 139649090267008 learning.py:507] global step 83: loss = 0.7041 (3.032 sec/step)\n",
            "I0619 16:05:55.113335 139649090267008 learning.py:507] global step 83: loss = 0.6809 (3.013 sec/step)\n",
            "I0619 16:05:58.130671 139649090267008 learning.py:507] global step 83: loss = 0.7802 (3.016 sec/step)\n",
            "I0619 16:06:01.152589 139649090267008 learning.py:507] global step 83: loss = 0.8502 (3.020 sec/step)\n",
            "I0619 16:06:04.049612 139649090267008 learning.py:507] global step 83: loss = 0.6949 (2.895 sec/step)\n",
            "I0619 16:06:07.177107 139649090267008 learning.py:507] global step 84: loss = 0.8606 (3.125 sec/step)\n",
            "I0619 16:06:10.195560 139649090267008 learning.py:507] global step 84: loss = 0.6779 (3.017 sec/step)\n",
            "I0619 16:06:13.232290 139649090267008 learning.py:507] global step 84: loss = 0.7283 (3.035 sec/step)\n",
            "I0619 16:06:16.347370 139649090267008 learning.py:507] global step 84: loss = 0.8827 (3.113 sec/step)\n",
            "I0619 16:06:19.362295 139649090267008 learning.py:507] global step 84: loss = 0.6636 (3.013 sec/step)\n",
            "I0619 16:06:24.279170 139649090267008 learning.py:507] global step 84: loss = 0.8912 (4.905 sec/step)\n",
            "I0619 16:06:26.205383 139646017689344 supervisor.py:1050] Recording summary at step 84.\n",
            "I0619 16:06:28.011422 139649090267008 learning.py:507] global step 84: loss = 0.7861 (3.730 sec/step)\n",
            "I0619 16:06:31.005868 139649090267008 learning.py:507] global step 84: loss = 0.7373 (2.993 sec/step)\n",
            "I0619 16:06:34.087053 139649090267008 learning.py:507] global step 85: loss = 0.6854 (3.079 sec/step)\n",
            "I0619 16:06:37.035373 139649090267008 learning.py:507] global step 85: loss = 0.8316 (2.947 sec/step)\n",
            "I0619 16:06:40.027093 139649090267008 learning.py:507] global step 85: loss = 0.7219 (2.990 sec/step)\n",
            "I0619 16:06:43.070451 139649090267008 learning.py:507] global step 85: loss = 0.7032 (3.041 sec/step)\n",
            "I0619 16:06:46.389834 139649090267008 learning.py:507] global step 85: loss = 0.7306 (3.318 sec/step)\n",
            "I0619 16:06:49.329507 139649090267008 learning.py:507] global step 85: loss = 0.7521 (2.938 sec/step)\n",
            "I0619 16:06:52.306786 139649090267008 learning.py:507] global step 85: loss = 0.7240 (2.975 sec/step)\n",
            "I0619 16:06:55.269951 139649090267008 learning.py:507] global step 85: loss = 0.6595 (2.961 sec/step)\n",
            "I0619 16:06:58.264895 139649090267008 learning.py:507] global step 86: loss = 0.7921 (2.993 sec/step)\n",
            "I0619 16:07:01.215347 139649090267008 learning.py:507] global step 86: loss = 0.7066 (2.948 sec/step)\n",
            "I0619 16:07:04.211956 139649090267008 learning.py:507] global step 86: loss = 0.7554 (2.995 sec/step)\n",
            "I0619 16:07:07.195021 139649090267008 learning.py:507] global step 86: loss = 0.7804 (2.981 sec/step)\n",
            "I0619 16:07:10.148855 139649090267008 learning.py:507] global step 86: loss = 0.7481 (2.952 sec/step)\n",
            "I0619 16:07:13.133935 139649090267008 learning.py:507] global step 86: loss = 0.7173 (2.983 sec/step)\n",
            "I0619 16:07:16.214949 139649090267008 learning.py:507] global step 86: loss = 0.7038 (3.079 sec/step)\n",
            "I0619 16:07:19.218231 139649090267008 learning.py:507] global step 86: loss = 0.6405 (3.002 sec/step)\n",
            "I0619 16:07:22.167568 139649090267008 learning.py:507] global step 87: loss = 0.6591 (2.947 sec/step)\n",
            "I0619 16:07:25.175324 139649090267008 learning.py:507] global step 87: loss = 0.7939 (3.006 sec/step)\n",
            "I0619 16:07:28.134712 139649090267008 learning.py:507] global step 87: loss = 0.6968 (2.958 sec/step)\n",
            "I0619 16:07:31.497355 139649090267008 learning.py:507] global step 87: loss = 0.6516 (3.361 sec/step)\n",
            "I0619 16:07:34.512708 139649090267008 learning.py:507] global step 87: loss = 0.7660 (3.013 sec/step)\n",
            "I0619 16:07:37.544463 139649090267008 learning.py:507] global step 87: loss = 0.6654 (3.030 sec/step)\n",
            "I0619 16:07:40.523100 139649090267008 learning.py:507] global step 87: loss = 0.6876 (2.977 sec/step)\n",
            "I0619 16:07:43.506312 139649090267008 learning.py:507] global step 87: loss = 0.6682 (2.981 sec/step)\n",
            "I0619 16:07:46.494220 139649090267008 learning.py:507] global step 88: loss = 0.8214 (2.985 sec/step)\n",
            "I0619 16:07:49.847790 139649090267008 learning.py:507] global step 88: loss = 0.6478 (3.352 sec/step)\n",
            "I0619 16:07:52.754956 139649090267008 learning.py:507] global step 88: loss = 0.7397 (2.905 sec/step)\n",
            "I0619 16:07:55.724299 139649090267008 learning.py:507] global step 88: loss = 0.6524 (2.967 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:07:58.678108 139649090267008 learning.py:507] global step 88: loss = 0.6714 (2.952 sec/step)\n",
            "I0619 16:08:01.636376 139649090267008 learning.py:507] global step 88: loss = 0.7468 (2.956 sec/step)\n",
            "I0619 16:08:04.599825 139649090267008 learning.py:507] global step 88: loss = 0.6207 (2.960 sec/step)\n",
            "I0619 16:08:07.602762 139649090267008 learning.py:507] global step 88: loss = 0.7346 (3.001 sec/step)\n",
            "I0619 16:08:10.578651 139649090267008 learning.py:507] global step 89: loss = 0.6049 (2.973 sec/step)\n",
            "I0619 16:08:13.583073 139649090267008 learning.py:507] global step 89: loss = 0.7340 (3.003 sec/step)\n",
            "I0619 16:08:16.585338 139649090267008 learning.py:507] global step 89: loss = 0.7044 (3.001 sec/step)\n",
            "I0619 16:08:19.623676 139649090267008 learning.py:507] global step 89: loss = 0.6017 (3.037 sec/step)\n",
            "I0619 16:08:24.482195 139649090267008 learning.py:507] global step 89: loss = 0.6673 (4.855 sec/step)\n",
            "I0619 16:08:25.941866 139646017689344 supervisor.py:1050] Recording summary at step 89.\n",
            "I0619 16:08:27.862092 139649090267008 learning.py:507] global step 89: loss = 0.5902 (3.374 sec/step)\n",
            "I0619 16:08:30.883047 139649090267008 learning.py:507] global step 89: loss = 0.6465 (3.019 sec/step)\n",
            "I0619 16:08:33.803653 139649090267008 learning.py:507] global step 89: loss = 0.7826 (2.919 sec/step)\n",
            "I0619 16:08:36.844424 139649090267008 learning.py:507] global step 90: loss = 0.5903 (3.038 sec/step)\n",
            "I0619 16:08:39.753221 139649090267008 learning.py:507] global step 90: loss = 0.6229 (2.907 sec/step)\n",
            "I0619 16:08:42.662090 139649090267008 learning.py:507] global step 90: loss = 0.7336 (2.907 sec/step)\n",
            "I0619 16:08:45.829222 139649090267008 learning.py:507] global step 90: loss = 0.6133 (3.165 sec/step)\n",
            "I0619 16:08:48.814028 139649090267008 learning.py:507] global step 90: loss = 0.6931 (2.983 sec/step)\n",
            "I0619 16:08:51.748560 139649090267008 learning.py:507] global step 90: loss = 0.7525 (2.933 sec/step)\n",
            "I0619 16:08:54.820413 139649090267008 learning.py:507] global step 90: loss = 0.6593 (3.070 sec/step)\n",
            "I0619 16:08:57.822102 139649090267008 learning.py:507] global step 90: loss = 0.6804 (3.000 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:09:00.791455 139649090267008 learning.py:507] global step 91: loss = 0.7673 (2.967 sec/step)\n",
            "I0619 16:09:03.878883 139649090267008 learning.py:507] global step 91: loss = 0.7585 (3.085 sec/step)\n",
            "I0619 16:09:06.886087 139649090267008 learning.py:507] global step 91: loss = 0.5727 (3.005 sec/step)\n",
            "I0619 16:09:09.885378 139649090267008 learning.py:507] global step 91: loss = 0.6940 (2.997 sec/step)\n",
            "I0619 16:09:12.893339 139649090267008 learning.py:507] global step 91: loss = 0.7442 (3.006 sec/step)\n",
            "I0619 16:09:15.917370 139649090267008 learning.py:507] global step 91: loss = 0.6798 (3.022 sec/step)\n",
            "I0619 16:09:18.907075 139649090267008 learning.py:507] global step 91: loss = 0.7809 (2.988 sec/step)\n",
            "I0619 16:09:21.904338 139649090267008 learning.py:507] global step 91: loss = 0.5995 (2.995 sec/step)\n",
            "I0619 16:09:24.803408 139649090267008 learning.py:507] global step 92: loss = 0.6493 (2.897 sec/step)\n",
            "I0619 16:09:27.766544 139649090267008 learning.py:507] global step 92: loss = 0.6517 (2.961 sec/step)\n",
            "I0619 16:09:30.722104 139649090267008 learning.py:507] global step 92: loss = 0.6679 (2.954 sec/step)\n",
            "I0619 16:09:33.785706 139649090267008 learning.py:507] global step 92: loss = 0.6952 (3.062 sec/step)\n",
            "I0619 16:09:36.765606 139649090267008 learning.py:507] global step 92: loss = 0.6751 (2.978 sec/step)\n",
            "I0619 16:09:39.714400 139649090267008 learning.py:507] global step 92: loss = 0.6187 (2.947 sec/step)\n",
            "I0619 16:09:42.704433 139649090267008 learning.py:507] global step 92: loss = 0.7226 (2.988 sec/step)\n",
            "I0619 16:09:45.694318 139649090267008 learning.py:507] global step 92: loss = 0.6798 (2.988 sec/step)\n",
            "I0619 16:09:48.695408 139649090267008 learning.py:507] global step 93: loss = 0.6326 (2.998 sec/step)\n",
            "I0619 16:09:51.776729 139649090267008 learning.py:507] global step 93: loss = 0.6267 (3.080 sec/step)\n",
            "I0619 16:09:54.726623 139649090267008 learning.py:507] global step 93: loss = 0.7241 (2.948 sec/step)\n",
            "I0619 16:09:57.686979 139649090267008 learning.py:507] global step 93: loss = 0.6796 (2.959 sec/step)\n",
            "I0619 16:10:00.721777 139649090267008 learning.py:507] global step 93: loss = 0.7211 (3.033 sec/step)\n",
            "I0619 16:10:03.747020 139649090267008 learning.py:507] global step 93: loss = 0.5468 (3.024 sec/step)\n",
            "I0619 16:10:06.691714 139649090267008 learning.py:507] global step 93: loss = 0.6179 (2.943 sec/step)\n",
            "I0619 16:10:09.682976 139649090267008 learning.py:507] global step 93: loss = 0.6984 (2.990 sec/step)\n",
            "I0619 16:10:12.660546 139649090267008 learning.py:507] global step 94: loss = 0.6804 (2.975 sec/step)\n",
            "I0619 16:10:15.655331 139649090267008 learning.py:507] global step 94: loss = 0.7511 (2.993 sec/step)\n",
            "I0619 16:10:18.634601 139649090267008 learning.py:507] global step 94: loss = 0.7729 (2.978 sec/step)\n",
            "I0619 16:10:20.933065 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 16:10:23.306115 139649090267008 learning.py:507] global step 94: loss = 0.7541 (4.660 sec/step)\n",
            "I0619 16:10:27.586351 139646017689344 supervisor.py:1050] Recording summary at step 94.\n",
            "I0619 16:10:28.480702 139649090267008 learning.py:507] global step 94: loss = 0.7726 (5.172 sec/step)\n",
            "I0619 16:10:31.499089 139649090267008 learning.py:507] global step 94: loss = 0.7905 (3.017 sec/step)\n",
            "I0619 16:10:34.461392 139649090267008 learning.py:507] global step 94: loss = 0.6131 (2.961 sec/step)\n",
            "I0619 16:10:37.479103 139649090267008 learning.py:507] global step 94: loss = 0.6938 (3.016 sec/step)\n",
            "I0619 16:10:40.450729 139649090267008 learning.py:507] global step 95: loss = 0.6501 (2.970 sec/step)\n",
            "I0619 16:10:43.462216 139649090267008 learning.py:507] global step 95: loss = 0.7943 (3.010 sec/step)\n",
            "I0619 16:10:46.453822 139649090267008 learning.py:507] global step 95: loss = 0.7844 (2.990 sec/step)\n",
            "I0619 16:10:49.391407 139649090267008 learning.py:507] global step 95: loss = 0.5867 (2.936 sec/step)\n",
            "I0619 16:10:52.365869 139649090267008 learning.py:507] global step 95: loss = 0.6077 (2.973 sec/step)\n",
            "I0619 16:10:55.229310 139649090267008 learning.py:507] global step 95: loss = 0.6179 (2.861 sec/step)\n",
            "I0619 16:10:58.199688 139649090267008 learning.py:507] global step 95: loss = 0.6555 (2.969 sec/step)\n",
            "I0619 16:11:01.159123 139649090267008 learning.py:507] global step 95: loss = 0.6869 (2.958 sec/step)\n",
            "I0619 16:11:04.090011 139649090267008 learning.py:507] global step 96: loss = 0.7685 (2.929 sec/step)\n",
            "I0619 16:11:07.039731 139649090267008 learning.py:507] global step 96: loss = 0.5894 (2.948 sec/step)\n",
            "I0619 16:11:09.962163 139649090267008 learning.py:507] global step 96: loss = 0.5858 (2.921 sec/step)\n",
            "I0619 16:11:12.930867 139649090267008 learning.py:507] global step 96: loss = 0.6466 (2.967 sec/step)\n",
            "I0619 16:11:15.867584 139649090267008 learning.py:507] global step 96: loss = 0.6058 (2.935 sec/step)\n",
            "I0619 16:11:18.783022 139649090267008 learning.py:507] global step 96: loss = 0.8426 (2.913 sec/step)\n",
            "I0619 16:11:21.818549 139649090267008 learning.py:507] global step 96: loss = 0.5825 (3.034 sec/step)\n",
            "I0619 16:11:24.699920 139649090267008 learning.py:507] global step 96: loss = 0.6313 (2.880 sec/step)\n",
            "I0619 16:11:27.674860 139649090267008 learning.py:507] global step 97: loss = 0.7230 (2.973 sec/step)\n",
            "I0619 16:11:30.607311 139649090267008 learning.py:507] global step 97: loss = 0.5885 (2.931 sec/step)\n",
            "I0619 16:11:33.595856 139649090267008 learning.py:507] global step 97: loss = 0.6260 (2.987 sec/step)\n",
            "I0619 16:11:36.536215 139649090267008 learning.py:507] global step 97: loss = 0.8074 (2.939 sec/step)\n",
            "I0619 16:11:39.670247 139649090267008 learning.py:507] global step 97: loss = 0.6904 (3.132 sec/step)\n",
            "I0619 16:11:42.651054 139649090267008 learning.py:507] global step 97: loss = 0.7208 (2.979 sec/step)\n",
            "I0619 16:11:45.596340 139649090267008 learning.py:507] global step 97: loss = 0.7109 (2.943 sec/step)\n",
            "I0619 16:11:48.514925 139649090267008 learning.py:507] global step 97: loss = 0.5899 (2.915 sec/step)\n",
            "I0619 16:11:51.467839 139649090267008 learning.py:507] global step 98: loss = 0.7559 (2.951 sec/step)\n",
            "I0619 16:11:54.432235 139649090267008 learning.py:507] global step 98: loss = 0.5809 (2.963 sec/step)\n",
            "I0619 16:11:57.349164 139649090267008 learning.py:507] global step 98: loss = 0.8010 (2.915 sec/step)\n",
            "I0619 16:12:00.291285 139649090267008 learning.py:507] global step 98: loss = 0.6583 (2.940 sec/step)\n",
            "I0619 16:12:03.176710 139649090267008 learning.py:507] global step 98: loss = 0.7071 (2.884 sec/step)\n",
            "I0619 16:12:06.098904 139649090267008 learning.py:507] global step 98: loss = 0.6154 (2.921 sec/step)\n",
            "I0619 16:12:09.041700 139649090267008 learning.py:507] global step 98: loss = 0.6664 (2.941 sec/step)\n",
            "I0619 16:12:11.972050 139649090267008 learning.py:507] global step 98: loss = 0.6096 (2.929 sec/step)\n",
            "I0619 16:12:15.094017 139649090267008 learning.py:507] global step 99: loss = 0.5894 (3.119 sec/step)\n",
            "I0619 16:12:18.025261 139649090267008 learning.py:507] global step 99: loss = 0.6027 (2.929 sec/step)\n",
            "I0619 16:12:20.967474 139649090267008 learning.py:507] global step 99: loss = 0.7455 (2.938 sec/step)\n",
            "I0619 16:12:25.970590 139646017689344 supervisor.py:1050] Recording summary at step 99.\n",
            "I0619 16:12:26.549882 139649090267008 learning.py:507] global step 99: loss = 0.6287 (4.933 sec/step)\n",
            "I0619 16:12:29.475204 139649090267008 learning.py:507] global step 99: loss = 0.7224 (2.923 sec/step)\n",
            "I0619 16:12:32.382215 139649090267008 learning.py:507] global step 99: loss = 0.7183 (2.905 sec/step)\n",
            "I0619 16:12:35.306200 139649090267008 learning.py:507] global step 99: loss = 0.6482 (2.922 sec/step)\n",
            "I0619 16:12:38.224341 139649090267008 learning.py:507] global step 99: loss = 0.6561 (2.916 sec/step)\n",
            "I0619 16:12:41.538781 139649090267008 learning.py:507] global step 100: loss = 0.6588 (3.312 sec/step)\n",
            "I0619 16:12:44.448637 139649090267008 learning.py:507] global step 100: loss = 0.7486 (2.908 sec/step)\n",
            "I0619 16:12:47.402725 139649090267008 learning.py:507] global step 100: loss = 0.6793 (2.952 sec/step)\n",
            "I0619 16:12:50.370305 139649090267008 learning.py:507] global step 100: loss = 0.6470 (2.966 sec/step)\n",
            "I0619 16:12:53.371865 139649090267008 learning.py:507] global step 100: loss = 0.5702 (3.000 sec/step)\n",
            "I0619 16:12:56.368246 139649090267008 learning.py:507] global step 100: loss = 0.7238 (2.995 sec/step)\n",
            "I0619 16:12:59.500547 139649090267008 learning.py:507] global step 100: loss = 0.7032 (3.130 sec/step)\n",
            "I0619 16:13:02.384249 139649090267008 learning.py:507] global step 100: loss = 0.6525 (2.882 sec/step)\n",
            "I0619 16:13:05.332400 139649090267008 learning.py:507] global step 101: loss = 0.5925 (2.945 sec/step)\n",
            "I0619 16:13:08.210258 139649090267008 learning.py:507] global step 101: loss = 0.6066 (2.876 sec/step)\n",
            "I0619 16:13:11.175443 139649090267008 learning.py:507] global step 101: loss = 0.6679 (2.964 sec/step)\n",
            "I0619 16:13:14.075635 139649090267008 learning.py:507] global step 101: loss = 0.5901 (2.898 sec/step)\n",
            "I0619 16:13:17.061504 139649090267008 learning.py:507] global step 101: loss = 0.6595 (2.984 sec/step)\n",
            "I0619 16:13:19.926493 139649090267008 learning.py:507] global step 101: loss = 0.6878 (2.863 sec/step)\n",
            "I0619 16:13:22.889039 139649090267008 learning.py:507] global step 101: loss = 0.5152 (2.961 sec/step)\n",
            "I0619 16:13:25.835406 139649090267008 learning.py:507] global step 101: loss = 0.6032 (2.945 sec/step)\n",
            "I0619 16:13:28.766578 139649090267008 learning.py:507] global step 102: loss = 0.6729 (2.929 sec/step)\n",
            "I0619 16:13:31.744750 139649090267008 learning.py:507] global step 102: loss = 0.6138 (2.976 sec/step)\n",
            "I0619 16:13:34.643143 139649090267008 learning.py:507] global step 102: loss = 0.7378 (2.896 sec/step)\n",
            "I0619 16:13:37.646830 139649090267008 learning.py:507] global step 102: loss = 0.6766 (3.002 sec/step)\n",
            "I0619 16:13:40.574140 139649090267008 learning.py:507] global step 102: loss = 0.6132 (2.926 sec/step)\n",
            "I0619 16:13:43.520209 139649090267008 learning.py:507] global step 102: loss = 0.5743 (2.944 sec/step)\n",
            "I0619 16:13:46.478363 139649090267008 learning.py:507] global step 102: loss = 0.5335 (2.956 sec/step)\n",
            "I0619 16:13:49.415842 139649090267008 learning.py:507] global step 102: loss = 0.6699 (2.936 sec/step)\n",
            "I0619 16:13:52.399476 139649090267008 learning.py:507] global step 103: loss = 0.6375 (2.981 sec/step)\n",
            "I0619 16:13:55.311833 139649090267008 learning.py:507] global step 103: loss = 0.7216 (2.910 sec/step)\n",
            "I0619 16:13:58.605503 139649090267008 learning.py:507] global step 103: loss = 0.6526 (3.292 sec/step)\n",
            "I0619 16:14:01.558765 139649090267008 learning.py:507] global step 103: loss = 0.5866 (2.952 sec/step)\n",
            "I0619 16:14:04.448838 139649090267008 learning.py:507] global step 103: loss = 0.6444 (2.888 sec/step)\n",
            "I0619 16:14:07.365764 139649090267008 learning.py:507] global step 103: loss = 0.6112 (2.915 sec/step)\n",
            "I0619 16:14:10.316043 139649090267008 learning.py:507] global step 103: loss = 0.6674 (2.949 sec/step)\n",
            "I0619 16:14:13.241894 139649090267008 learning.py:507] global step 103: loss = 0.6614 (2.924 sec/step)\n",
            "I0619 16:14:16.687865 139649090267008 learning.py:507] global step 104: loss = 0.7804 (3.444 sec/step)\n",
            "I0619 16:14:19.750898 139649090267008 learning.py:507] global step 104: loss = 0.6731 (3.061 sec/step)\n",
            "I0619 16:14:24.691548 139649090267008 learning.py:507] global step 104: loss = 0.5531 (4.928 sec/step)\n",
            "I0619 16:14:26.136187 139646017689344 supervisor.py:1050] Recording summary at step 104.\n",
            "I0619 16:14:27.947462 139649090267008 learning.py:507] global step 104: loss = 0.7430 (3.247 sec/step)\n",
            "I0619 16:14:30.916386 139649090267008 learning.py:507] global step 104: loss = 0.7221 (2.967 sec/step)\n",
            "I0619 16:14:33.795122 139649090267008 learning.py:507] global step 104: loss = 0.5715 (2.877 sec/step)\n",
            "I0619 16:14:36.691900 139649090267008 learning.py:507] global step 104: loss = 0.5891 (2.895 sec/step)\n",
            "I0619 16:14:39.663670 139649090267008 learning.py:507] global step 104: loss = 0.5865 (2.970 sec/step)\n",
            "I0619 16:14:42.543137 139649090267008 learning.py:507] global step 105: loss = 0.6299 (2.877 sec/step)\n",
            "I0619 16:14:45.427610 139649090267008 learning.py:507] global step 105: loss = 0.6729 (2.883 sec/step)\n",
            "I0619 16:14:48.442548 139649090267008 learning.py:507] global step 105: loss = 0.7555 (3.013 sec/step)\n",
            "I0619 16:14:51.384000 139649090267008 learning.py:507] global step 105: loss = 0.6221 (2.940 sec/step)\n",
            "I0619 16:14:54.260976 139649090267008 learning.py:507] global step 105: loss = 0.5727 (2.875 sec/step)\n",
            "I0619 16:14:57.187816 139649090267008 learning.py:507] global step 105: loss = 0.6757 (2.925 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:15:00.198812 139649090267008 learning.py:507] global step 105: loss = 0.6512 (3.009 sec/step)\n",
            "I0619 16:15:03.090578 139649090267008 learning.py:507] global step 105: loss = 0.6105 (2.890 sec/step)\n",
            "I0619 16:15:06.143456 139649090267008 learning.py:507] global step 106: loss = 0.5594 (3.051 sec/step)\n",
            "I0619 16:15:09.063561 139649090267008 learning.py:507] global step 106: loss = 0.7246 (2.918 sec/step)\n",
            "I0619 16:15:12.063444 139649090267008 learning.py:507] global step 106: loss = 0.6379 (2.998 sec/step)\n",
            "I0619 16:15:15.014883 139649090267008 learning.py:507] global step 106: loss = 0.5901 (2.950 sec/step)\n",
            "I0619 16:15:17.993938 139649090267008 learning.py:507] global step 106: loss = 0.6315 (2.977 sec/step)\n",
            "I0619 16:15:20.993613 139649090267008 learning.py:507] global step 106: loss = 0.5950 (2.998 sec/step)\n",
            "I0619 16:15:23.911290 139649090267008 learning.py:507] global step 106: loss = 0.5865 (2.916 sec/step)\n",
            "I0619 16:15:26.795655 139649090267008 learning.py:507] global step 106: loss = 0.6232 (2.883 sec/step)\n",
            "I0619 16:15:29.730486 139649090267008 learning.py:507] global step 107: loss = 0.6742 (2.933 sec/step)\n",
            "I0619 16:15:32.729113 139649090267008 learning.py:507] global step 107: loss = 0.5738 (2.997 sec/step)\n",
            "I0619 16:15:35.769950 139649090267008 learning.py:507] global step 107: loss = 0.7380 (3.039 sec/step)\n",
            "I0619 16:15:38.746798 139649090267008 learning.py:507] global step 107: loss = 0.5941 (2.975 sec/step)\n",
            "I0619 16:15:41.739555 139649090267008 learning.py:507] global step 107: loss = 0.6719 (2.991 sec/step)\n",
            "I0619 16:15:44.671943 139649090267008 learning.py:507] global step 107: loss = 0.5503 (2.931 sec/step)\n",
            "I0619 16:15:47.635425 139649090267008 learning.py:507] global step 107: loss = 0.5515 (2.962 sec/step)\n",
            "I0619 16:15:50.757935 139649090267008 learning.py:507] global step 107: loss = 0.7251 (3.121 sec/step)\n",
            "I0619 16:15:53.670536 139649090267008 learning.py:507] global step 108: loss = 0.6970 (2.910 sec/step)\n",
            "I0619 16:15:56.527409 139649090267008 learning.py:507] global step 108: loss = 0.6329 (2.855 sec/step)\n",
            "I0619 16:15:59.398720 139649090267008 learning.py:507] global step 108: loss = 0.6357 (2.870 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:16:02.356879 139649090267008 learning.py:507] global step 108: loss = 0.6551 (2.956 sec/step)\n",
            "I0619 16:16:05.269829 139649090267008 learning.py:507] global step 108: loss = 0.7096 (2.911 sec/step)\n",
            "I0619 16:16:08.711597 139649090267008 learning.py:507] global step 108: loss = 0.7782 (3.440 sec/step)\n",
            "I0619 16:16:11.673555 139649090267008 learning.py:507] global step 108: loss = 0.6381 (2.960 sec/step)\n",
            "I0619 16:16:14.629044 139649090267008 learning.py:507] global step 108: loss = 0.5644 (2.954 sec/step)\n",
            "I0619 16:16:17.559723 139649090267008 learning.py:507] global step 109: loss = 0.6459 (2.928 sec/step)\n",
            "I0619 16:16:20.480394 139649090267008 learning.py:507] global step 109: loss = 0.5748 (2.919 sec/step)\n",
            "I0619 16:16:25.963367 139649090267008 learning.py:507] global step 109: loss = 0.7234 (5.480 sec/step)\n",
            "I0619 16:16:26.060194 139646017689344 supervisor.py:1050] Recording summary at step 109.\n",
            "I0619 16:16:28.954030 139649090267008 learning.py:507] global step 109: loss = 0.5942 (2.986 sec/step)\n",
            "I0619 16:16:31.943292 139649090267008 learning.py:507] global step 109: loss = 0.5590 (2.988 sec/step)\n",
            "I0619 16:16:34.844477 139649090267008 learning.py:507] global step 109: loss = 0.7622 (2.900 sec/step)\n",
            "I0619 16:16:37.810238 139649090267008 learning.py:507] global step 109: loss = 0.6337 (2.964 sec/step)\n",
            "I0619 16:16:40.709784 139649090267008 learning.py:507] global step 109: loss = 0.6453 (2.898 sec/step)\n",
            "I0619 16:16:43.626661 139649090267008 learning.py:507] global step 110: loss = 0.6634 (2.914 sec/step)\n",
            "I0619 16:16:46.633950 139649090267008 learning.py:507] global step 110: loss = 0.6181 (3.006 sec/step)\n",
            "I0619 16:16:49.620493 139649090267008 learning.py:507] global step 110: loss = 0.5089 (2.985 sec/step)\n",
            "I0619 16:16:52.570312 139649090267008 learning.py:507] global step 110: loss = 0.7071 (2.948 sec/step)\n",
            "I0619 16:16:55.599739 139649090267008 learning.py:507] global step 110: loss = 0.6301 (3.028 sec/step)\n",
            "I0619 16:16:58.521925 139649090267008 learning.py:507] global step 110: loss = 0.6403 (2.921 sec/step)\n",
            "I0619 16:17:01.471281 139649090267008 learning.py:507] global step 110: loss = 0.6120 (2.948 sec/step)\n",
            "I0619 16:17:04.614154 139649090267008 learning.py:507] global step 110: loss = 0.5607 (3.141 sec/step)\n",
            "I0619 16:17:07.567552 139649090267008 learning.py:507] global step 111: loss = 0.6697 (2.952 sec/step)\n",
            "I0619 16:17:10.501037 139649090267008 learning.py:507] global step 111: loss = 0.5925 (2.932 sec/step)\n",
            "I0619 16:17:13.389704 139649090267008 learning.py:507] global step 111: loss = 0.6656 (2.887 sec/step)\n",
            "I0619 16:17:16.361951 139649090267008 learning.py:507] global step 111: loss = 0.5773 (2.971 sec/step)\n",
            "I0619 16:17:19.257872 139649090267008 learning.py:507] global step 111: loss = 0.6520 (2.894 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:17:22.221671 139649090267008 learning.py:507] global step 111: loss = 0.5779 (2.962 sec/step)\n",
            "I0619 16:17:25.117701 139649090267008 learning.py:507] global step 111: loss = 0.5992 (2.894 sec/step)\n",
            "I0619 16:17:27.978673 139649090267008 learning.py:507] global step 111: loss = 0.6642 (2.859 sec/step)\n",
            "I0619 16:17:30.898190 139649090267008 learning.py:507] global step 112: loss = 0.6222 (2.918 sec/step)\n",
            "I0619 16:17:33.904001 139649090267008 learning.py:507] global step 112: loss = 0.6995 (3.004 sec/step)\n",
            "I0619 16:17:36.880825 139649090267008 learning.py:507] global step 112: loss = 0.5118 (2.975 sec/step)\n",
            "I0619 16:17:39.845259 139649090267008 learning.py:507] global step 112: loss = 0.7525 (2.963 sec/step)\n",
            "I0619 16:17:42.782203 139649090267008 learning.py:507] global step 112: loss = 0.6052 (2.935 sec/step)\n",
            "I0619 16:17:45.685313 139649090267008 learning.py:507] global step 112: loss = 0.6505 (2.902 sec/step)\n",
            "I0619 16:17:48.546470 139649090267008 learning.py:507] global step 112: loss = 0.6152 (2.860 sec/step)\n",
            "I0619 16:17:51.504780 139649090267008 learning.py:507] global step 112: loss = 0.6681 (2.957 sec/step)\n",
            "I0619 16:17:54.419360 139649090267008 learning.py:507] global step 113: loss = 0.6376 (2.912 sec/step)\n",
            "I0619 16:17:57.340081 139649090267008 learning.py:507] global step 113: loss = 0.7586 (2.919 sec/step)\n",
            "I0619 16:18:00.239771 139649090267008 learning.py:507] global step 113: loss = 0.7158 (2.898 sec/step)\n",
            "I0619 16:18:03.151046 139649090267008 learning.py:507] global step 113: loss = 0.6351 (2.910 sec/step)\n",
            "I0619 16:18:06.100438 139649090267008 learning.py:507] global step 113: loss = 0.5515 (2.948 sec/step)\n",
            "I0619 16:18:08.994044 139649090267008 learning.py:507] global step 113: loss = 0.6671 (2.892 sec/step)\n",
            "I0619 16:18:11.985942 139649090267008 learning.py:507] global step 113: loss = 0.6805 (2.990 sec/step)\n",
            "I0619 16:18:14.907365 139649090267008 learning.py:507] global step 113: loss = 0.5238 (2.920 sec/step)\n",
            "I0619 16:18:17.823903 139649090267008 learning.py:507] global step 114: loss = 0.6051 (2.914 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:18:20.764808 139649090267008 learning.py:507] global step 114: loss = 0.5700 (2.939 sec/step)\n",
            "I0619 16:18:25.882594 139649090267008 learning.py:507] global step 114: loss = 0.6662 (5.115 sec/step)\n",
            "I0619 16:18:26.447595 139646017689344 supervisor.py:1050] Recording summary at step 114.\n",
            "I0619 16:18:28.988976 139649090267008 learning.py:507] global step 114: loss = 0.6700 (3.105 sec/step)\n",
            "I0619 16:18:31.922261 139649090267008 learning.py:507] global step 114: loss = 0.5692 (2.932 sec/step)\n",
            "I0619 16:18:34.809407 139649090267008 learning.py:507] global step 114: loss = 0.6506 (2.885 sec/step)\n",
            "I0619 16:18:37.748933 139649090267008 learning.py:507] global step 114: loss = 0.6407 (2.937 sec/step)\n",
            "I0619 16:18:40.686393 139649090267008 learning.py:507] global step 114: loss = 0.6596 (2.936 sec/step)\n",
            "I0619 16:18:43.740259 139649090267008 learning.py:507] global step 115: loss = 0.6545 (3.052 sec/step)\n",
            "I0619 16:18:47.248090 139649090267008 learning.py:507] global step 115: loss = 0.5603 (3.506 sec/step)\n",
            "I0619 16:18:50.152719 139649090267008 learning.py:507] global step 115: loss = 0.6481 (2.903 sec/step)\n",
            "I0619 16:18:53.045558 139649090267008 learning.py:507] global step 115: loss = 0.5618 (2.891 sec/step)\n",
            "I0619 16:18:56.058487 139649090267008 learning.py:507] global step 115: loss = 0.5768 (3.011 sec/step)\n",
            "I0619 16:18:59.012453 139649090267008 learning.py:507] global step 115: loss = 0.7458 (2.952 sec/step)\n",
            "I0619 16:19:02.119516 139649090267008 learning.py:507] global step 115: loss = 0.5690 (3.105 sec/step)\n",
            "I0619 16:19:05.514748 139649090267008 learning.py:507] global step 115: loss = 0.6240 (3.394 sec/step)\n",
            "I0619 16:19:08.455811 139649090267008 learning.py:507] global step 116: loss = 0.7845 (2.939 sec/step)\n",
            "I0619 16:19:11.386620 139649090267008 learning.py:507] global step 116: loss = 0.6547 (2.929 sec/step)\n",
            "I0619 16:19:14.274806 139649090267008 learning.py:507] global step 116: loss = 0.5527 (2.887 sec/step)\n",
            "I0619 16:19:17.226594 139649090267008 learning.py:507] global step 116: loss = 0.7280 (2.950 sec/step)\n",
            "I0619 16:19:20.160553 139649090267008 learning.py:507] global step 116: loss = 0.6281 (2.932 sec/step)\n",
            "I0619 16:19:23.368743 139649090267008 learning.py:507] global step 116: loss = 0.6037 (3.206 sec/step)\n",
            "I0619 16:19:26.227515 139649090267008 learning.py:507] global step 116: loss = 0.7334 (2.857 sec/step)\n",
            "I0619 16:19:29.121290 139649090267008 learning.py:507] global step 116: loss = 0.5931 (2.892 sec/step)\n",
            "I0619 16:19:32.123736 139649090267008 learning.py:507] global step 117: loss = 0.5509 (3.001 sec/step)\n",
            "I0619 16:19:35.108069 139649090267008 learning.py:507] global step 117: loss = 0.5199 (2.982 sec/step)\n",
            "I0619 16:19:38.016785 139649090267008 learning.py:507] global step 117: loss = 0.5961 (2.907 sec/step)\n",
            "I0619 16:19:41.318108 139649090267008 learning.py:507] global step 117: loss = 0.5452 (3.299 sec/step)\n",
            "I0619 16:19:44.246736 139649090267008 learning.py:507] global step 117: loss = 0.6991 (2.927 sec/step)\n",
            "I0619 16:19:47.193120 139649090267008 learning.py:507] global step 117: loss = 0.5849 (2.945 sec/step)\n",
            "I0619 16:19:50.075774 139649090267008 learning.py:507] global step 117: loss = 0.7751 (2.881 sec/step)\n",
            "I0619 16:19:53.030239 139649090267008 learning.py:507] global step 117: loss = 0.5751 (2.953 sec/step)\n",
            "I0619 16:19:55.945278 139649090267008 learning.py:507] global step 118: loss = 0.5740 (2.913 sec/step)\n",
            "I0619 16:19:58.908126 139649090267008 learning.py:507] global step 118: loss = 0.6111 (2.961 sec/step)\n",
            "I0619 16:20:01.759633 139649090267008 learning.py:507] global step 118: loss = 0.6877 (2.850 sec/step)\n",
            "I0619 16:20:04.652118 139649090267008 learning.py:507] global step 118: loss = 0.7623 (2.891 sec/step)\n",
            "I0619 16:20:07.557535 139649090267008 learning.py:507] global step 118: loss = 0.6199 (2.902 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:20:10.517361 139649090267008 learning.py:507] global step 118: loss = 0.6347 (2.958 sec/step)\n",
            "I0619 16:20:13.451436 139649090267008 learning.py:507] global step 118: loss = 0.6098 (2.932 sec/step)\n",
            "I0619 16:20:16.390786 139649090267008 learning.py:507] global step 118: loss = 0.6355 (2.938 sec/step)\n",
            "I0619 16:20:19.251787 139649090267008 learning.py:507] global step 119: loss = 0.5366 (2.859 sec/step)\n",
            "I0619 16:20:20.931305 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "W0619 16:20:21.018244 139646034474752 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0619 16:20:25.046519 139649090267008 learning.py:507] global step 119: loss = 0.5160 (5.783 sec/step)\n",
            "I0619 16:20:26.716701 139646017689344 supervisor.py:1050] Recording summary at step 119.\n",
            "I0619 16:20:28.990653 139649090267008 learning.py:507] global step 119: loss = 0.6816 (3.427 sec/step)\n",
            "I0619 16:20:31.939855 139649090267008 learning.py:507] global step 119: loss = 0.7394 (2.948 sec/step)\n",
            "I0619 16:20:34.868873 139649090267008 learning.py:507] global step 119: loss = 0.6172 (2.927 sec/step)\n",
            "I0619 16:20:37.716824 139649090267008 learning.py:507] global step 119: loss = 0.6417 (2.846 sec/step)\n",
            "I0619 16:20:40.615139 139649090267008 learning.py:507] global step 119: loss = 0.5892 (2.897 sec/step)\n",
            "I0619 16:20:43.662273 139649090267008 learning.py:507] global step 119: loss = 0.7811 (3.046 sec/step)\n",
            "I0619 16:20:46.591683 139649090267008 learning.py:507] global step 120: loss = 0.6131 (2.927 sec/step)\n",
            "I0619 16:20:49.492117 139649090267008 learning.py:507] global step 120: loss = 0.6333 (2.899 sec/step)\n",
            "I0619 16:20:52.417915 139649090267008 learning.py:507] global step 120: loss = 0.6031 (2.924 sec/step)\n",
            "I0619 16:20:55.398879 139649090267008 learning.py:507] global step 120: loss = 0.5578 (2.979 sec/step)\n",
            "I0619 16:20:58.352778 139649090267008 learning.py:507] global step 120: loss = 0.5789 (2.952 sec/step)\n",
            "I0619 16:21:01.268541 139649090267008 learning.py:507] global step 120: loss = 0.5723 (2.914 sec/step)\n",
            "I0619 16:21:04.252240 139649090267008 learning.py:507] global step 120: loss = 0.5850 (2.982 sec/step)\n",
            "I0619 16:21:07.170287 139649090267008 learning.py:507] global step 120: loss = 0.5804 (2.916 sec/step)\n",
            "I0619 16:21:10.037715 139649090267008 learning.py:507] global step 121: loss = 0.6315 (2.865 sec/step)\n",
            "I0619 16:21:12.949147 139649090267008 learning.py:507] global step 121: loss = 0.5710 (2.910 sec/step)\n",
            "I0619 16:21:15.907071 139649090267008 learning.py:507] global step 121: loss = 0.5947 (2.956 sec/step)\n",
            "I0619 16:21:18.842813 139649090267008 learning.py:507] global step 121: loss = 0.6275 (2.934 sec/step)\n",
            "I0619 16:21:21.800250 139649090267008 learning.py:507] global step 121: loss = 0.6793 (2.956 sec/step)\n",
            "I0619 16:21:24.747895 139649090267008 learning.py:507] global step 121: loss = 0.6373 (2.946 sec/step)\n",
            "I0619 16:21:27.683704 139649090267008 learning.py:507] global step 121: loss = 0.5561 (2.934 sec/step)\n",
            "I0619 16:21:30.577413 139649090267008 learning.py:507] global step 121: loss = 0.5413 (2.892 sec/step)\n",
            "I0619 16:21:33.499178 139649090267008 learning.py:507] global step 122: loss = 0.6399 (2.920 sec/step)\n",
            "I0619 16:21:36.362622 139649090267008 learning.py:507] global step 122: loss = 0.6079 (2.862 sec/step)\n",
            "I0619 16:21:39.274773 139649090267008 learning.py:507] global step 122: loss = 0.6035 (2.910 sec/step)\n",
            "I0619 16:21:42.172874 139649090267008 learning.py:507] global step 122: loss = 0.6790 (2.896 sec/step)\n",
            "I0619 16:21:45.351642 139649090267008 learning.py:507] global step 122: loss = 0.6337 (3.177 sec/step)\n",
            "I0619 16:21:48.230681 139649090267008 learning.py:507] global step 122: loss = 0.6094 (2.877 sec/step)\n",
            "I0619 16:21:51.155079 139649090267008 learning.py:507] global step 122: loss = 0.6729 (2.923 sec/step)\n",
            "I0619 16:21:54.049556 139649090267008 learning.py:507] global step 122: loss = 0.6751 (2.893 sec/step)\n",
            "I0619 16:21:56.972351 139649090267008 learning.py:507] global step 123: loss = 0.7422 (2.920 sec/step)\n",
            "I0619 16:21:59.996782 139649090267008 learning.py:507] global step 123: loss = 0.5661 (3.023 sec/step)\n",
            "I0619 16:22:03.247944 139649090267008 learning.py:507] global step 123: loss = 0.5872 (3.249 sec/step)\n",
            "I0619 16:22:06.158019 139649090267008 learning.py:507] global step 123: loss = 0.5983 (2.908 sec/step)\n",
            "I0619 16:22:09.061029 139649090267008 learning.py:507] global step 123: loss = 0.6268 (2.902 sec/step)\n",
            "I0619 16:22:11.882120 139649090267008 learning.py:507] global step 123: loss = 0.5752 (2.819 sec/step)\n",
            "I0619 16:22:14.782058 139649090267008 learning.py:507] global step 123: loss = 0.5747 (2.898 sec/step)\n",
            "I0619 16:22:17.857431 139649090267008 learning.py:507] global step 123: loss = 0.6070 (3.074 sec/step)\n",
            "I0619 16:22:20.739780 139649090267008 learning.py:507] global step 124: loss = 0.6180 (2.881 sec/step)\n",
            "I0619 16:22:25.759768 139649090267008 learning.py:507] global step 124: loss = 0.7718 (5.017 sec/step)\n",
            "I0619 16:22:25.797862 139646017689344 supervisor.py:1050] Recording summary at step 124.\n",
            "I0619 16:22:28.750192 139649090267008 learning.py:507] global step 124: loss = 0.5595 (2.988 sec/step)\n",
            "I0619 16:22:31.626628 139649090267008 learning.py:507] global step 124: loss = 0.5897 (2.875 sec/step)\n",
            "I0619 16:22:34.519444 139649090267008 learning.py:507] global step 124: loss = 0.6472 (2.891 sec/step)\n",
            "I0619 16:22:37.478987 139649090267008 learning.py:507] global step 124: loss = 0.7604 (2.958 sec/step)\n",
            "I0619 16:22:40.381470 139649090267008 learning.py:507] global step 124: loss = 0.6129 (2.901 sec/step)\n",
            "I0619 16:22:43.312182 139649090267008 learning.py:507] global step 124: loss = 0.6077 (2.929 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:22:46.223638 139649090267008 learning.py:507] global step 125: loss = 0.6636 (2.909 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:22:49.162684 139649090267008 learning.py:507] global step 125: loss = 0.7099 (2.937 sec/step)\n",
            "I0619 16:22:52.099495 139649090267008 learning.py:507] global step 125: loss = 0.6453 (2.935 sec/step)\n",
            "I0619 16:22:54.985433 139649090267008 learning.py:507] global step 125: loss = 0.5641 (2.883 sec/step)\n",
            "I0619 16:22:57.826761 139649090267008 learning.py:507] global step 125: loss = 0.6710 (2.840 sec/step)\n",
            "I0619 16:23:00.672616 139649090267008 learning.py:507] global step 125: loss = 0.5832 (2.844 sec/step)\n",
            "I0619 16:23:03.625941 139649090267008 learning.py:507] global step 125: loss = 0.6213 (2.951 sec/step)\n",
            "I0619 16:23:06.586069 139649090267008 learning.py:507] global step 125: loss = 0.5599 (2.958 sec/step)\n",
            "I0619 16:23:09.516458 139649090267008 learning.py:507] global step 126: loss = 0.6938 (2.929 sec/step)\n",
            "I0619 16:23:12.430487 139649090267008 learning.py:507] global step 126: loss = 0.5557 (2.912 sec/step)\n",
            "I0619 16:23:15.339431 139649090267008 learning.py:507] global step 126: loss = 0.6190 (2.907 sec/step)\n",
            "I0619 16:23:18.273153 139649090267008 learning.py:507] global step 126: loss = 0.5681 (2.932 sec/step)\n",
            "I0619 16:23:21.169808 139649090267008 learning.py:507] global step 126: loss = 0.5962 (2.895 sec/step)\n",
            "I0619 16:23:24.234715 139649090267008 learning.py:507] global step 126: loss = 0.5832 (3.063 sec/step)\n",
            "I0619 16:23:27.093931 139649090267008 learning.py:507] global step 126: loss = 0.6089 (2.858 sec/step)\n",
            "I0619 16:23:30.051178 139649090267008 learning.py:507] global step 126: loss = 0.5080 (2.955 sec/step)\n",
            "I0619 16:23:33.002338 139649090267008 learning.py:507] global step 127: loss = 0.7040 (2.949 sec/step)\n",
            "I0619 16:23:35.944575 139649090267008 learning.py:507] global step 127: loss = 0.5724 (2.940 sec/step)\n",
            "I0619 16:23:38.910169 139649090267008 learning.py:507] global step 127: loss = 0.5440 (2.964 sec/step)\n",
            "I0619 16:23:41.823632 139649090267008 learning.py:507] global step 127: loss = 0.6065 (2.912 sec/step)\n",
            "I0619 16:23:44.781291 139649090267008 learning.py:507] global step 127: loss = 0.6648 (2.956 sec/step)\n",
            "I0619 16:23:47.704475 139649090267008 learning.py:507] global step 127: loss = 0.7906 (2.922 sec/step)\n",
            "I0619 16:23:50.573907 139649090267008 learning.py:507] global step 127: loss = 0.5726 (2.868 sec/step)\n",
            "I0619 16:23:53.453176 139649090267008 learning.py:507] global step 127: loss = 0.5995 (2.878 sec/step)\n",
            "I0619 16:23:56.453534 139649090267008 learning.py:507] global step 128: loss = 0.5977 (2.998 sec/step)\n",
            "I0619 16:23:59.386407 139649090267008 learning.py:507] global step 128: loss = 0.5298 (2.931 sec/step)\n",
            "I0619 16:24:02.451934 139649090267008 learning.py:507] global step 128: loss = 0.6126 (3.063 sec/step)\n",
            "I0619 16:24:05.491058 139649090267008 learning.py:507] global step 128: loss = 0.6075 (3.037 sec/step)\n",
            "I0619 16:24:08.363701 139649090267008 learning.py:507] global step 128: loss = 0.6615 (2.871 sec/step)\n",
            "I0619 16:24:11.281265 139649090267008 learning.py:507] global step 128: loss = 0.6215 (2.916 sec/step)\n",
            "I0619 16:24:14.156233 139649090267008 learning.py:507] global step 128: loss = 0.6072 (2.873 sec/step)\n",
            "I0619 16:24:17.008752 139649090267008 learning.py:507] global step 128: loss = 0.6007 (2.851 sec/step)\n",
            "I0619 16:24:19.962383 139649090267008 learning.py:507] global step 129: loss = 0.6273 (2.951 sec/step)\n",
            "I0619 16:24:24.924181 139649090267008 learning.py:507] global step 129: loss = 0.6717 (4.959 sec/step)\n",
            "I0619 16:24:26.101841 139646017689344 supervisor.py:1050] Recording summary at step 129.\n",
            "I0619 16:24:28.032981 139649090267008 learning.py:507] global step 129: loss = 0.5457 (3.102 sec/step)\n",
            "I0619 16:24:30.925143 139649090267008 learning.py:507] global step 129: loss = 0.5218 (2.890 sec/step)\n",
            "I0619 16:24:33.800213 139649090267008 learning.py:507] global step 129: loss = 0.4703 (2.873 sec/step)\n",
            "I0619 16:24:36.710667 139649090267008 learning.py:507] global step 129: loss = 0.5966 (2.909 sec/step)\n",
            "I0619 16:24:39.605793 139649090267008 learning.py:507] global step 129: loss = 0.5645 (2.893 sec/step)\n",
            "I0619 16:24:42.619115 139649090267008 learning.py:507] global step 129: loss = 0.5890 (3.012 sec/step)\n",
            "I0619 16:24:45.544538 139649090267008 learning.py:507] global step 130: loss = 0.6038 (2.924 sec/step)\n",
            "I0619 16:24:48.515176 139649090267008 learning.py:507] global step 130: loss = 0.5763 (2.969 sec/step)\n",
            "I0619 16:24:51.442506 139649090267008 learning.py:507] global step 130: loss = 0.5407 (2.926 sec/step)\n",
            "I0619 16:24:54.404906 139649090267008 learning.py:507] global step 130: loss = 0.6390 (2.961 sec/step)\n",
            "I0619 16:24:57.390325 139649090267008 learning.py:507] global step 130: loss = 0.5306 (2.984 sec/step)\n",
            "I0619 16:25:00.336874 139649090267008 learning.py:507] global step 130: loss = 0.5334 (2.945 sec/step)\n",
            "I0619 16:25:03.363680 139649090267008 learning.py:507] global step 130: loss = 0.5828 (3.025 sec/step)\n",
            "I0619 16:25:06.287724 139649090267008 learning.py:507] global step 130: loss = 0.6388 (2.922 sec/step)\n",
            "I0619 16:25:09.478346 139649090267008 learning.py:507] global step 131: loss = 0.6373 (3.188 sec/step)\n",
            "I0619 16:25:12.364448 139649090267008 learning.py:507] global step 131: loss = 0.5390 (2.884 sec/step)\n",
            "I0619 16:25:15.347533 139649090267008 learning.py:507] global step 131: loss = 0.4869 (2.981 sec/step)\n",
            "I0619 16:25:18.228054 139649090267008 learning.py:507] global step 131: loss = 0.7631 (2.879 sec/step)\n",
            "I0619 16:25:21.159836 139649090267008 learning.py:507] global step 131: loss = 0.5337 (2.930 sec/step)\n",
            "I0619 16:25:24.047601 139649090267008 learning.py:507] global step 131: loss = 0.5426 (2.886 sec/step)\n",
            "I0619 16:25:27.314054 139649090267008 learning.py:507] global step 131: loss = 0.5174 (3.265 sec/step)\n",
            "I0619 16:25:30.180826 139649090267008 learning.py:507] global step 131: loss = 0.4903 (2.865 sec/step)\n",
            "I0619 16:25:33.026299 139649090267008 learning.py:507] global step 132: loss = 0.7005 (2.843 sec/step)\n",
            "I0619 16:25:35.987097 139649090267008 learning.py:507] global step 132: loss = 0.6698 (2.959 sec/step)\n",
            "I0619 16:25:38.900259 139649090267008 learning.py:507] global step 132: loss = 0.5755 (2.912 sec/step)\n",
            "I0619 16:25:41.783272 139649090267008 learning.py:507] global step 132: loss = 0.6771 (2.881 sec/step)\n",
            "I0619 16:25:44.674800 139649090267008 learning.py:507] global step 132: loss = 0.6849 (2.890 sec/step)\n",
            "I0619 16:25:47.594808 139649090267008 learning.py:507] global step 132: loss = 0.5118 (2.918 sec/step)\n",
            "I0619 16:25:50.450210 139649090267008 learning.py:507] global step 132: loss = 0.5373 (2.854 sec/step)\n",
            "I0619 16:25:53.294073 139649090267008 learning.py:507] global step 132: loss = 0.5202 (2.842 sec/step)\n",
            "I0619 16:25:56.191696 139649090267008 learning.py:507] global step 133: loss = 0.5404 (2.896 sec/step)\n",
            "I0619 16:25:59.068264 139649090267008 learning.py:507] global step 133: loss = 0.7968 (2.875 sec/step)\n",
            "I0619 16:26:02.005280 139649090267008 learning.py:507] global step 133: loss = 0.5963 (2.935 sec/step)\n",
            "I0619 16:26:04.899790 139649090267008 learning.py:507] global step 133: loss = 0.5275 (2.893 sec/step)\n",
            "I0619 16:26:07.814829 139649090267008 learning.py:507] global step 133: loss = 0.5812 (2.913 sec/step)\n",
            "I0619 16:26:10.744090 139649090267008 learning.py:507] global step 133: loss = 0.5759 (2.928 sec/step)\n",
            "I0619 16:26:13.589435 139649090267008 learning.py:507] global step 133: loss = 0.7694 (2.844 sec/step)\n",
            "I0619 16:26:16.517499 139649090267008 learning.py:507] global step 133: loss = 0.7287 (2.926 sec/step)\n",
            "I0619 16:26:19.449130 139649090267008 learning.py:507] global step 134: loss = 0.6672 (2.926 sec/step)\n",
            "I0619 16:26:24.447381 139649090267008 learning.py:507] global step 134: loss = 0.7247 (4.989 sec/step)\n",
            "I0619 16:26:26.193314 139646017689344 supervisor.py:1050] Recording summary at step 134.\n",
            "I0619 16:26:27.774724 139649090267008 learning.py:507] global step 134: loss = 0.5843 (3.326 sec/step)\n",
            "I0619 16:26:30.682448 139649090267008 learning.py:507] global step 134: loss = 0.5798 (2.906 sec/step)\n",
            "I0619 16:26:33.593847 139649090267008 learning.py:507] global step 134: loss = 0.5874 (2.909 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:26:36.572289 139649090267008 learning.py:507] global step 134: loss = 0.5866 (2.977 sec/step)\n",
            "I0619 16:26:39.733040 139649090267008 learning.py:507] global step 134: loss = 0.7082 (3.159 sec/step)\n",
            "I0619 16:26:42.743740 139649090267008 learning.py:507] global step 134: loss = 0.5393 (3.009 sec/step)\n",
            "I0619 16:26:45.608327 139649090267008 learning.py:507] global step 135: loss = 0.5762 (2.862 sec/step)\n",
            "I0619 16:26:48.436654 139649090267008 learning.py:507] global step 135: loss = 0.6108 (2.827 sec/step)\n",
            "I0619 16:26:51.278322 139649090267008 learning.py:507] global step 135: loss = 0.6003 (2.840 sec/step)\n",
            "I0619 16:26:54.171318 139649090267008 learning.py:507] global step 135: loss = 0.5362 (2.891 sec/step)\n",
            "I0619 16:26:57.422161 139649090267008 learning.py:507] global step 135: loss = 0.5589 (3.249 sec/step)\n",
            "I0619 16:27:00.326973 139649090267008 learning.py:507] global step 135: loss = 0.5958 (2.903 sec/step)\n",
            "I0619 16:27:03.220190 139649090267008 learning.py:507] global step 135: loss = 0.5223 (2.892 sec/step)\n",
            "I0619 16:27:06.117716 139649090267008 learning.py:507] global step 135: loss = 0.5924 (2.896 sec/step)\n",
            "I0619 16:27:09.034220 139649090267008 learning.py:507] global step 136: loss = 0.6202 (2.914 sec/step)\n",
            "I0619 16:27:11.972414 139649090267008 learning.py:507] global step 136: loss = 0.5568 (2.937 sec/step)\n",
            "I0619 16:27:14.850226 139649090267008 learning.py:507] global step 136: loss = 0.5750 (2.876 sec/step)\n",
            "I0619 16:27:17.726911 139649090267008 learning.py:507] global step 136: loss = 0.5441 (2.875 sec/step)\n",
            "I0619 16:27:20.596943 139649090267008 learning.py:507] global step 136: loss = 0.7024 (2.868 sec/step)\n",
            "I0619 16:27:23.461889 139649090267008 learning.py:507] global step 136: loss = 0.5370 (2.863 sec/step)\n",
            "I0619 16:27:26.494885 139649090267008 learning.py:507] global step 136: loss = 0.7285 (3.031 sec/step)\n",
            "I0619 16:27:29.440241 139649090267008 learning.py:507] global step 136: loss = 0.5943 (2.944 sec/step)\n",
            "I0619 16:27:32.377628 139649090267008 learning.py:507] global step 137: loss = 0.6380 (2.935 sec/step)\n",
            "I0619 16:27:35.341293 139649090267008 learning.py:507] global step 137: loss = 0.6056 (2.962 sec/step)\n",
            "I0619 16:27:38.225558 139649090267008 learning.py:507] global step 137: loss = 0.5323 (2.883 sec/step)\n",
            "I0619 16:27:41.144256 139649090267008 learning.py:507] global step 137: loss = 0.5734 (2.917 sec/step)\n",
            "I0619 16:27:44.243937 139649090267008 learning.py:507] global step 137: loss = 0.6237 (3.098 sec/step)\n",
            "I0619 16:27:47.090849 139649090267008 learning.py:507] global step 137: loss = 0.5826 (2.845 sec/step)\n",
            "I0619 16:27:50.100631 139649090267008 learning.py:507] global step 137: loss = 0.5949 (3.008 sec/step)\n",
            "I0619 16:27:53.052907 139649090267008 learning.py:507] global step 137: loss = 0.5390 (2.951 sec/step)\n",
            "I0619 16:27:55.886374 139649090267008 learning.py:507] global step 138: loss = 0.5364 (2.831 sec/step)\n",
            "I0619 16:27:58.817812 139649090267008 learning.py:507] global step 138: loss = 0.5890 (2.930 sec/step)\n",
            "I0619 16:28:01.775501 139649090267008 learning.py:507] global step 138: loss = 0.6255 (2.956 sec/step)\n",
            "I0619 16:28:04.701937 139649090267008 learning.py:507] global step 138: loss = 0.6177 (2.925 sec/step)\n",
            "I0619 16:28:07.606046 139649090267008 learning.py:507] global step 138: loss = 0.4547 (2.902 sec/step)\n",
            "I0619 16:28:10.551294 139649090267008 learning.py:507] global step 138: loss = 0.5512 (2.944 sec/step)\n",
            "I0619 16:28:13.453403 139649090267008 learning.py:507] global step 138: loss = 0.5888 (2.901 sec/step)\n",
            "I0619 16:28:16.330597 139649090267008 learning.py:507] global step 138: loss = 0.6021 (2.876 sec/step)\n",
            "I0619 16:28:19.234057 139649090267008 learning.py:507] global step 139: loss = 0.7128 (2.902 sec/step)\n",
            "I0619 16:28:23.731246 139649090267008 learning.py:507] global step 139: loss = 0.6308 (4.481 sec/step)\n",
            "I0619 16:28:26.261268 139646017689344 supervisor.py:1050] Recording summary at step 139.\n",
            "I0619 16:28:27.416770 139649090267008 learning.py:507] global step 139: loss = 0.5331 (3.683 sec/step)\n",
            "I0619 16:28:30.282170 139649090267008 learning.py:507] global step 139: loss = 0.5058 (2.864 sec/step)\n",
            "I0619 16:28:33.182614 139649090267008 learning.py:507] global step 139: loss = 0.4992 (2.899 sec/step)\n",
            "I0619 16:28:36.062225 139649090267008 learning.py:507] global step 139: loss = 0.5088 (2.878 sec/step)\n",
            "I0619 16:28:38.987591 139649090267008 learning.py:507] global step 139: loss = 0.4942 (2.924 sec/step)\n",
            "I0619 16:28:42.145112 139649090267008 learning.py:507] global step 139: loss = 0.6257 (3.156 sec/step)\n",
            "I0619 16:28:45.335592 139649090267008 learning.py:507] global step 140: loss = 0.5814 (3.188 sec/step)\n",
            "I0619 16:28:48.262372 139649090267008 learning.py:507] global step 140: loss = 0.6025 (2.925 sec/step)\n",
            "I0619 16:28:51.153774 139649090267008 learning.py:507] global step 140: loss = 0.5429 (2.890 sec/step)\n",
            "I0619 16:28:54.090289 139649090267008 learning.py:507] global step 140: loss = 0.5610 (2.935 sec/step)\n",
            "I0619 16:28:56.983767 139649090267008 learning.py:507] global step 140: loss = 0.5825 (2.892 sec/step)\n",
            "I0619 16:29:00.315176 139649090267008 learning.py:507] global step 140: loss = 0.6196 (3.330 sec/step)\n",
            "I0619 16:29:03.345900 139649090267008 learning.py:507] global step 140: loss = 0.6423 (3.029 sec/step)\n",
            "I0619 16:29:06.355026 139649090267008 learning.py:507] global step 140: loss = 0.5350 (3.007 sec/step)\n",
            "I0619 16:29:09.264640 139649090267008 learning.py:507] global step 141: loss = 0.5165 (2.908 sec/step)\n",
            "I0619 16:29:12.254635 139649090267008 learning.py:507] global step 141: loss = 0.5810 (2.988 sec/step)\n",
            "I0619 16:29:15.119601 139649090267008 learning.py:507] global step 141: loss = 0.5307 (2.863 sec/step)\n",
            "I0619 16:29:18.015705 139649090267008 learning.py:507] global step 141: loss = 0.5727 (2.895 sec/step)\n",
            "I0619 16:29:20.866990 139649090267008 learning.py:507] global step 141: loss = 0.5225 (2.850 sec/step)\n",
            "I0619 16:29:23.782245 139649090267008 learning.py:507] global step 141: loss = 0.5637 (2.914 sec/step)\n",
            "I0619 16:29:26.696355 139649090267008 learning.py:507] global step 141: loss = 0.5703 (2.913 sec/step)\n",
            "I0619 16:29:29.551598 139649090267008 learning.py:507] global step 141: loss = 0.5689 (2.854 sec/step)\n",
            "I0619 16:29:32.395718 139649090267008 learning.py:507] global step 142: loss = 0.5775 (2.842 sec/step)\n",
            "I0619 16:29:35.342762 139649090267008 learning.py:507] global step 142: loss = 0.5610 (2.945 sec/step)\n",
            "I0619 16:29:38.286863 139649090267008 learning.py:507] global step 142: loss = 0.5844 (2.942 sec/step)\n",
            "I0619 16:29:41.175933 139649090267008 learning.py:507] global step 142: loss = 0.4995 (2.887 sec/step)\n",
            "I0619 16:29:44.064432 139649090267008 learning.py:507] global step 142: loss = 0.6146 (2.887 sec/step)\n",
            "I0619 16:29:46.955106 139649090267008 learning.py:507] global step 142: loss = 0.5186 (2.889 sec/step)\n",
            "I0619 16:29:50.018634 139649090267008 learning.py:507] global step 142: loss = 0.6224 (3.062 sec/step)\n",
            "I0619 16:29:52.999340 139649090267008 learning.py:507] global step 142: loss = 0.6010 (2.979 sec/step)\n",
            "I0619 16:29:55.979039 139649090267008 learning.py:507] global step 143: loss = 0.5573 (2.977 sec/step)\n",
            "I0619 16:29:58.997696 139649090267008 learning.py:507] global step 143: loss = 0.5995 (3.017 sec/step)\n",
            "I0619 16:30:01.964412 139649090267008 learning.py:507] global step 143: loss = 0.5209 (2.964 sec/step)\n",
            "I0619 16:30:04.878932 139649090267008 learning.py:507] global step 143: loss = 0.6236 (2.913 sec/step)\n",
            "I0619 16:30:07.991023 139649090267008 learning.py:507] global step 143: loss = 0.5918 (3.110 sec/step)\n",
            "I0619 16:30:10.909536 139649090267008 learning.py:507] global step 143: loss = 0.6019 (2.917 sec/step)\n",
            "I0619 16:30:13.787169 139649090267008 learning.py:507] global step 143: loss = 0.6542 (2.876 sec/step)\n",
            "I0619 16:30:16.727390 139649090267008 learning.py:507] global step 143: loss = 0.4938 (2.938 sec/step)\n",
            "I0619 16:30:19.655187 139649090267008 learning.py:507] global step 144: loss = 0.5747 (2.926 sec/step)\n",
            "I0619 16:30:20.931339 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 16:30:25.712781 139649090267008 learning.py:507] global step 144: loss = 0.6961 (6.047 sec/step)\n",
            "I0619 16:30:27.953904 139646017689344 supervisor.py:1050] Recording summary at step 144.\n",
            "I0619 16:30:29.631381 139649090267008 learning.py:507] global step 144: loss = 0.6163 (3.916 sec/step)\n",
            "I0619 16:30:32.533503 139649090267008 learning.py:507] global step 144: loss = 0.6079 (2.900 sec/step)\n",
            "I0619 16:30:35.421509 139649090267008 learning.py:507] global step 144: loss = 0.5933 (2.886 sec/step)\n",
            "I0619 16:30:38.343424 139649090267008 learning.py:507] global step 144: loss = 0.5317 (2.920 sec/step)\n",
            "I0619 16:30:41.274288 139649090267008 learning.py:507] global step 144: loss = 0.6250 (2.929 sec/step)\n",
            "I0619 16:30:44.105265 139649090267008 learning.py:507] global step 144: loss = 0.7099 (2.829 sec/step)\n",
            "I0619 16:30:47.329102 139649090267008 learning.py:507] global step 145: loss = 0.6374 (3.222 sec/step)\n",
            "I0619 16:30:50.195482 139649090267008 learning.py:507] global step 145: loss = 0.6473 (2.865 sec/step)\n",
            "I0619 16:30:53.016930 139649090267008 learning.py:507] global step 145: loss = 0.5697 (2.820 sec/step)\n",
            "I0619 16:30:55.866800 139649090267008 learning.py:507] global step 145: loss = 0.5927 (2.848 sec/step)\n",
            "I0619 16:30:58.752906 139649090267008 learning.py:507] global step 145: loss = 0.5610 (2.884 sec/step)\n",
            "I0619 16:31:01.630500 139649090267008 learning.py:507] global step 145: loss = 0.6340 (2.876 sec/step)\n",
            "I0619 16:31:04.491605 139649090267008 learning.py:507] global step 145: loss = 0.6382 (2.859 sec/step)\n",
            "I0619 16:31:07.462280 139649090267008 learning.py:507] global step 145: loss = 0.5788 (2.969 sec/step)\n",
            "I0619 16:31:10.341199 139649090267008 learning.py:507] global step 146: loss = 0.6157 (2.877 sec/step)\n",
            "I0619 16:31:13.234542 139649090267008 learning.py:507] global step 146: loss = 0.5537 (2.892 sec/step)\n",
            "I0619 16:31:16.129854 139649090267008 learning.py:507] global step 146: loss = 0.5122 (2.893 sec/step)\n",
            "I0619 16:31:19.122167 139649090267008 learning.py:507] global step 146: loss = 0.5740 (2.991 sec/step)\n",
            "I0619 16:31:22.033630 139649090267008 learning.py:507] global step 146: loss = 0.5122 (2.910 sec/step)\n",
            "I0619 16:31:24.873895 139649090267008 learning.py:507] global step 146: loss = 0.6218 (2.839 sec/step)\n",
            "I0619 16:31:27.871326 139649090267008 learning.py:507] global step 146: loss = 0.6105 (2.995 sec/step)\n",
            "I0619 16:31:30.811139 139649090267008 learning.py:507] global step 146: loss = 0.5332 (2.938 sec/step)\n",
            "I0619 16:31:33.770221 139649090267008 learning.py:507] global step 147: loss = 0.6121 (2.957 sec/step)\n",
            "I0619 16:31:36.677739 139649090267008 learning.py:507] global step 147: loss = 0.5000 (2.906 sec/step)\n",
            "I0619 16:31:39.660267 139649090267008 learning.py:507] global step 147: loss = 0.5803 (2.981 sec/step)\n",
            "I0619 16:31:42.556931 139649090267008 learning.py:507] global step 147: loss = 0.5860 (2.895 sec/step)\n",
            "I0619 16:31:45.542849 139649090267008 learning.py:507] global step 147: loss = 0.5429 (2.984 sec/step)\n",
            "I0619 16:31:48.622653 139649090267008 learning.py:507] global step 147: loss = 0.4916 (3.078 sec/step)\n",
            "I0619 16:31:51.548930 139649090267008 learning.py:507] global step 147: loss = 0.5376 (2.925 sec/step)\n",
            "I0619 16:31:54.437036 139649090267008 learning.py:507] global step 147: loss = 0.5830 (2.886 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:31:57.304355 139649090267008 learning.py:507] global step 148: loss = 0.5142 (2.864 sec/step)\n",
            "tcmalloc: large alloc 3068264448 bytes == 0x7efdc7896000 @  0x7f029625fb6b 0x7f029627f379 0x7f02701f0337 0x7f02701a32af 0x7f026fe9a30b 0x7f026fe67f86 0x7f026fe68875 0x7f02787ce4a8 0x7f027888b2dc 0x7f027888b8a8 0x7f027876022f 0x7f02787e6093 0x7f02787e52db 0x7f0270117651 0x7f02701188df 0x7f02701befc9 0x7f02701bbea8 0x7f0294b5f66f 0x7f0295c416db 0x7f0295f7a88f\n",
            "I0619 16:32:00.896435 139649090267008 learning.py:507] global step 148: loss = 0.5150 (3.590 sec/step)\n",
            "I0619 16:32:03.831263 139649090267008 learning.py:507] global step 148: loss = 0.5168 (2.933 sec/step)\n",
            "I0619 16:32:06.867670 139649090267008 learning.py:507] global step 148: loss = 0.5320 (3.035 sec/step)\n",
            "I0619 16:32:09.793944 139649090267008 learning.py:507] global step 148: loss = 0.5811 (2.925 sec/step)\n",
            "I0619 16:32:12.739050 139649090267008 learning.py:507] global step 148: loss = 0.6023 (2.944 sec/step)\n",
            "I0619 16:32:15.651806 139649090267008 learning.py:507] global step 148: loss = 0.5871 (2.911 sec/step)\n",
            "I0619 16:32:19.154589 139649090267008 learning.py:507] global step 148: loss = 0.5477 (3.501 sec/step)\n",
            "I0619 16:32:23.367266 139649090267008 learning.py:507] global step 149: loss = 0.5444 (4.201 sec/step)\n",
            "I0619 16:32:26.109101 139646017689344 supervisor.py:1050] Recording summary at step 149.\n",
            "I0619 16:32:27.041738 139649090267008 learning.py:507] global step 149: loss = 0.5263 (3.672 sec/step)\n",
            "I0619 16:32:29.963502 139649090267008 learning.py:507] global step 149: loss = 0.6619 (2.920 sec/step)\n",
            "I0619 16:32:32.823583 139649090267008 learning.py:507] global step 149: loss = 0.5936 (2.858 sec/step)\n",
            "I0619 16:32:35.709155 139649090267008 learning.py:507] global step 149: loss = 0.5023 (2.884 sec/step)\n",
            "I0619 16:32:38.590753 139649090267008 learning.py:507] global step 149: loss = 0.6134 (2.880 sec/step)\n",
            "I0619 16:32:41.547769 139649090267008 learning.py:507] global step 149: loss = 0.5711 (2.955 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:32:44.472797 139649090267008 learning.py:507] global step 149: loss = 0.5529 (2.923 sec/step)\n",
            "I0619 16:32:47.328385 139649090267008 learning.py:507] global step 150: loss = 0.6887 (2.853 sec/step)\n",
            "I0619 16:32:50.279290 139649090267008 learning.py:507] global step 150: loss = 0.7752 (2.949 sec/step)\n",
            "I0619 16:32:53.192040 139649090267008 learning.py:507] global step 150: loss = 0.5003 (2.911 sec/step)\n",
            "I0619 16:32:56.058287 139649090267008 learning.py:507] global step 150: loss = 0.7224 (2.865 sec/step)\n",
            "I0619 16:32:58.993568 139649090267008 learning.py:507] global step 150: loss = 0.4949 (2.934 sec/step)\n",
            "I0619 16:33:01.937866 139649090267008 learning.py:507] global step 150: loss = 0.6649 (2.942 sec/step)\n",
            "I0619 16:33:04.796602 139649090267008 learning.py:507] global step 150: loss = 0.5187 (2.857 sec/step)\n",
            "I0619 16:33:07.715512 139649090267008 learning.py:507] global step 150: loss = 0.5919 (2.917 sec/step)\n",
            "I0619 16:33:10.605401 139649090267008 learning.py:507] global step 151: loss = 0.5490 (2.888 sec/step)\n",
            "I0619 16:33:13.502100 139649090267008 learning.py:507] global step 151: loss = 0.5822 (2.895 sec/step)\n",
            "I0619 16:33:16.468503 139649090267008 learning.py:507] global step 151: loss = 0.5551 (2.964 sec/step)\n",
            "I0619 16:33:19.375870 139649090267008 learning.py:507] global step 151: loss = 0.6715 (2.906 sec/step)\n",
            "I0619 16:33:22.323216 139649090267008 learning.py:507] global step 151: loss = 0.4966 (2.945 sec/step)\n",
            "I0619 16:33:25.245978 139649090267008 learning.py:507] global step 151: loss = 0.5639 (2.921 sec/step)\n",
            "I0619 16:33:28.131866 139649090267008 learning.py:507] global step 151: loss = 0.5839 (2.884 sec/step)\n",
            "I0619 16:33:31.003614 139649090267008 learning.py:507] global step 151: loss = 0.5671 (2.870 sec/step)\n",
            "I0619 16:33:33.919267 139649090267008 learning.py:507] global step 152: loss = 0.6327 (2.913 sec/step)\n",
            "I0619 16:33:36.777890 139649090267008 learning.py:507] global step 152: loss = 0.6543 (2.857 sec/step)\n",
            "I0619 16:33:39.672696 139649090267008 learning.py:507] global step 152: loss = 0.5759 (2.893 sec/step)\n",
            "I0619 16:33:42.601144 139649090267008 learning.py:507] global step 152: loss = 0.4447 (2.927 sec/step)\n",
            "I0619 16:33:45.471677 139649090267008 learning.py:507] global step 152: loss = 0.5844 (2.869 sec/step)\n",
            "I0619 16:33:48.282265 139649090267008 learning.py:507] global step 152: loss = 0.5804 (2.809 sec/step)\n",
            "I0619 16:33:51.190124 139649090267008 learning.py:507] global step 152: loss = 0.5871 (2.906 sec/step)\n",
            "I0619 16:33:54.072841 139649090267008 learning.py:507] global step 152: loss = 0.5522 (2.881 sec/step)\n",
            "I0619 16:33:56.970125 139649090267008 learning.py:507] global step 153: loss = 0.5555 (2.896 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:33:59.881472 139649090267008 learning.py:507] global step 153: loss = 0.5771 (2.910 sec/step)\n",
            "I0619 16:34:02.787341 139649090267008 learning.py:507] global step 153: loss = 0.5753 (2.904 sec/step)\n",
            "I0619 16:34:05.660327 139649090267008 learning.py:507] global step 153: loss = 0.5677 (2.871 sec/step)\n",
            "I0619 16:34:08.523167 139649090267008 learning.py:507] global step 153: loss = 0.5851 (2.861 sec/step)\n",
            "I0619 16:34:11.465402 139649090267008 learning.py:507] global step 153: loss = 0.5885 (2.941 sec/step)\n",
            "I0619 16:34:14.418866 139649090267008 learning.py:507] global step 153: loss = 0.6331 (2.952 sec/step)\n",
            "I0619 16:34:17.353290 139649090267008 learning.py:507] global step 153: loss = 0.5673 (2.933 sec/step)\n",
            "I0619 16:34:20.276874 139649090267008 learning.py:507] global step 154: loss = 0.5317 (2.922 sec/step)\n",
            "I0619 16:34:25.337925 139649090267008 learning.py:507] global step 154: loss = 0.5615 (5.059 sec/step)\n",
            "I0619 16:34:25.424454 139646017689344 supervisor.py:1050] Recording summary at step 154.\n",
            "I0619 16:34:28.292386 139649090267008 learning.py:507] global step 154: loss = 0.6116 (2.950 sec/step)\n",
            "I0619 16:34:31.209518 139649090267008 learning.py:507] global step 154: loss = 0.5969 (2.915 sec/step)\n",
            "I0619 16:34:34.136295 139649090267008 learning.py:507] global step 154: loss = 0.5455 (2.925 sec/step)\n",
            "I0619 16:34:36.989461 139649090267008 learning.py:507] global step 154: loss = 0.4980 (2.852 sec/step)\n",
            "I0619 16:34:39.867147 139649090267008 learning.py:507] global step 154: loss = 0.5606 (2.876 sec/step)\n",
            "I0619 16:34:42.764586 139649090267008 learning.py:507] global step 154: loss = 0.5846 (2.896 sec/step)\n",
            "I0619 16:34:45.648033 139649090267008 learning.py:507] global step 155: loss = 0.6006 (2.881 sec/step)\n",
            "I0619 16:34:48.500730 139649090267008 learning.py:507] global step 155: loss = 0.5506 (2.851 sec/step)\n",
            "I0619 16:34:51.453354 139649090267008 learning.py:507] global step 155: loss = 0.5162 (2.951 sec/step)\n",
            "I0619 16:34:54.284175 139649090267008 learning.py:507] global step 155: loss = 0.6197 (2.829 sec/step)\n",
            "I0619 16:34:57.231824 139649090267008 learning.py:507] global step 155: loss = 0.4930 (2.946 sec/step)\n",
            "I0619 16:35:00.065874 139649090267008 learning.py:507] global step 155: loss = 0.5797 (2.832 sec/step)\n",
            "I0619 16:35:03.099831 139649090267008 learning.py:507] global step 155: loss = 0.5222 (3.032 sec/step)\n",
            "I0619 16:35:06.049266 139649090267008 learning.py:507] global step 155: loss = 0.6965 (2.948 sec/step)\n",
            "I0619 16:35:09.053385 139649090267008 learning.py:507] global step 156: loss = 0.6888 (3.002 sec/step)\n",
            "I0619 16:35:12.167438 139649090267008 learning.py:507] global step 156: loss = 0.4721 (3.112 sec/step)\n",
            "I0619 16:35:15.010180 139649090267008 learning.py:507] global step 156: loss = 0.6024 (2.841 sec/step)\n",
            "I0619 16:35:17.932674 139649090267008 learning.py:507] global step 156: loss = 0.5562 (2.921 sec/step)\n",
            "I0619 16:35:20.788488 139649090267008 learning.py:507] global step 156: loss = 0.4198 (2.854 sec/step)\n",
            "I0619 16:35:23.739952 139649090267008 learning.py:507] global step 156: loss = 0.5066 (2.950 sec/step)\n",
            "I0619 16:35:26.713817 139649090267008 learning.py:507] global step 156: loss = 0.5531 (2.972 sec/step)\n",
            "I0619 16:35:29.930996 139649090267008 learning.py:507] global step 156: loss = 0.5365 (3.215 sec/step)\n",
            "I0619 16:35:32.817258 139649090267008 learning.py:507] global step 157: loss = 0.4946 (2.884 sec/step)\n",
            "I0619 16:35:35.671725 139649090267008 learning.py:507] global step 157: loss = 0.5864 (2.853 sec/step)\n",
            "I0619 16:35:38.521206 139649090267008 learning.py:507] global step 157: loss = 0.5733 (2.848 sec/step)\n",
            "I0619 16:35:41.409955 139649090267008 learning.py:507] global step 157: loss = 0.4851 (2.887 sec/step)\n",
            "I0619 16:35:44.360886 139649090267008 learning.py:507] global step 157: loss = 0.5460 (2.949 sec/step)\n",
            "I0619 16:35:47.321254 139649090267008 learning.py:507] global step 157: loss = 0.5043 (2.959 sec/step)\n",
            "I0619 16:35:50.204502 139649090267008 learning.py:507] global step 157: loss = 0.5334 (2.882 sec/step)\n",
            "I0619 16:35:53.064906 139649090267008 learning.py:507] global step 157: loss = 0.5561 (2.859 sec/step)\n",
            "I0619 16:35:55.985908 139649090267008 learning.py:507] global step 158: loss = 0.5406 (2.916 sec/step)\n",
            "I0619 16:35:58.913347 139649090267008 learning.py:507] global step 158: loss = 0.5226 (2.926 sec/step)\n",
            "I0619 16:36:01.824275 139649090267008 learning.py:507] global step 158: loss = 0.6157 (2.909 sec/step)\n",
            "I0619 16:36:04.717535 139649090267008 learning.py:507] global step 158: loss = 0.4605 (2.891 sec/step)\n",
            "I0619 16:36:07.649861 139649090267008 learning.py:507] global step 158: loss = 0.5330 (2.931 sec/step)\n",
            "I0619 16:36:10.514260 139649090267008 learning.py:507] global step 158: loss = 0.5133 (2.863 sec/step)\n",
            "I0619 16:36:13.405519 139649090267008 learning.py:507] global step 158: loss = 0.5333 (2.889 sec/step)\n",
            "I0619 16:36:16.255894 139649090267008 learning.py:507] global step 158: loss = 0.7805 (2.849 sec/step)\n",
            "I0619 16:36:19.344188 139649090267008 learning.py:507] global step 159: loss = 0.5804 (3.086 sec/step)\n",
            "I0619 16:36:23.658066 139649090267008 learning.py:507] global step 159: loss = 0.5981 (4.309 sec/step)\n",
            "I0619 16:36:26.360917 139646017689344 supervisor.py:1050] Recording summary at step 159.\n",
            "I0619 16:36:27.342772 139649090267008 learning.py:507] global step 159: loss = 0.5964 (3.675 sec/step)\n",
            "I0619 16:36:30.299788 139649090267008 learning.py:507] global step 159: loss = 0.6298 (2.955 sec/step)\n",
            "I0619 16:36:33.119069 139649090267008 learning.py:507] global step 159: loss = 0.6455 (2.818 sec/step)\n",
            "I0619 16:36:36.216178 139649090267008 learning.py:507] global step 159: loss = 0.6449 (3.095 sec/step)\n",
            "I0619 16:36:39.136772 139649090267008 learning.py:507] global step 159: loss = 0.5575 (2.919 sec/step)\n",
            "I0619 16:36:42.030245 139649090267008 learning.py:507] global step 159: loss = 0.5817 (2.892 sec/step)\n",
            "I0619 16:36:44.893910 139649090267008 learning.py:507] global step 160: loss = 0.5781 (2.862 sec/step)\n",
            "I0619 16:36:47.786827 139649090267008 learning.py:507] global step 160: loss = 0.5040 (2.891 sec/step)\n",
            "I0619 16:36:50.676775 139649090267008 learning.py:507] global step 160: loss = 0.6154 (2.888 sec/step)\n",
            "I0619 16:36:53.546394 139649090267008 learning.py:507] global step 160: loss = 0.6271 (2.868 sec/step)\n",
            "I0619 16:36:56.407357 139649090267008 learning.py:507] global step 160: loss = 0.5805 (2.859 sec/step)\n",
            "I0619 16:36:59.384311 139649090267008 learning.py:507] global step 160: loss = 0.4414 (2.975 sec/step)\n",
            "I0619 16:37:02.311446 139649090267008 learning.py:507] global step 160: loss = 0.4287 (2.925 sec/step)\n",
            "I0619 16:37:05.301800 139649090267008 learning.py:507] global step 160: loss = 0.5033 (2.989 sec/step)\n",
            "I0619 16:37:08.255098 139649090267008 learning.py:507] global step 161: loss = 0.5393 (2.951 sec/step)\n",
            "I0619 16:37:11.157119 139649090267008 learning.py:507] global step 161: loss = 0.7752 (2.900 sec/step)\n",
            "I0619 16:37:14.047390 139649090267008 learning.py:507] global step 161: loss = 0.4935 (2.888 sec/step)\n",
            "I0619 16:37:16.914636 139649090267008 learning.py:507] global step 161: loss = 0.4253 (2.866 sec/step)\n",
            "I0619 16:37:19.871111 139649090267008 learning.py:507] global step 161: loss = 0.5510 (2.955 sec/step)\n",
            "I0619 16:37:22.775754 139649090267008 learning.py:507] global step 161: loss = 0.5565 (2.903 sec/step)\n",
            "I0619 16:37:25.654062 139649090267008 learning.py:507] global step 161: loss = 0.5747 (2.877 sec/step)\n",
            "I0619 16:37:28.534594 139649090267008 learning.py:507] global step 161: loss = 0.5328 (2.879 sec/step)\n",
            "I0619 16:37:31.377872 139649090267008 learning.py:507] global step 162: loss = 0.6256 (2.841 sec/step)\n",
            "I0619 16:37:34.218891 139649090267008 learning.py:507] global step 162: loss = 0.5220 (2.839 sec/step)\n",
            "I0619 16:37:37.152233 139649090267008 learning.py:507] global step 162: loss = 0.5117 (2.932 sec/step)\n",
            "I0619 16:37:40.017758 139649090267008 learning.py:507] global step 162: loss = 0.5169 (2.864 sec/step)\n",
            "I0619 16:37:42.952955 139649090267008 learning.py:507] global step 162: loss = 0.4375 (2.933 sec/step)\n",
            "I0619 16:37:45.865586 139649090267008 learning.py:507] global step 162: loss = 0.5380 (2.911 sec/step)\n",
            "I0619 16:37:48.728839 139649090267008 learning.py:507] global step 162: loss = 0.6234 (2.861 sec/step)\n",
            "I0619 16:37:51.570140 139649090267008 learning.py:507] global step 162: loss = 0.5305 (2.840 sec/step)\n",
            "I0619 16:37:54.408307 139649090267008 learning.py:507] global step 163: loss = 0.5450 (2.836 sec/step)\n",
            "I0619 16:37:57.308704 139649090267008 learning.py:507] global step 163: loss = 0.4682 (2.899 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:38:00.238364 139649090267008 learning.py:507] global step 163: loss = 0.4253 (2.928 sec/step)\n",
            "I0619 16:38:03.121716 139649090267008 learning.py:507] global step 163: loss = 0.5156 (2.882 sec/step)\n",
            "I0619 16:38:06.046642 139649090267008 learning.py:507] global step 163: loss = 0.4416 (2.923 sec/step)\n",
            "I0619 16:38:08.905452 139649090267008 learning.py:507] global step 163: loss = 0.6112 (2.857 sec/step)\n",
            "I0619 16:38:11.734864 139649090267008 learning.py:507] global step 163: loss = 0.6564 (2.828 sec/step)\n",
            "I0619 16:38:14.588685 139649090267008 learning.py:507] global step 163: loss = 0.5754 (2.852 sec/step)\n",
            "I0619 16:38:17.464374 139649090267008 learning.py:507] global step 164: loss = 0.5140 (2.873 sec/step)\n",
            "I0619 16:38:20.385673 139649090267008 learning.py:507] global step 164: loss = 0.5215 (2.920 sec/step)\n",
            "I0619 16:38:25.477489 139649090267008 learning.py:507] global step 164: loss = 0.5308 (5.075 sec/step)\n",
            "I0619 16:38:25.892761 139646017689344 supervisor.py:1050] Recording summary at step 164.\n",
            "I0619 16:38:28.453346 139649090267008 learning.py:507] global step 164: loss = 0.5708 (2.973 sec/step)\n",
            "I0619 16:38:31.350555 139649090267008 learning.py:507] global step 164: loss = 0.4915 (2.895 sec/step)\n",
            "I0619 16:38:34.257284 139649090267008 learning.py:507] global step 164: loss = 0.5430 (2.905 sec/step)\n",
            "I0619 16:38:37.170951 139649090267008 learning.py:507] global step 164: loss = 0.5480 (2.912 sec/step)\n",
            "I0619 16:38:40.115212 139649090267008 learning.py:507] global step 164: loss = 0.5604 (2.943 sec/step)\n",
            "I0619 16:38:42.977407 139649090267008 learning.py:507] global step 165: loss = 0.4982 (2.860 sec/step)\n",
            "I0619 16:38:45.869926 139649090267008 learning.py:507] global step 165: loss = 0.5367 (2.891 sec/step)\n",
            "I0619 16:38:48.794584 139649090267008 learning.py:507] global step 165: loss = 0.5631 (2.923 sec/step)\n",
            "I0619 16:38:51.678882 139649090267008 learning.py:507] global step 165: loss = 0.5035 (2.883 sec/step)\n",
            "I0619 16:38:54.775830 139649090267008 learning.py:507] global step 165: loss = 0.6548 (3.095 sec/step)\n",
            "I0619 16:38:57.678566 139649090267008 learning.py:507] global step 165: loss = 0.5899 (2.901 sec/step)\n",
            "I0619 16:39:00.493749 139649090267008 learning.py:507] global step 165: loss = 0.4874 (2.813 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:39:03.439306 139649090267008 learning.py:507] global step 165: loss = 0.6603 (2.944 sec/step)\n",
            "I0619 16:39:06.333054 139649090267008 learning.py:507] global step 166: loss = 0.5617 (2.890 sec/step)\n",
            "I0619 16:39:09.193190 139649090267008 learning.py:507] global step 166: loss = 0.5693 (2.859 sec/step)\n",
            "I0619 16:39:12.088868 139649090267008 learning.py:507] global step 166: loss = 0.5143 (2.894 sec/step)\n",
            "I0619 16:39:15.022109 139649090267008 learning.py:507] global step 166: loss = 0.5457 (2.932 sec/step)\n",
            "I0619 16:39:17.982566 139649090267008 learning.py:507] global step 166: loss = 0.5168 (2.959 sec/step)\n",
            "I0619 16:39:20.975920 139649090267008 learning.py:507] global step 166: loss = 0.5250 (2.992 sec/step)\n",
            "I0619 16:39:23.851337 139649090267008 learning.py:507] global step 166: loss = 0.4556 (2.874 sec/step)\n",
            "I0619 16:39:26.760541 139649090267008 learning.py:507] global step 166: loss = 0.5284 (2.908 sec/step)\n",
            "I0619 16:39:29.640762 139649090267008 learning.py:507] global step 167: loss = 0.6411 (2.878 sec/step)\n",
            "I0619 16:39:32.484552 139649090267008 learning.py:507] global step 167: loss = 0.6118 (2.842 sec/step)\n",
            "I0619 16:39:35.373826 139649090267008 learning.py:507] global step 167: loss = 0.5216 (2.887 sec/step)\n",
            "I0619 16:39:38.270277 139649090267008 learning.py:507] global step 167: loss = 0.5034 (2.895 sec/step)\n",
            "I0619 16:39:41.143457 139649090267008 learning.py:507] global step 167: loss = 0.5110 (2.871 sec/step)\n",
            "I0619 16:39:44.067566 139649090267008 learning.py:507] global step 167: loss = 0.4388 (2.922 sec/step)\n",
            "I0619 16:39:46.984826 139649090267008 learning.py:507] global step 167: loss = 0.5508 (2.915 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:39:49.843029 139649090267008 learning.py:507] global step 167: loss = 0.5248 (2.856 sec/step)\n",
            "I0619 16:39:52.746450 139649090267008 learning.py:507] global step 168: loss = 0.5327 (2.901 sec/step)\n",
            "I0619 16:39:55.695006 139649090267008 learning.py:507] global step 168: loss = 0.5681 (2.946 sec/step)\n",
            "I0619 16:39:58.597260 139649090267008 learning.py:507] global step 168: loss = 0.4822 (2.900 sec/step)\n",
            "I0619 16:40:01.449985 139649090267008 learning.py:507] global step 168: loss = 0.7330 (2.851 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:40:04.371978 139649090267008 learning.py:507] global step 168: loss = 0.4827 (2.920 sec/step)\n",
            "I0619 16:40:07.226124 139649090267008 learning.py:507] global step 168: loss = 0.4503 (2.852 sec/step)\n",
            "I0619 16:40:10.096256 139649090267008 learning.py:507] global step 168: loss = 0.4596 (2.868 sec/step)\n",
            "I0619 16:40:13.073733 139649090267008 learning.py:507] global step 168: loss = 0.6008 (2.976 sec/step)\n",
            "I0619 16:40:15.987986 139649090267008 learning.py:507] global step 169: loss = 0.5857 (2.912 sec/step)\n",
            "I0619 16:40:18.993085 139649090267008 learning.py:507] global step 169: loss = 0.4929 (3.003 sec/step)\n",
            "I0619 16:40:20.931328 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 16:40:23.951785 139649090267008 learning.py:507] global step 169: loss = 0.4864 (4.950 sec/step)\n",
            "I0619 16:40:27.512220 139646017689344 supervisor.py:1050] Recording summary at step 169.\n",
            "I0619 16:40:28.481776 139649090267008 learning.py:507] global step 169: loss = 0.5237 (4.491 sec/step)\n",
            "I0619 16:40:31.326598 139649090267008 learning.py:507] global step 169: loss = 0.5310 (2.843 sec/step)\n",
            "I0619 16:40:34.266425 139649090267008 learning.py:507] global step 169: loss = 0.5521 (2.938 sec/step)\n",
            "I0619 16:40:37.138728 139649090267008 learning.py:507] global step 169: loss = 0.5138 (2.871 sec/step)\n",
            "I0619 16:40:40.040094 139649090267008 learning.py:507] global step 169: loss = 0.4999 (2.900 sec/step)\n",
            "I0619 16:40:42.909695 139649090267008 learning.py:507] global step 170: loss = 0.5585 (2.868 sec/step)\n",
            "I0619 16:40:45.797364 139649090267008 learning.py:507] global step 170: loss = 0.4322 (2.886 sec/step)\n",
            "I0619 16:40:48.652274 139649090267008 learning.py:507] global step 170: loss = 0.5270 (2.853 sec/step)\n",
            "I0619 16:40:51.539185 139649090267008 learning.py:507] global step 170: loss = 0.5147 (2.885 sec/step)\n",
            "I0619 16:40:54.419126 139649090267008 learning.py:507] global step 170: loss = 0.5191 (2.878 sec/step)\n",
            "I0619 16:40:57.271624 139649090267008 learning.py:507] global step 170: loss = 0.5376 (2.851 sec/step)\n",
            "I0619 16:41:00.258194 139649090267008 learning.py:507] global step 170: loss = 0.4753 (2.985 sec/step)\n",
            "I0619 16:41:03.158148 139649090267008 learning.py:507] global step 170: loss = 0.4607 (2.898 sec/step)\n",
            "I0619 16:41:06.068632 139649090267008 learning.py:507] global step 171: loss = 0.5652 (2.908 sec/step)\n",
            "I0619 16:41:09.017549 139649090267008 learning.py:507] global step 171: loss = 0.5208 (2.945 sec/step)\n",
            "I0619 16:41:11.967546 139649090267008 learning.py:507] global step 171: loss = 0.5271 (2.946 sec/step)\n",
            "I0619 16:41:14.919466 139649090267008 learning.py:507] global step 171: loss = 0.4656 (2.950 sec/step)\n",
            "I0619 16:41:17.886526 139649090267008 learning.py:507] global step 171: loss = 0.5445 (2.965 sec/step)\n",
            "I0619 16:41:20.798455 139649090267008 learning.py:507] global step 171: loss = 0.4644 (2.910 sec/step)\n",
            "I0619 16:41:23.776353 139649090267008 learning.py:507] global step 171: loss = 0.5826 (2.976 sec/step)\n",
            "I0619 16:41:26.672818 139649090267008 learning.py:507] global step 171: loss = 0.5069 (2.895 sec/step)\n",
            "I0619 16:41:29.601668 139649090267008 learning.py:507] global step 172: loss = 0.4899 (2.927 sec/step)\n",
            "I0619 16:41:32.511718 139649090267008 learning.py:507] global step 172: loss = 0.4975 (2.908 sec/step)\n",
            "I0619 16:41:35.426650 139649090267008 learning.py:507] global step 172: loss = 0.5689 (2.913 sec/step)\n",
            "I0619 16:41:38.309347 139649090267008 learning.py:507] global step 172: loss = 0.4872 (2.881 sec/step)\n",
            "I0619 16:41:41.258765 139649090267008 learning.py:507] global step 172: loss = 0.4261 (2.948 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:41:44.273869 139649090267008 learning.py:507] global step 172: loss = 0.5686 (3.013 sec/step)\n",
            "I0619 16:41:47.173543 139649090267008 learning.py:507] global step 172: loss = 0.5370 (2.898 sec/step)\n",
            "I0619 16:41:50.129106 139649090267008 learning.py:507] global step 172: loss = 0.5232 (2.953 sec/step)\n",
            "I0619 16:41:53.007671 139649090267008 learning.py:507] global step 173: loss = 0.4270 (2.877 sec/step)\n",
            "I0619 16:41:55.893267 139649090267008 learning.py:507] global step 173: loss = 0.4993 (2.884 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:41:58.789092 139649090267008 learning.py:507] global step 173: loss = 0.4791 (2.894 sec/step)\n",
            "I0619 16:42:01.815130 139649090267008 learning.py:507] global step 173: loss = 0.5040 (3.024 sec/step)\n",
            "I0619 16:42:04.773104 139649090267008 learning.py:507] global step 173: loss = 0.4368 (2.956 sec/step)\n",
            "I0619 16:42:07.731837 139649090267008 learning.py:507] global step 173: loss = 0.5525 (2.957 sec/step)\n",
            "I0619 16:42:10.669120 139649090267008 learning.py:507] global step 173: loss = 0.5169 (2.936 sec/step)\n",
            "I0619 16:42:13.608001 139649090267008 learning.py:507] global step 173: loss = 0.5265 (2.937 sec/step)\n",
            "I0619 16:42:16.525906 139649090267008 learning.py:507] global step 174: loss = 0.4862 (2.916 sec/step)\n",
            "I0619 16:42:19.493937 139649090267008 learning.py:507] global step 174: loss = 0.6136 (2.966 sec/step)\n",
            "I0619 16:42:24.610305 139649090267008 learning.py:507] global step 174: loss = 0.5770 (5.109 sec/step)\n",
            "I0619 16:42:26.122340 139646017689344 supervisor.py:1050] Recording summary at step 174.\n",
            "I0619 16:42:27.959489 139649090267008 learning.py:507] global step 174: loss = 0.6548 (3.343 sec/step)\n",
            "I0619 16:42:30.889281 139649090267008 learning.py:507] global step 174: loss = 0.6194 (2.928 sec/step)\n",
            "I0619 16:42:33.824295 139649090267008 learning.py:507] global step 174: loss = 0.4682 (2.933 sec/step)\n",
            "I0619 16:42:37.157080 139649090267008 learning.py:507] global step 174: loss = 0.4760 (3.331 sec/step)\n",
            "I0619 16:42:40.279880 139649090267008 learning.py:507] global step 174: loss = 0.5837 (3.121 sec/step)\n",
            "I0619 16:42:43.348110 139649090267008 learning.py:507] global step 175: loss = 0.5334 (3.066 sec/step)\n",
            "I0619 16:42:46.320980 139649090267008 learning.py:507] global step 175: loss = 0.5144 (2.971 sec/step)\n",
            "I0619 16:42:49.243488 139649090267008 learning.py:507] global step 175: loss = 0.6098 (2.921 sec/step)\n",
            "I0619 16:42:52.232054 139649090267008 learning.py:507] global step 175: loss = 0.5129 (2.987 sec/step)\n",
            "I0619 16:42:55.643470 139649090267008 learning.py:507] global step 175: loss = 0.5083 (3.409 sec/step)\n",
            "I0619 16:42:58.658557 139649090267008 learning.py:507] global step 175: loss = 0.4157 (3.013 sec/step)\n",
            "I0619 16:43:01.589598 139649090267008 learning.py:507] global step 175: loss = 0.5452 (2.929 sec/step)\n",
            "I0619 16:43:04.515258 139649090267008 learning.py:507] global step 175: loss = 0.5340 (2.924 sec/step)\n",
            "I0619 16:43:07.474302 139649090267008 learning.py:507] global step 176: loss = 0.4580 (2.957 sec/step)\n",
            "I0619 16:43:10.418379 139649090267008 learning.py:507] global step 176: loss = 0.5213 (2.942 sec/step)\n",
            "I0619 16:43:13.529569 139649090267008 learning.py:507] global step 176: loss = 0.5580 (3.109 sec/step)\n",
            "I0619 16:43:16.393779 139649090267008 learning.py:507] global step 176: loss = 0.4622 (2.863 sec/step)\n",
            "I0619 16:43:19.347051 139649090267008 learning.py:507] global step 176: loss = 0.5697 (2.952 sec/step)\n",
            "I0619 16:43:22.219729 139649090267008 learning.py:507] global step 176: loss = 0.5045 (2.871 sec/step)\n",
            "I0619 16:43:25.121120 139649090267008 learning.py:507] global step 176: loss = 0.5792 (2.900 sec/step)\n",
            "I0619 16:43:28.139925 139649090267008 learning.py:507] global step 176: loss = 0.4854 (3.017 sec/step)\n",
            "I0619 16:43:31.122344 139649090267008 learning.py:507] global step 177: loss = 0.4971 (2.981 sec/step)\n",
            "I0619 16:43:34.061139 139649090267008 learning.py:507] global step 177: loss = 0.6149 (2.936 sec/step)\n",
            "I0619 16:43:37.005108 139649090267008 learning.py:507] global step 177: loss = 0.5578 (2.942 sec/step)\n",
            "I0619 16:43:39.856209 139649090267008 learning.py:507] global step 177: loss = 0.5274 (2.848 sec/step)\n",
            "I0619 16:43:42.777688 139649090267008 learning.py:507] global step 177: loss = 0.5937 (2.920 sec/step)\n",
            "I0619 16:43:46.161322 139649090267008 learning.py:507] global step 177: loss = 0.5854 (3.382 sec/step)\n",
            "I0619 16:43:49.134135 139649090267008 learning.py:507] global step 177: loss = 0.4603 (2.971 sec/step)\n",
            "I0619 16:43:52.042588 139649090267008 learning.py:507] global step 177: loss = 0.5965 (2.907 sec/step)\n",
            "I0619 16:43:54.934995 139649090267008 learning.py:507] global step 178: loss = 0.4260 (2.890 sec/step)\n",
            "I0619 16:43:57.859860 139649090267008 learning.py:507] global step 178: loss = 0.5786 (2.923 sec/step)\n",
            "I0619 16:44:00.791290 139649090267008 learning.py:507] global step 178: loss = 0.5024 (2.930 sec/step)\n",
            "I0619 16:44:04.004919 139649090267008 learning.py:507] global step 178: loss = 0.5244 (3.212 sec/step)\n",
            "I0619 16:44:06.981629 139649090267008 learning.py:507] global step 178: loss = 0.4424 (2.975 sec/step)\n",
            "I0619 16:44:09.849570 139649090267008 learning.py:507] global step 178: loss = 0.4939 (2.866 sec/step)\n",
            "I0619 16:44:12.740885 139649090267008 learning.py:507] global step 178: loss = 0.5344 (2.890 sec/step)\n",
            "I0619 16:44:15.779816 139649090267008 learning.py:507] global step 178: loss = 0.4216 (3.037 sec/step)\n",
            "I0619 16:44:18.705381 139649090267008 learning.py:507] global step 179: loss = 0.4953 (2.923 sec/step)\n",
            "I0619 16:44:22.433769 139649090267008 learning.py:507] global step 179: loss = 0.5185 (3.684 sec/step)\n",
            "I0619 16:44:25.960371 139646017689344 supervisor.py:1050] Recording summary at step 179.\n",
            "I0619 16:44:26.739004 139649090267008 learning.py:507] global step 179: loss = 0.5490 (4.230 sec/step)\n",
            "I0619 16:44:29.633124 139649090267008 learning.py:507] global step 179: loss = 0.5446 (2.892 sec/step)\n",
            "I0619 16:44:32.783690 139649090267008 learning.py:507] global step 179: loss = 0.6009 (3.149 sec/step)\n",
            "I0619 16:44:35.692325 139649090267008 learning.py:507] global step 179: loss = 0.5253 (2.907 sec/step)\n",
            "I0619 16:44:38.693210 139649090267008 learning.py:507] global step 179: loss = 0.4703 (2.999 sec/step)\n",
            "I0619 16:44:41.616332 139649090267008 learning.py:507] global step 179: loss = 0.6035 (2.921 sec/step)\n",
            "I0619 16:44:44.509823 139649090267008 learning.py:507] global step 180: loss = 0.5163 (2.892 sec/step)\n",
            "I0619 16:44:47.477801 139649090267008 learning.py:507] global step 180: loss = 0.6565 (2.966 sec/step)\n",
            "I0619 16:44:50.426289 139649090267008 learning.py:507] global step 180: loss = 0.4956 (2.947 sec/step)\n",
            "I0619 16:44:53.298177 139649090267008 learning.py:507] global step 180: loss = 0.5866 (2.870 sec/step)\n",
            "I0619 16:44:56.235739 139649090267008 learning.py:507] global step 180: loss = 0.5466 (2.936 sec/step)\n",
            "I0619 16:44:59.171291 139649090267008 learning.py:507] global step 180: loss = 0.5235 (2.934 sec/step)\n",
            "I0619 16:45:02.074551 139649090267008 learning.py:507] global step 180: loss = 0.4580 (2.901 sec/step)\n",
            "I0619 16:45:05.015001 139649090267008 learning.py:507] global step 180: loss = 0.4679 (2.939 sec/step)\n",
            "I0619 16:45:07.987547 139649090267008 learning.py:507] global step 181: loss = 0.4722 (2.971 sec/step)\n",
            "I0619 16:45:10.917216 139649090267008 learning.py:507] global step 181: loss = 0.5872 (2.928 sec/step)\n",
            "I0619 16:45:13.875860 139649090267008 learning.py:507] global step 181: loss = 0.5077 (2.957 sec/step)\n",
            "I0619 16:45:16.747184 139649090267008 learning.py:507] global step 181: loss = 0.5666 (2.869 sec/step)\n",
            "I0619 16:45:19.766978 139649090267008 learning.py:507] global step 181: loss = 0.5508 (3.018 sec/step)\n",
            "I0619 16:45:22.747586 139649090267008 learning.py:507] global step 181: loss = 0.5534 (2.979 sec/step)\n",
            "I0619 16:45:25.765336 139649090267008 learning.py:507] global step 181: loss = 0.5549 (3.016 sec/step)\n",
            "I0619 16:45:29.210091 139649090267008 learning.py:507] global step 181: loss = 0.6634 (3.443 sec/step)\n",
            "I0619 16:45:32.210588 139649090267008 learning.py:507] global step 182: loss = 0.5178 (2.999 sec/step)\n",
            "I0619 16:45:35.106104 139649090267008 learning.py:507] global step 182: loss = 0.5972 (2.894 sec/step)\n",
            "I0619 16:45:37.995883 139649090267008 learning.py:507] global step 182: loss = 0.4958 (2.888 sec/step)\n",
            "I0619 16:45:40.884739 139649090267008 learning.py:507] global step 182: loss = 0.4924 (2.887 sec/step)\n",
            "I0619 16:45:43.763142 139649090267008 learning.py:507] global step 182: loss = 0.4477 (2.877 sec/step)\n",
            "I0619 16:45:47.232032 139649090267008 learning.py:507] global step 182: loss = 0.4969 (3.467 sec/step)\n",
            "I0619 16:45:50.158456 139649090267008 learning.py:507] global step 182: loss = 0.4900 (2.924 sec/step)\n",
            "I0619 16:45:53.039270 139649090267008 learning.py:507] global step 182: loss = 0.4825 (2.879 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:45:55.958957 139649090267008 learning.py:507] global step 183: loss = 0.4925 (2.917 sec/step)\n",
            "I0619 16:45:58.897099 139649090267008 learning.py:507] global step 183: loss = 0.4566 (2.936 sec/step)\n",
            "I0619 16:46:01.810728 139649090267008 learning.py:507] global step 183: loss = 0.5204 (2.912 sec/step)\n",
            "I0619 16:46:04.731988 139649090267008 learning.py:507] global step 183: loss = 0.5410 (2.919 sec/step)\n",
            "I0619 16:46:07.617426 139649090267008 learning.py:507] global step 183: loss = 0.4525 (2.884 sec/step)\n",
            "I0619 16:46:10.510724 139649090267008 learning.py:507] global step 183: loss = 0.5114 (2.891 sec/step)\n",
            "I0619 16:46:13.436772 139649090267008 learning.py:507] global step 183: loss = 0.6031 (2.924 sec/step)\n",
            "I0619 16:46:16.354464 139649090267008 learning.py:507] global step 183: loss = 0.5900 (2.916 sec/step)\n",
            "I0619 16:46:19.243556 139649090267008 learning.py:507] global step 184: loss = 0.6043 (2.887 sec/step)\n",
            "I0619 16:46:23.803576 139649090267008 learning.py:507] global step 184: loss = 0.5026 (4.551 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:46:26.115005 139646017689344 supervisor.py:1050] Recording summary at step 184.\n",
            "I0619 16:46:27.316334 139649090267008 learning.py:507] global step 184: loss = 0.5400 (3.507 sec/step)\n",
            "I0619 16:46:30.253899 139649090267008 learning.py:507] global step 184: loss = 0.5118 (2.936 sec/step)\n",
            "I0619 16:46:33.152805 139649090267008 learning.py:507] global step 184: loss = 0.4189 (2.897 sec/step)\n",
            "I0619 16:46:36.002015 139649090267008 learning.py:507] global step 184: loss = 0.5271 (2.847 sec/step)\n",
            "I0619 16:46:38.849055 139649090267008 learning.py:507] global step 184: loss = 0.5537 (2.845 sec/step)\n",
            "I0619 16:46:41.746661 139649090267008 learning.py:507] global step 184: loss = 0.4093 (2.896 sec/step)\n",
            "I0619 16:46:44.645637 139649090267008 learning.py:507] global step 185: loss = 0.5345 (2.897 sec/step)\n",
            "I0619 16:46:47.499875 139649090267008 learning.py:507] global step 185: loss = 0.4374 (2.853 sec/step)\n",
            "I0619 16:46:50.442605 139649090267008 learning.py:507] global step 185: loss = 0.5348 (2.941 sec/step)\n",
            "I0619 16:46:53.368101 139649090267008 learning.py:507] global step 185: loss = 0.4959 (2.924 sec/step)\n",
            "I0619 16:46:56.311846 139649090267008 learning.py:507] global step 185: loss = 0.5739 (2.942 sec/step)\n",
            "I0619 16:46:59.146201 139649090267008 learning.py:507] global step 185: loss = 0.5914 (2.833 sec/step)\n",
            "I0619 16:47:02.019021 139649090267008 learning.py:507] global step 185: loss = 0.4951 (2.871 sec/step)\n",
            "I0619 16:47:04.888788 139649090267008 learning.py:507] global step 185: loss = 0.6215 (2.868 sec/step)\n",
            "I0619 16:47:07.791288 139649090267008 learning.py:507] global step 186: loss = 0.5294 (2.900 sec/step)\n",
            "I0619 16:47:10.766984 139649090267008 learning.py:507] global step 186: loss = 0.5641 (2.974 sec/step)\n",
            "I0619 16:47:13.684691 139649090267008 learning.py:507] global step 186: loss = 0.5849 (2.916 sec/step)\n",
            "I0619 16:47:16.570895 139649090267008 learning.py:507] global step 186: loss = 0.4983 (2.884 sec/step)\n",
            "I0619 16:47:19.649763 139649090267008 learning.py:507] global step 186: loss = 0.4241 (3.077 sec/step)\n",
            "I0619 16:47:22.536599 139649090267008 learning.py:507] global step 186: loss = 0.4650 (2.885 sec/step)\n",
            "I0619 16:47:25.409165 139649090267008 learning.py:507] global step 186: loss = 0.4975 (2.871 sec/step)\n",
            "I0619 16:47:28.351594 139649090267008 learning.py:507] global step 186: loss = 0.5188 (2.941 sec/step)\n",
            "I0619 16:47:31.204040 139649090267008 learning.py:507] global step 187: loss = 0.4738 (2.850 sec/step)\n",
            "I0619 16:47:34.573530 139649090267008 learning.py:507] global step 187: loss = 0.4120 (3.368 sec/step)\n",
            "I0619 16:47:37.488974 139649090267008 learning.py:507] global step 187: loss = 0.4760 (2.914 sec/step)\n",
            "I0619 16:47:40.368349 139649090267008 learning.py:507] global step 187: loss = 0.5052 (2.878 sec/step)\n",
            "I0619 16:47:43.292885 139649090267008 learning.py:507] global step 187: loss = 0.5000 (2.923 sec/step)\n",
            "I0619 16:47:46.307669 139649090267008 learning.py:507] global step 187: loss = 0.5131 (3.013 sec/step)\n",
            "I0619 16:47:49.660256 139649090267008 learning.py:507] global step 187: loss = 0.5051 (3.351 sec/step)\n",
            "I0619 16:47:52.896239 139649090267008 learning.py:507] global step 187: loss = 0.4391 (3.234 sec/step)\n",
            "I0619 16:47:55.781383 139649090267008 learning.py:507] global step 188: loss = 0.4479 (2.882 sec/step)\n",
            "I0619 16:47:58.734556 139649090267008 learning.py:507] global step 188: loss = 0.5008 (2.951 sec/step)\n",
            "I0619 16:48:01.645356 139649090267008 learning.py:507] global step 188: loss = 0.5634 (2.909 sec/step)\n",
            "I0619 16:48:04.571291 139649090267008 learning.py:507] global step 188: loss = 0.5003 (2.924 sec/step)\n",
            "I0619 16:48:07.810479 139649090267008 learning.py:507] global step 188: loss = 0.4900 (3.237 sec/step)\n",
            "I0619 16:48:10.688372 139649090267008 learning.py:507] global step 188: loss = 0.5356 (2.876 sec/step)\n",
            "I0619 16:48:13.582695 139649090267008 learning.py:507] global step 188: loss = 0.5882 (2.893 sec/step)\n",
            "I0619 16:48:16.439698 139649090267008 learning.py:507] global step 188: loss = 0.6007 (2.855 sec/step)\n",
            "I0619 16:48:19.375183 139649090267008 learning.py:507] global step 189: loss = 0.4031 (2.934 sec/step)\n",
            "I0619 16:48:23.818272 139649090267008 learning.py:507] global step 189: loss = 0.4713 (4.431 sec/step)\n",
            "I0619 16:48:26.017666 139646017689344 supervisor.py:1050] Recording summary at step 189.\n",
            "I0619 16:48:27.293090 139649090267008 learning.py:507] global step 189: loss = 0.5370 (3.472 sec/step)\n",
            "I0619 16:48:30.138677 139649090267008 learning.py:507] global step 189: loss = 0.5319 (2.844 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:48:33.034167 139649090267008 learning.py:507] global step 189: loss = 0.5053 (2.894 sec/step)\n",
            "I0619 16:48:36.020360 139649090267008 learning.py:507] global step 189: loss = 0.4361 (2.985 sec/step)\n",
            "I0619 16:48:38.961363 139649090267008 learning.py:507] global step 189: loss = 0.4906 (2.939 sec/step)\n",
            "I0619 16:48:41.826063 139649090267008 learning.py:507] global step 189: loss = 0.4621 (2.863 sec/step)\n",
            "I0619 16:48:44.728316 139649090267008 learning.py:507] global step 190: loss = 0.5970 (2.900 sec/step)\n",
            "I0619 16:48:47.621640 139649090267008 learning.py:507] global step 190: loss = 0.5490 (2.892 sec/step)\n",
            "I0619 16:48:50.533496 139649090267008 learning.py:507] global step 190: loss = 0.3918 (2.910 sec/step)\n",
            "I0619 16:48:53.578090 139649090267008 learning.py:507] global step 190: loss = 0.5496 (3.043 sec/step)\n",
            "I0619 16:48:56.495608 139649090267008 learning.py:507] global step 190: loss = 0.5207 (2.916 sec/step)\n",
            "I0619 16:48:59.437460 139649090267008 learning.py:507] global step 190: loss = 0.5103 (2.940 sec/step)\n",
            "I0619 16:49:02.365921 139649090267008 learning.py:507] global step 190: loss = 0.4400 (2.927 sec/step)\n",
            "I0619 16:49:05.420720 139649090267008 learning.py:507] global step 190: loss = 0.4804 (3.053 sec/step)\n",
            "I0619 16:49:08.429979 139649090267008 learning.py:507] global step 191: loss = 0.3989 (3.007 sec/step)\n",
            "I0619 16:49:11.582246 139649090267008 learning.py:507] global step 191: loss = 0.6181 (3.151 sec/step)\n",
            "I0619 16:49:14.525462 139649090267008 learning.py:507] global step 191: loss = 0.4432 (2.942 sec/step)\n",
            "I0619 16:49:17.524888 139649090267008 learning.py:507] global step 191: loss = 0.4381 (2.998 sec/step)\n",
            "I0619 16:49:20.407033 139649090267008 learning.py:507] global step 191: loss = 0.4655 (2.880 sec/step)\n",
            "I0619 16:49:23.432652 139649090267008 learning.py:507] global step 191: loss = 0.5291 (3.024 sec/step)\n",
            "I0619 16:49:26.444843 139649090267008 learning.py:507] global step 191: loss = 0.4734 (3.010 sec/step)\n",
            "I0619 16:49:29.487678 139649090267008 learning.py:507] global step 191: loss = 0.4841 (3.041 sec/step)\n",
            "I0619 16:49:32.428382 139649090267008 learning.py:507] global step 192: loss = 0.4900 (2.939 sec/step)\n",
            "I0619 16:49:35.392005 139649090267008 learning.py:507] global step 192: loss = 0.5342 (2.962 sec/step)\n",
            "I0619 16:49:38.299957 139649090267008 learning.py:507] global step 192: loss = 0.4496 (2.906 sec/step)\n",
            "I0619 16:49:41.187093 139649090267008 learning.py:507] global step 192: loss = 0.5480 (2.885 sec/step)\n",
            "I0619 16:49:44.266063 139649090267008 learning.py:507] global step 192: loss = 0.4660 (3.077 sec/step)\n",
            "I0619 16:49:47.229065 139649090267008 learning.py:507] global step 192: loss = 0.4216 (2.961 sec/step)\n",
            "I0619 16:49:50.183777 139649090267008 learning.py:507] global step 192: loss = 0.5334 (2.953 sec/step)\n",
            "I0619 16:49:53.077237 139649090267008 learning.py:507] global step 192: loss = 0.4168 (2.892 sec/step)\n",
            "I0619 16:49:55.939367 139649090267008 learning.py:507] global step 193: loss = 0.5881 (2.860 sec/step)\n",
            "I0619 16:49:58.860050 139649090267008 learning.py:507] global step 193: loss = 0.4736 (2.919 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:50:01.977269 139649090267008 learning.py:507] global step 193: loss = 0.5480 (3.116 sec/step)\n",
            "I0619 16:50:04.830506 139649090267008 learning.py:507] global step 193: loss = 0.5345 (2.851 sec/step)\n",
            "I0619 16:50:07.766848 139649090267008 learning.py:507] global step 193: loss = 0.4749 (2.935 sec/step)\n",
            "I0619 16:50:10.654736 139649090267008 learning.py:507] global step 193: loss = 0.5884 (2.886 sec/step)\n",
            "I0619 16:50:13.611427 139649090267008 learning.py:507] global step 193: loss = 0.4832 (2.955 sec/step)\n",
            "I0619 16:50:16.506196 139649090267008 learning.py:507] global step 193: loss = 0.4373 (2.893 sec/step)\n",
            "I0619 16:50:19.367640 139649090267008 learning.py:507] global step 194: loss = 0.5061 (2.859 sec/step)\n",
            "I0619 16:50:20.931310 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 16:50:25.379727 139649090267008 learning.py:507] global step 194: loss = 0.4410 (6.010 sec/step)\n",
            "I0619 16:50:26.964796 139646017689344 supervisor.py:1050] Recording summary at step 194.\n",
            "I0619 16:50:28.941556 139649090267008 learning.py:507] global step 194: loss = 0.5366 (3.551 sec/step)\n",
            "I0619 16:50:31.981838 139649090267008 learning.py:507] global step 194: loss = 0.4821 (3.035 sec/step)\n",
            "I0619 16:50:34.991795 139649090267008 learning.py:507] global step 194: loss = 0.5013 (3.008 sec/step)\n",
            "I0619 16:50:37.980492 139649090267008 learning.py:507] global step 194: loss = 0.4834 (2.987 sec/step)\n",
            "I0619 16:50:40.953223 139649090267008 learning.py:507] global step 194: loss = 0.4978 (2.970 sec/step)\n",
            "I0619 16:50:43.901286 139649090267008 learning.py:507] global step 194: loss = 0.4727 (2.946 sec/step)\n",
            "I0619 16:50:46.759690 139649090267008 learning.py:507] global step 195: loss = 0.4691 (2.856 sec/step)\n",
            "I0619 16:50:49.636615 139649090267008 learning.py:507] global step 195: loss = 0.3790 (2.875 sec/step)\n",
            "I0619 16:50:52.564568 139649090267008 learning.py:507] global step 195: loss = 0.6609 (2.926 sec/step)\n",
            "I0619 16:50:55.490573 139649090267008 learning.py:507] global step 195: loss = 0.4859 (2.924 sec/step)\n",
            "I0619 16:50:58.384008 139649090267008 learning.py:507] global step 195: loss = 0.4294 (2.892 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:51:01.336309 139649090267008 learning.py:507] global step 195: loss = 0.5388 (2.951 sec/step)\n",
            "I0619 16:51:04.188162 139649090267008 learning.py:507] global step 195: loss = 0.4258 (2.850 sec/step)\n",
            "I0619 16:51:07.088729 139649090267008 learning.py:507] global step 195: loss = 0.4279 (2.898 sec/step)\n",
            "I0619 16:51:09.984355 139649090267008 learning.py:507] global step 196: loss = 0.3742 (2.893 sec/step)\n",
            "I0619 16:51:12.970939 139649090267008 learning.py:507] global step 196: loss = 0.5637 (2.985 sec/step)\n",
            "I0619 16:51:15.874017 139649090267008 learning.py:507] global step 196: loss = 0.3849 (2.901 sec/step)\n",
            "I0619 16:51:18.735568 139649090267008 learning.py:507] global step 196: loss = 0.5021 (2.860 sec/step)\n",
            "I0619 16:51:21.607048 139649090267008 learning.py:507] global step 196: loss = 0.4413 (2.870 sec/step)\n",
            "I0619 16:51:24.502767 139649090267008 learning.py:507] global step 196: loss = 0.4304 (2.894 sec/step)\n",
            "I0619 16:51:27.462329 139649090267008 learning.py:507] global step 196: loss = 0.4437 (2.958 sec/step)\n",
            "I0619 16:51:30.397824 139649090267008 learning.py:507] global step 196: loss = 0.4042 (2.934 sec/step)\n",
            "I0619 16:51:33.401799 139649090267008 learning.py:507] global step 197: loss = 0.5574 (3.002 sec/step)\n",
            "I0619 16:51:36.292319 139649090267008 learning.py:507] global step 197: loss = 0.4784 (2.889 sec/step)\n",
            "I0619 16:51:39.195696 139649090267008 learning.py:507] global step 197: loss = 0.4514 (2.902 sec/step)\n",
            "I0619 16:51:42.059271 139649090267008 learning.py:507] global step 197: loss = 0.4457 (2.862 sec/step)\n",
            "I0619 16:51:44.989320 139649090267008 learning.py:507] global step 197: loss = 0.5850 (2.928 sec/step)\n",
            "I0619 16:51:48.072156 139649090267008 learning.py:507] global step 197: loss = 0.4277 (3.081 sec/step)\n",
            "I0619 16:51:50.966414 139649090267008 learning.py:507] global step 197: loss = 0.5458 (2.893 sec/step)\n",
            "I0619 16:51:53.940941 139649090267008 learning.py:507] global step 197: loss = 0.4884 (2.973 sec/step)\n",
            "I0619 16:51:56.834259 139649090267008 learning.py:507] global step 198: loss = 0.5303 (2.891 sec/step)\n",
            "I0619 16:51:59.717046 139649090267008 learning.py:507] global step 198: loss = 0.4835 (2.881 sec/step)\n",
            "I0619 16:52:02.636280 139649090267008 learning.py:507] global step 198: loss = 0.4380 (2.918 sec/step)\n",
            "I0619 16:52:05.791804 139649090267008 learning.py:507] global step 198: loss = 0.5277 (3.154 sec/step)\n",
            "I0619 16:52:08.744199 139649090267008 learning.py:507] global step 198: loss = 0.5281 (2.951 sec/step)\n",
            "I0619 16:52:11.621550 139649090267008 learning.py:507] global step 198: loss = 0.4840 (2.876 sec/step)\n",
            "I0619 16:52:14.535051 139649090267008 learning.py:507] global step 198: loss = 0.4227 (2.912 sec/step)\n",
            "I0619 16:52:17.504755 139649090267008 learning.py:507] global step 198: loss = 0.5548 (2.968 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:52:20.381328 139649090267008 learning.py:507] global step 199: loss = 0.4534 (2.874 sec/step)\n",
            "I0619 16:52:25.447314 139649090267008 learning.py:507] global step 199: loss = 0.5445 (5.059 sec/step)\n",
            "I0619 16:52:26.180246 139646017689344 supervisor.py:1050] Recording summary at step 199.\n",
            "I0619 16:52:28.335860 139649090267008 learning.py:507] global step 199: loss = 0.4934 (2.885 sec/step)\n",
            "I0619 16:52:31.206825 139649090267008 learning.py:507] global step 199: loss = 0.5096 (2.869 sec/step)\n",
            "I0619 16:52:34.278702 139649090267008 learning.py:507] global step 199: loss = 0.4675 (3.070 sec/step)\n",
            "I0619 16:52:37.266032 139649090267008 learning.py:507] global step 199: loss = 0.4442 (2.986 sec/step)\n",
            "I0619 16:52:40.203853 139649090267008 learning.py:507] global step 199: loss = 0.5658 (2.936 sec/step)\n",
            "I0619 16:52:43.088108 139649090267008 learning.py:507] global step 199: loss = 0.4841 (2.883 sec/step)\n",
            "I0619 16:52:46.033270 139649090267008 learning.py:507] global step 200: loss = 0.5028 (2.943 sec/step)\n",
            "I0619 16:52:48.899367 139649090267008 learning.py:507] global step 200: loss = 0.4845 (2.864 sec/step)\n",
            "I0619 16:52:51.767220 139649090267008 learning.py:507] global step 200: loss = 0.5038 (2.866 sec/step)\n",
            "I0619 16:52:54.876595 139649090267008 learning.py:507] global step 200: loss = 0.5772 (3.108 sec/step)\n",
            "I0619 16:52:57.784135 139649090267008 learning.py:507] global step 200: loss = 0.5703 (2.906 sec/step)\n",
            "I0619 16:53:00.712610 139649090267008 learning.py:507] global step 200: loss = 0.5045 (2.927 sec/step)\n",
            "I0619 16:53:03.633356 139649090267008 learning.py:507] global step 200: loss = 0.4770 (2.919 sec/step)\n",
            "I0619 16:53:06.633615 139649090267008 learning.py:507] global step 200: loss = 0.4749 (2.999 sec/step)\n",
            "I0619 16:53:09.625132 139649090267008 learning.py:507] global step 201: loss = 0.4237 (2.989 sec/step)\n",
            "I0619 16:53:12.812378 139649090267008 learning.py:507] global step 201: loss = 0.4214 (3.185 sec/step)\n",
            "I0619 16:53:15.702326 139649090267008 learning.py:507] global step 201: loss = 0.4579 (2.888 sec/step)\n",
            "I0619 16:53:18.581800 139649090267008 learning.py:507] global step 201: loss = 0.5090 (2.878 sec/step)\n",
            "I0619 16:53:21.548182 139649090267008 learning.py:507] global step 201: loss = 0.5033 (2.965 sec/step)\n",
            "I0619 16:53:24.465559 139649090267008 learning.py:507] global step 201: loss = 0.5852 (2.916 sec/step)\n",
            "I0619 16:53:27.422070 139649090267008 learning.py:507] global step 201: loss = 0.4556 (2.955 sec/step)\n",
            "I0619 16:53:30.380971 139649090267008 learning.py:507] global step 201: loss = 0.4283 (2.957 sec/step)\n",
            "I0619 16:53:33.310327 139649090267008 learning.py:507] global step 202: loss = 0.4840 (2.927 sec/step)\n",
            "I0619 16:53:36.232762 139649090267008 learning.py:507] global step 202: loss = 0.4542 (2.921 sec/step)\n",
            "I0619 16:53:39.332599 139649090267008 learning.py:507] global step 202: loss = 0.4587 (3.098 sec/step)\n",
            "I0619 16:53:42.246593 139649090267008 learning.py:507] global step 202: loss = 0.4761 (2.912 sec/step)\n",
            "I0619 16:53:45.192009 139649090267008 learning.py:507] global step 202: loss = 0.4388 (2.944 sec/step)\n",
            "I0619 16:53:48.159630 139649090267008 learning.py:507] global step 202: loss = 0.4525 (2.966 sec/step)\n",
            "I0619 16:53:51.112757 139649090267008 learning.py:507] global step 202: loss = 0.3924 (2.951 sec/step)\n",
            "I0619 16:53:53.999499 139649090267008 learning.py:507] global step 202: loss = 0.4004 (2.885 sec/step)\n",
            "I0619 16:53:56.908154 139649090267008 learning.py:507] global step 203: loss = 0.5839 (2.906 sec/step)\n",
            "I0619 16:53:59.862277 139649090267008 learning.py:507] global step 203: loss = 0.4527 (2.953 sec/step)\n",
            "I0619 16:54:02.771345 139649090267008 learning.py:507] global step 203: loss = 0.5709 (2.907 sec/step)\n",
            "I0619 16:54:05.679622 139649090267008 learning.py:507] global step 203: loss = 0.5228 (2.907 sec/step)\n",
            "I0619 16:54:08.609264 139649090267008 learning.py:507] global step 203: loss = 0.4081 (2.928 sec/step)\n",
            "I0619 16:54:11.600063 139649090267008 learning.py:507] global step 203: loss = 0.4802 (2.989 sec/step)\n",
            "I0619 16:54:14.539051 139649090267008 learning.py:507] global step 203: loss = 0.4942 (2.937 sec/step)\n",
            "I0619 16:54:17.407660 139649090267008 learning.py:507] global step 203: loss = 0.3839 (2.867 sec/step)\n",
            "I0619 16:54:20.401531 139649090267008 learning.py:507] global step 204: loss = 0.4554 (2.992 sec/step)\n",
            "I0619 16:54:25.492958 139649090267008 learning.py:507] global step 204: loss = 0.4363 (5.084 sec/step)\n",
            "I0619 16:54:26.180912 139646017689344 supervisor.py:1050] Recording summary at step 204.\n",
            "I0619 16:54:28.480391 139649090267008 learning.py:507] global step 204: loss = 0.5096 (2.985 sec/step)\n",
            "I0619 16:54:31.438771 139649090267008 learning.py:507] global step 204: loss = 0.4424 (2.956 sec/step)\n",
            "I0619 16:54:34.451103 139649090267008 learning.py:507] global step 204: loss = 0.6353 (3.011 sec/step)\n",
            "I0619 16:54:37.361464 139649090267008 learning.py:507] global step 204: loss = 0.5549 (2.909 sec/step)\n",
            "I0619 16:54:40.318197 139649090267008 learning.py:507] global step 204: loss = 0.4988 (2.955 sec/step)\n",
            "I0619 16:54:43.301035 139649090267008 learning.py:507] global step 204: loss = 0.6710 (2.981 sec/step)\n",
            "I0619 16:54:46.269929 139649090267008 learning.py:507] global step 205: loss = 0.4469 (2.966 sec/step)\n",
            "I0619 16:54:49.124534 139649090267008 learning.py:507] global step 205: loss = 0.5466 (2.853 sec/step)\n",
            "I0619 16:54:52.145047 139649090267008 learning.py:507] global step 205: loss = 0.4605 (3.019 sec/step)\n",
            "I0619 16:54:55.092002 139649090267008 learning.py:507] global step 205: loss = 0.4856 (2.945 sec/step)\n",
            "I0619 16:54:58.045856 139649090267008 learning.py:507] global step 205: loss = 0.5188 (2.952 sec/step)\n",
            "I0619 16:55:01.195022 139649090267008 learning.py:507] global step 205: loss = 0.5203 (3.147 sec/step)\n",
            "I0619 16:55:04.087472 139649090267008 learning.py:507] global step 205: loss = 0.4189 (2.891 sec/step)\n",
            "I0619 16:55:07.009531 139649090267008 learning.py:507] global step 205: loss = 0.7251 (2.920 sec/step)\n",
            "I0619 16:55:09.958571 139649090267008 learning.py:507] global step 206: loss = 0.5154 (2.945 sec/step)\n",
            "I0619 16:55:12.959941 139649090267008 learning.py:507] global step 206: loss = 0.4954 (2.999 sec/step)\n",
            "I0619 16:55:15.888589 139649090267008 learning.py:507] global step 206: loss = 0.4418 (2.927 sec/step)\n",
            "I0619 16:55:18.982876 139649090267008 learning.py:507] global step 206: loss = 0.4910 (3.093 sec/step)\n",
            "I0619 16:55:21.885993 139649090267008 learning.py:507] global step 206: loss = 0.4606 (2.901 sec/step)\n",
            "I0619 16:55:24.783913 139649090267008 learning.py:507] global step 206: loss = 0.4670 (2.896 sec/step)\n",
            "I0619 16:55:27.714740 139649090267008 learning.py:507] global step 206: loss = 0.5476 (2.929 sec/step)\n",
            "I0619 16:55:30.700516 139649090267008 learning.py:507] global step 206: loss = 0.5659 (2.984 sec/step)\n",
            "I0619 16:55:33.680988 139649090267008 learning.py:507] global step 207: loss = 0.4443 (2.979 sec/step)\n",
            "I0619 16:55:36.589096 139649090267008 learning.py:507] global step 207: loss = 0.3976 (2.906 sec/step)\n",
            "I0619 16:55:39.543175 139649090267008 learning.py:507] global step 207: loss = 0.4050 (2.952 sec/step)\n",
            "I0619 16:55:42.582063 139649090267008 learning.py:507] global step 207: loss = 0.4494 (3.037 sec/step)\n",
            "I0619 16:55:45.582408 139649090267008 learning.py:507] global step 207: loss = 0.4203 (2.998 sec/step)\n",
            "I0619 16:55:48.535373 139649090267008 learning.py:507] global step 207: loss = 0.4876 (2.951 sec/step)\n",
            "I0619 16:55:51.535624 139649090267008 learning.py:507] global step 207: loss = 0.4121 (2.998 sec/step)\n",
            "I0619 16:55:54.550194 139649090267008 learning.py:507] global step 207: loss = 0.5263 (3.013 sec/step)\n",
            "I0619 16:55:57.538931 139649090267008 learning.py:507] global step 208: loss = 0.4440 (2.986 sec/step)\n",
            "I0619 16:56:00.518951 139649090267008 learning.py:507] global step 208: loss = 0.4509 (2.978 sec/step)\n",
            "I0619 16:56:03.545696 139649090267008 learning.py:507] global step 208: loss = 0.4960 (3.025 sec/step)\n",
            "I0619 16:56:06.494102 139649090267008 learning.py:507] global step 208: loss = 0.4396 (2.946 sec/step)\n",
            "I0619 16:56:09.481228 139649090267008 learning.py:507] global step 208: loss = 0.5641 (2.986 sec/step)\n",
            "I0619 16:56:12.450175 139649090267008 learning.py:507] global step 208: loss = 0.5374 (2.967 sec/step)\n",
            "I0619 16:56:15.506918 139649090267008 learning.py:507] global step 208: loss = 0.4642 (3.055 sec/step)\n",
            "I0619 16:56:18.415401 139649090267008 learning.py:507] global step 208: loss = 0.4365 (2.907 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 16:56:21.391177 139649090267008 learning.py:507] global step 209: loss = 0.4626 (2.973 sec/step)\n",
            "I0619 16:56:26.034111 139646017689344 supervisor.py:1050] Recording summary at step 209.\n",
            "I0619 16:56:26.599601 139649090267008 learning.py:507] global step 209: loss = 0.4815 (5.206 sec/step)\n",
            "I0619 16:56:29.537610 139649090267008 learning.py:507] global step 209: loss = 0.4153 (2.936 sec/step)\n",
            "I0619 16:56:32.632054 139649090267008 learning.py:507] global step 209: loss = 0.4945 (3.093 sec/step)\n",
            "I0619 16:56:35.654603 139649090267008 learning.py:507] global step 209: loss = 0.4280 (3.021 sec/step)\n",
            "I0619 16:56:38.554136 139649090267008 learning.py:507] global step 209: loss = 0.4617 (2.898 sec/step)\n",
            "I0619 16:56:41.478596 139649090267008 learning.py:507] global step 209: loss = 0.4360 (2.923 sec/step)\n",
            "I0619 16:56:44.448836 139649090267008 learning.py:507] global step 209: loss = 0.4647 (2.968 sec/step)\n",
            "I0619 16:56:47.387665 139649090267008 learning.py:507] global step 210: loss = 0.4841 (2.937 sec/step)\n",
            "I0619 16:56:50.359147 139649090267008 learning.py:507] global step 210: loss = 0.3954 (2.970 sec/step)\n",
            "I0619 16:56:53.265853 139649090267008 learning.py:507] global step 210: loss = 0.4737 (2.905 sec/step)\n",
            "I0619 16:56:56.225808 139649090267008 learning.py:507] global step 210: loss = 0.5643 (2.958 sec/step)\n",
            "I0619 16:56:59.156268 139649090267008 learning.py:507] global step 210: loss = 0.4478 (2.929 sec/step)\n",
            "I0619 16:57:02.082831 139649090267008 learning.py:507] global step 210: loss = 0.4439 (2.925 sec/step)\n",
            "I0619 16:57:05.093492 139649090267008 learning.py:507] global step 210: loss = 0.4736 (3.009 sec/step)\n",
            "I0619 16:57:08.122136 139649090267008 learning.py:507] global step 210: loss = 0.5075 (3.027 sec/step)\n",
            "I0619 16:57:11.085855 139649090267008 learning.py:507] global step 211: loss = 0.5637 (2.961 sec/step)\n",
            "I0619 16:57:14.157866 139649090267008 learning.py:507] global step 211: loss = 0.4251 (3.070 sec/step)\n",
            "I0619 16:57:17.147674 139649090267008 learning.py:507] global step 211: loss = 0.5891 (2.988 sec/step)\n",
            "I0619 16:57:20.134567 139649090267008 learning.py:507] global step 211: loss = 0.4654 (2.985 sec/step)\n",
            "I0619 16:57:23.068261 139649090267008 learning.py:507] global step 211: loss = 0.4633 (2.932 sec/step)\n",
            "I0619 16:57:26.012276 139649090267008 learning.py:507] global step 211: loss = 0.4997 (2.942 sec/step)\n",
            "I0619 16:57:28.918815 139649090267008 learning.py:507] global step 211: loss = 0.5159 (2.905 sec/step)\n",
            "I0619 16:57:31.903455 139649090267008 learning.py:507] global step 211: loss = 0.5505 (2.983 sec/step)\n",
            "I0619 16:57:34.788192 139649090267008 learning.py:507] global step 212: loss = 0.4463 (2.882 sec/step)\n",
            "I0619 16:57:37.836109 139649090267008 learning.py:507] global step 212: loss = 0.4915 (3.046 sec/step)\n",
            "I0619 16:57:40.772281 139649090267008 learning.py:507] global step 212: loss = 0.4560 (2.934 sec/step)\n",
            "I0619 16:57:43.755310 139649090267008 learning.py:507] global step 212: loss = 0.4771 (2.981 sec/step)\n",
            "I0619 16:57:46.697311 139649090267008 learning.py:507] global step 212: loss = 0.5123 (2.940 sec/step)\n",
            "I0619 16:57:49.576945 139649090267008 learning.py:507] global step 212: loss = 0.4115 (2.878 sec/step)\n",
            "I0619 16:57:52.476503 139649090267008 learning.py:507] global step 212: loss = 0.4950 (2.898 sec/step)\n",
            "I0619 16:57:55.410099 139649090267008 learning.py:507] global step 212: loss = 0.4938 (2.932 sec/step)\n",
            "I0619 16:57:58.474597 139649090267008 learning.py:507] global step 213: loss = 0.5354 (3.063 sec/step)\n",
            "I0619 16:58:01.346209 139649090267008 learning.py:507] global step 213: loss = 0.5062 (2.870 sec/step)\n",
            "I0619 16:58:04.237240 139649090267008 learning.py:507] global step 213: loss = 0.5461 (2.889 sec/step)\n",
            "I0619 16:58:07.185473 139649090267008 learning.py:507] global step 213: loss = 0.4303 (2.946 sec/step)\n",
            "I0619 16:58:10.135251 139649090267008 learning.py:507] global step 213: loss = 0.5189 (2.948 sec/step)\n",
            "I0619 16:58:13.053391 139649090267008 learning.py:507] global step 213: loss = 0.4924 (2.916 sec/step)\n",
            "I0619 16:58:16.165556 139649090267008 learning.py:507] global step 213: loss = 0.4493 (3.110 sec/step)\n",
            "I0619 16:58:19.134929 139649090267008 learning.py:507] global step 213: loss = 0.4920 (2.968 sec/step)\n",
            "I0619 16:58:23.424023 139649090267008 learning.py:507] global step 214: loss = 0.5319 (4.271 sec/step)\n",
            "I0619 16:58:26.004544 139646017689344 supervisor.py:1050] Recording summary at step 214.\n",
            "I0619 16:58:27.194288 139649090267008 learning.py:507] global step 214: loss = 0.3736 (3.757 sec/step)\n",
            "I0619 16:58:30.140029 139649090267008 learning.py:507] global step 214: loss = 0.3846 (2.944 sec/step)\n",
            "I0619 16:58:33.201264 139649090267008 learning.py:507] global step 214: loss = 0.4927 (3.060 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 16:58:36.219770 139649090267008 learning.py:507] global step 214: loss = 0.4946 (3.017 sec/step)\n",
            "I0619 16:58:39.160053 139649090267008 learning.py:507] global step 214: loss = 0.4074 (2.939 sec/step)\n",
            "I0619 16:58:42.065856 139649090267008 learning.py:507] global step 214: loss = 0.4125 (2.904 sec/step)\n",
            "I0619 16:58:44.979581 139649090267008 learning.py:507] global step 214: loss = 0.4705 (2.912 sec/step)\n",
            "I0619 16:58:47.903094 139649090267008 learning.py:507] global step 215: loss = 0.3755 (2.921 sec/step)\n",
            "I0619 16:58:50.967614 139649090267008 learning.py:507] global step 215: loss = 0.4485 (3.059 sec/step)\n",
            "I0619 16:58:53.962827 139649090267008 learning.py:507] global step 215: loss = 0.4598 (2.993 sec/step)\n",
            "I0619 16:58:56.887644 139649090267008 learning.py:507] global step 215: loss = 0.5437 (2.923 sec/step)\n",
            "I0619 16:58:59.897860 139649090267008 learning.py:507] global step 215: loss = 0.4175 (3.008 sec/step)\n",
            "I0619 16:59:02.785084 139649090267008 learning.py:507] global step 215: loss = 0.4306 (2.885 sec/step)\n",
            "I0619 16:59:05.724578 139649090267008 learning.py:507] global step 215: loss = 0.4787 (2.938 sec/step)\n",
            "I0619 16:59:08.670506 139649090267008 learning.py:507] global step 215: loss = 0.4974 (2.944 sec/step)\n",
            "I0619 16:59:11.726323 139649090267008 learning.py:507] global step 216: loss = 0.4212 (3.053 sec/step)\n",
            "I0619 16:59:14.707129 139649090267008 learning.py:507] global step 216: loss = 0.5061 (2.979 sec/step)\n",
            "I0619 16:59:17.630208 139649090267008 learning.py:507] global step 216: loss = 0.4951 (2.921 sec/step)\n",
            "I0619 16:59:20.496630 139649090267008 learning.py:507] global step 216: loss = 0.4518 (2.865 sec/step)\n",
            "I0619 16:59:23.382321 139649090267008 learning.py:507] global step 216: loss = 0.4402 (2.884 sec/step)\n",
            "I0619 16:59:26.471754 139649090267008 learning.py:507] global step 216: loss = 0.5068 (3.088 sec/step)\n",
            "I0619 16:59:29.377242 139649090267008 learning.py:507] global step 216: loss = 0.4158 (2.904 sec/step)\n",
            "I0619 16:59:32.354575 139649090267008 learning.py:507] global step 216: loss = 0.5020 (2.976 sec/step)\n",
            "I0619 16:59:35.238987 139649090267008 learning.py:507] global step 217: loss = 0.4853 (2.882 sec/step)\n",
            "I0619 16:59:38.253830 139649090267008 learning.py:507] global step 217: loss = 0.4287 (3.013 sec/step)\n",
            "I0619 16:59:41.243304 139649090267008 learning.py:507] global step 217: loss = 0.4709 (2.988 sec/step)\n",
            "I0619 16:59:44.407503 139649090267008 learning.py:507] global step 217: loss = 0.4203 (3.162 sec/step)\n",
            "I0619 16:59:47.525082 139649090267008 learning.py:507] global step 217: loss = 0.5764 (3.115 sec/step)\n",
            "I0619 16:59:50.451431 139649090267008 learning.py:507] global step 217: loss = 0.4778 (2.924 sec/step)\n",
            "I0619 16:59:53.362591 139649090267008 learning.py:507] global step 217: loss = 0.4664 (2.910 sec/step)\n",
            "I0619 16:59:56.291823 139649090267008 learning.py:507] global step 217: loss = 0.3844 (2.927 sec/step)\n",
            "I0619 16:59:59.268333 139649090267008 learning.py:507] global step 218: loss = 0.5428 (2.974 sec/step)\n",
            "I0619 17:00:02.219651 139649090267008 learning.py:507] global step 218: loss = 0.4092 (2.950 sec/step)\n",
            "I0619 17:00:05.437255 139649090267008 learning.py:507] global step 218: loss = 0.5109 (3.216 sec/step)\n",
            "I0619 17:00:08.296478 139649090267008 learning.py:507] global step 218: loss = 0.4944 (2.858 sec/step)\n",
            "I0619 17:00:11.246614 139649090267008 learning.py:507] global step 218: loss = 0.4695 (2.949 sec/step)\n",
            "I0619 17:00:14.158846 139649090267008 learning.py:507] global step 218: loss = 0.5059 (2.910 sec/step)\n",
            "I0619 17:00:17.079934 139649090267008 learning.py:507] global step 218: loss = 0.4729 (2.919 sec/step)\n",
            "I0619 17:00:19.970191 139649090267008 learning.py:507] global step 218: loss = 0.4570 (2.888 sec/step)\n",
            "I0619 17:00:20.931331 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 17:00:26.574551 139646017689344 supervisor.py:1050] Recording summary at step 219.\n",
            "I0619 17:00:26.742056 139649090267008 learning.py:507] global step 219: loss = 0.4309 (6.603 sec/step)\n",
            "I0619 17:00:29.688595 139649090267008 learning.py:507] global step 219: loss = 0.3931 (2.943 sec/step)\n",
            "I0619 17:00:32.714165 139649090267008 learning.py:507] global step 219: loss = 0.4721 (3.024 sec/step)\n",
            "I0619 17:00:35.712218 139649090267008 learning.py:507] global step 219: loss = 0.4269 (2.996 sec/step)\n",
            "I0619 17:00:38.709456 139649090267008 learning.py:507] global step 219: loss = 0.4909 (2.995 sec/step)\n",
            "I0619 17:00:41.717406 139649090267008 learning.py:507] global step 219: loss = 0.4082 (3.007 sec/step)\n",
            "I0619 17:00:44.689527 139649090267008 learning.py:507] global step 219: loss = 0.5156 (2.970 sec/step)\n",
            "I0619 17:00:47.639599 139649090267008 learning.py:507] global step 219: loss = 0.4288 (2.948 sec/step)\n",
            "I0619 17:00:50.626213 139649090267008 learning.py:507] global step 220: loss = 0.4291 (2.984 sec/step)\n",
            "I0619 17:00:53.631380 139649090267008 learning.py:507] global step 220: loss = 0.4764 (3.003 sec/step)\n",
            "I0619 17:00:56.666361 139649090267008 learning.py:507] global step 220: loss = 0.4584 (3.033 sec/step)\n",
            "I0619 17:00:59.705669 139649090267008 learning.py:507] global step 220: loss = 0.4490 (3.038 sec/step)\n",
            "I0619 17:01:02.727259 139649090267008 learning.py:507] global step 220: loss = 0.4248 (3.020 sec/step)\n",
            "I0619 17:01:05.779292 139649090267008 learning.py:507] global step 220: loss = 0.4564 (3.050 sec/step)\n",
            "I0619 17:01:08.833753 139649090267008 learning.py:507] global step 220: loss = 0.4488 (3.053 sec/step)\n",
            "I0619 17:01:11.899217 139649090267008 learning.py:507] global step 220: loss = 0.4462 (3.064 sec/step)\n",
            "I0619 17:01:15.009729 139649090267008 learning.py:507] global step 221: loss = 0.4663 (3.109 sec/step)\n",
            "I0619 17:01:18.011412 139649090267008 learning.py:507] global step 221: loss = 0.5286 (3.000 sec/step)\n",
            "I0619 17:01:21.012953 139649090267008 learning.py:507] global step 221: loss = 0.5790 (3.000 sec/step)\n",
            "I0619 17:01:23.967633 139649090267008 learning.py:507] global step 221: loss = 0.5330 (2.953 sec/step)\n",
            "I0619 17:01:26.952895 139649090267008 learning.py:507] global step 221: loss = 0.4108 (2.983 sec/step)\n",
            "I0619 17:01:29.956108 139649090267008 learning.py:507] global step 221: loss = 0.5934 (3.001 sec/step)\n",
            "I0619 17:01:33.158276 139649090267008 learning.py:507] global step 221: loss = 0.5983 (3.200 sec/step)\n",
            "I0619 17:01:36.147105 139649090267008 learning.py:507] global step 221: loss = 0.5547 (2.987 sec/step)\n",
            "I0619 17:01:39.149152 139649090267008 learning.py:507] global step 222: loss = 0.4793 (3.000 sec/step)\n",
            "I0619 17:01:42.133228 139649090267008 learning.py:507] global step 222: loss = 0.6452 (2.982 sec/step)\n",
            "I0619 17:01:45.192303 139649090267008 learning.py:507] global step 222: loss = 0.4650 (3.057 sec/step)\n",
            "I0619 17:01:48.207715 139649090267008 learning.py:507] global step 222: loss = 0.4696 (3.014 sec/step)\n",
            "I0619 17:01:51.217032 139649090267008 learning.py:507] global step 222: loss = 0.4615 (3.008 sec/step)\n",
            "I0619 17:01:54.200602 139649090267008 learning.py:507] global step 222: loss = 0.3691 (2.982 sec/step)\n",
            "I0619 17:01:57.137540 139649090267008 learning.py:507] global step 222: loss = 0.4763 (2.935 sec/step)\n",
            "I0619 17:02:00.209288 139649090267008 learning.py:507] global step 222: loss = 0.4523 (3.070 sec/step)\n",
            "I0619 17:02:03.237121 139649090267008 learning.py:507] global step 223: loss = 0.5067 (3.026 sec/step)\n",
            "I0619 17:02:06.232379 139649090267008 learning.py:507] global step 223: loss = 0.5611 (2.993 sec/step)\n",
            "I0619 17:02:09.207541 139649090267008 learning.py:507] global step 223: loss = 0.4677 (2.973 sec/step)\n",
            "I0619 17:02:12.229892 139649090267008 learning.py:507] global step 223: loss = 0.5105 (3.021 sec/step)\n",
            "I0619 17:02:15.214451 139649090267008 learning.py:507] global step 223: loss = 0.4733 (2.983 sec/step)\n",
            "I0619 17:02:18.375936 139649090267008 learning.py:507] global step 223: loss = 0.4548 (3.158 sec/step)\n",
            "I0619 17:02:21.781271 139649090267008 learning.py:507] global step 223: loss = 0.3977 (3.232 sec/step)\n",
            "I0619 17:02:25.946162 139646017689344 supervisor.py:1050] Recording summary at step 223.\n",
            "I0619 17:02:26.683231 139649090267008 learning.py:507] global step 223: loss = 0.4342 (4.877 sec/step)\n",
            "I0619 17:02:29.692858 139649090267008 learning.py:507] global step 224: loss = 0.6155 (3.007 sec/step)\n",
            "I0619 17:02:32.720190 139649090267008 learning.py:507] global step 224: loss = 0.3840 (3.026 sec/step)\n",
            "I0619 17:02:35.769657 139649090267008 learning.py:507] global step 224: loss = 0.4193 (3.048 sec/step)\n",
            "I0619 17:02:38.735247 139649090267008 learning.py:507] global step 224: loss = 0.4185 (2.964 sec/step)\n",
            "I0619 17:02:41.748810 139649090267008 learning.py:507] global step 224: loss = 0.4624 (3.012 sec/step)\n",
            "I0619 17:02:44.718761 139649090267008 learning.py:507] global step 224: loss = 0.4304 (2.968 sec/step)\n",
            "I0619 17:02:47.685764 139649090267008 learning.py:507] global step 224: loss = 0.5036 (2.965 sec/step)\n",
            "I0619 17:02:50.711908 139649090267008 learning.py:507] global step 224: loss = 0.4408 (3.025 sec/step)\n",
            "I0619 17:02:53.735753 139649090267008 learning.py:507] global step 225: loss = 0.4556 (3.021 sec/step)\n",
            "I0619 17:02:56.734038 139649090267008 learning.py:507] global step 225: loss = 0.4894 (2.997 sec/step)\n",
            "I0619 17:02:59.753403 139649090267008 learning.py:507] global step 225: loss = 0.4856 (3.018 sec/step)\n",
            "I0619 17:03:02.750305 139649090267008 learning.py:507] global step 225: loss = 0.5577 (2.995 sec/step)\n",
            "I0619 17:03:05.760652 139649090267008 learning.py:507] global step 225: loss = 0.4323 (3.008 sec/step)\n",
            "I0619 17:03:08.853394 139649090267008 learning.py:507] global step 225: loss = 0.4481 (3.091 sec/step)\n",
            "I0619 17:03:11.925540 139649090267008 learning.py:507] global step 225: loss = 0.3953 (3.070 sec/step)\n",
            "I0619 17:03:14.933867 139649090267008 learning.py:507] global step 225: loss = 0.4781 (3.007 sec/step)\n",
            "I0619 17:03:17.903271 139649090267008 learning.py:507] global step 226: loss = 0.5632 (2.967 sec/step)\n",
            "I0619 17:03:20.894936 139649090267008 learning.py:507] global step 226: loss = 0.4101 (2.989 sec/step)\n",
            "I0619 17:03:23.869413 139649090267008 learning.py:507] global step 226: loss = 0.5939 (2.972 sec/step)\n",
            "I0619 17:03:27.071997 139649090267008 learning.py:507] global step 226: loss = 0.5764 (3.201 sec/step)\n",
            "I0619 17:03:30.066864 139649090267008 learning.py:507] global step 226: loss = 0.4471 (2.993 sec/step)\n",
            "I0619 17:03:33.062402 139649090267008 learning.py:507] global step 226: loss = 0.5551 (2.994 sec/step)\n",
            "I0619 17:03:36.064361 139649090267008 learning.py:507] global step 226: loss = 0.5917 (3.000 sec/step)\n",
            "I0619 17:03:39.086565 139649090267008 learning.py:507] global step 226: loss = 0.4055 (3.020 sec/step)\n",
            "I0619 17:03:42.115406 139649090267008 learning.py:507] global step 227: loss = 0.6588 (3.026 sec/step)\n",
            "I0619 17:03:45.413633 139649090267008 learning.py:507] global step 227: loss = 0.4766 (3.296 sec/step)\n",
            "I0619 17:03:48.379594 139649090267008 learning.py:507] global step 227: loss = 0.4729 (2.964 sec/step)\n",
            "I0619 17:03:51.463166 139649090267008 learning.py:507] global step 227: loss = 0.4320 (3.082 sec/step)\n",
            "I0619 17:03:54.525514 139649090267008 learning.py:507] global step 227: loss = 0.3992 (3.061 sec/step)\n",
            "I0619 17:03:57.536940 139649090267008 learning.py:507] global step 227: loss = 0.3748 (3.010 sec/step)\n",
            "I0619 17:04:00.498750 139649090267008 learning.py:507] global step 227: loss = 0.4650 (2.960 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:04:03.522508 139649090267008 learning.py:507] global step 227: loss = 0.4917 (3.022 sec/step)\n",
            "I0619 17:04:06.525502 139649090267008 learning.py:507] global step 228: loss = 0.4305 (3.001 sec/step)\n",
            "I0619 17:04:09.495211 139649090267008 learning.py:507] global step 228: loss = 0.4441 (2.968 sec/step)\n",
            "I0619 17:04:12.441260 139649090267008 learning.py:507] global step 228: loss = 0.5061 (2.944 sec/step)\n",
            "I0619 17:04:15.487113 139649090267008 learning.py:507] global step 228: loss = 0.5324 (3.044 sec/step)\n",
            "I0619 17:04:18.509192 139649090267008 learning.py:507] global step 228: loss = 0.5796 (3.020 sec/step)\n",
            "I0619 17:04:22.345955 139649090267008 learning.py:507] global step 228: loss = 0.4703 (3.832 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:04:26.128407 139646017689344 supervisor.py:1050] Recording summary at step 228.\n",
            "I0619 17:04:27.000621 139649090267008 learning.py:507] global step 228: loss = 0.4261 (4.641 sec/step)\n",
            "I0619 17:04:29.988775 139649090267008 learning.py:507] global step 228: loss = 0.5178 (2.986 sec/step)\n",
            "I0619 17:04:33.019934 139649090267008 learning.py:507] global step 229: loss = 0.4330 (3.029 sec/step)\n",
            "I0619 17:04:35.978919 139649090267008 learning.py:507] global step 229: loss = 0.4338 (2.957 sec/step)\n",
            "I0619 17:04:39.042183 139649090267008 learning.py:507] global step 229: loss = 0.5940 (3.062 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:04:42.061529 139649090267008 learning.py:507] global step 229: loss = 0.6215 (3.017 sec/step)\n",
            "I0619 17:04:45.044209 139649090267008 learning.py:507] global step 229: loss = 0.5156 (2.981 sec/step)\n",
            "I0619 17:04:48.071816 139649090267008 learning.py:507] global step 229: loss = 0.4161 (3.026 sec/step)\n",
            "I0619 17:04:51.015662 139649090267008 learning.py:507] global step 229: loss = 0.4418 (2.942 sec/step)\n",
            "I0619 17:04:53.991576 139649090267008 learning.py:507] global step 229: loss = 0.5563 (2.974 sec/step)\n",
            "I0619 17:04:56.980479 139649090267008 learning.py:507] global step 230: loss = 0.4945 (2.987 sec/step)\n",
            "I0619 17:04:59.944756 139649090267008 learning.py:507] global step 230: loss = 0.5417 (2.963 sec/step)\n",
            "I0619 17:05:02.907311 139649090267008 learning.py:507] global step 230: loss = 0.5097 (2.961 sec/step)\n",
            "I0619 17:05:05.879135 139649090267008 learning.py:507] global step 230: loss = 0.3844 (2.970 sec/step)\n",
            "I0619 17:05:08.910239 139649090267008 learning.py:507] global step 230: loss = 0.3623 (3.029 sec/step)\n",
            "I0619 17:05:11.910484 139649090267008 learning.py:507] global step 230: loss = 0.4744 (2.999 sec/step)\n",
            "I0619 17:05:14.932120 139649090267008 learning.py:507] global step 230: loss = 0.5103 (3.020 sec/step)\n",
            "I0619 17:05:17.956316 139649090267008 learning.py:507] global step 230: loss = 0.5083 (3.022 sec/step)\n",
            "I0619 17:05:20.913588 139649090267008 learning.py:507] global step 231: loss = 0.4258 (2.955 sec/step)\n",
            "I0619 17:05:23.797244 139649090267008 learning.py:507] global step 231: loss = 0.5074 (2.882 sec/step)\n",
            "I0619 17:05:26.801209 139649090267008 learning.py:507] global step 231: loss = 0.3879 (3.002 sec/step)\n",
            "I0619 17:05:29.767510 139649090267008 learning.py:507] global step 231: loss = 0.3829 (2.965 sec/step)\n",
            "I0619 17:05:32.665768 139649090267008 learning.py:507] global step 231: loss = 0.5969 (2.896 sec/step)\n",
            "I0619 17:05:35.651928 139649090267008 learning.py:507] global step 231: loss = 0.4713 (2.984 sec/step)\n",
            "I0619 17:05:38.680364 139649090267008 learning.py:507] global step 231: loss = 0.5149 (3.027 sec/step)\n",
            "I0619 17:05:41.689781 139649090267008 learning.py:507] global step 231: loss = 0.5035 (3.008 sec/step)\n",
            "I0619 17:05:44.666632 139649090267008 learning.py:507] global step 232: loss = 0.7482 (2.975 sec/step)\n",
            "I0619 17:05:47.687511 139649090267008 learning.py:507] global step 232: loss = 0.4996 (3.019 sec/step)\n",
            "I0619 17:05:50.641889 139649090267008 learning.py:507] global step 232: loss = 0.5353 (2.953 sec/step)\n",
            "I0619 17:05:53.648763 139649090267008 learning.py:507] global step 232: loss = 0.4107 (3.005 sec/step)\n",
            "I0619 17:05:56.628976 139649090267008 learning.py:507] global step 232: loss = 0.4246 (2.978 sec/step)\n",
            "I0619 17:05:59.747833 139649090267008 learning.py:507] global step 232: loss = 0.4831 (3.117 sec/step)\n",
            "I0619 17:06:02.663521 139649090267008 learning.py:507] global step 232: loss = 0.4757 (2.914 sec/step)\n",
            "I0619 17:06:05.706669 139649090267008 learning.py:507] global step 232: loss = 0.4301 (3.041 sec/step)\n",
            "I0619 17:06:08.834921 139649090267008 learning.py:507] global step 233: loss = 0.4082 (3.126 sec/step)\n",
            "I0619 17:06:11.856810 139649090267008 learning.py:507] global step 233: loss = 0.4864 (3.020 sec/step)\n",
            "I0619 17:06:14.796627 139649090267008 learning.py:507] global step 233: loss = 0.4742 (2.938 sec/step)\n",
            "I0619 17:06:17.972362 139649090267008 learning.py:507] global step 233: loss = 0.4689 (3.174 sec/step)\n",
            "I0619 17:06:21.196704 139649090267008 learning.py:507] global step 233: loss = 0.5993 (3.023 sec/step)\n",
            "I0619 17:06:26.447372 139649090267008 learning.py:507] global step 233: loss = 0.4316 (5.126 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:06:26.963059 139646017689344 supervisor.py:1050] Recording summary at step 233.\n",
            "I0619 17:06:29.444988 139649090267008 learning.py:507] global step 233: loss = 0.4839 (2.995 sec/step)\n",
            "I0619 17:06:32.464742 139649090267008 learning.py:507] global step 233: loss = 0.4711 (3.018 sec/step)\n",
            "I0619 17:06:35.444666 139649090267008 learning.py:507] global step 234: loss = 0.6793 (2.977 sec/step)\n",
            "I0619 17:06:38.539891 139649090267008 learning.py:507] global step 234: loss = 0.5330 (3.093 sec/step)\n",
            "I0619 17:06:41.540289 139649090267008 learning.py:507] global step 234: loss = 0.4547 (2.999 sec/step)\n",
            "I0619 17:06:44.487883 139649090267008 learning.py:507] global step 234: loss = 0.4795 (2.946 sec/step)\n",
            "I0619 17:06:47.518421 139649090267008 learning.py:507] global step 234: loss = 0.4562 (3.029 sec/step)\n",
            "I0619 17:06:50.549182 139649090267008 learning.py:507] global step 234: loss = 0.4069 (3.029 sec/step)\n",
            "I0619 17:06:53.519019 139649090267008 learning.py:507] global step 234: loss = 0.3950 (2.968 sec/step)\n",
            "I0619 17:06:56.594926 139649090267008 learning.py:507] global step 234: loss = 0.4714 (3.074 sec/step)\n",
            "I0619 17:06:59.640934 139649090267008 learning.py:507] global step 235: loss = 0.3920 (3.044 sec/step)\n",
            "I0619 17:07:02.639856 139649090267008 learning.py:507] global step 235: loss = 0.5878 (2.997 sec/step)\n",
            "I0619 17:07:05.665208 139649090267008 learning.py:507] global step 235: loss = 0.4751 (3.024 sec/step)\n",
            "I0619 17:07:08.725822 139649090267008 learning.py:507] global step 235: loss = 0.5478 (3.059 sec/step)\n",
            "I0619 17:07:11.663865 139649090267008 learning.py:507] global step 235: loss = 0.4742 (2.936 sec/step)\n",
            "I0619 17:07:14.765392 139649090267008 learning.py:507] global step 235: loss = 0.5761 (3.100 sec/step)\n",
            "I0619 17:07:17.749443 139649090267008 learning.py:507] global step 235: loss = 0.4248 (2.982 sec/step)\n",
            "I0619 17:07:20.723156 139649090267008 learning.py:507] global step 235: loss = 0.4062 (2.969 sec/step)\n",
            "I0619 17:07:23.657771 139649090267008 learning.py:507] global step 236: loss = 0.4558 (2.932 sec/step)\n",
            "I0619 17:07:26.595614 139649090267008 learning.py:507] global step 236: loss = 0.4632 (2.936 sec/step)\n",
            "I0619 17:07:29.563060 139649090267008 learning.py:507] global step 236: loss = 0.4031 (2.965 sec/step)\n",
            "I0619 17:07:32.494617 139649090267008 learning.py:507] global step 236: loss = 0.4883 (2.930 sec/step)\n",
            "I0619 17:07:35.468466 139649090267008 learning.py:507] global step 236: loss = 0.4464 (2.972 sec/step)\n",
            "I0619 17:07:38.442309 139649090267008 learning.py:507] global step 236: loss = 0.3993 (2.972 sec/step)\n",
            "I0619 17:07:41.426360 139649090267008 learning.py:507] global step 236: loss = 0.4847 (2.982 sec/step)\n",
            "I0619 17:07:44.361490 139649090267008 learning.py:507] global step 236: loss = 0.4858 (2.933 sec/step)\n",
            "I0619 17:07:47.369621 139649090267008 learning.py:507] global step 237: loss = 0.3833 (3.006 sec/step)\n",
            "I0619 17:07:50.433292 139649090267008 learning.py:507] global step 237: loss = 0.5061 (3.062 sec/step)\n",
            "I0619 17:07:53.468193 139649090267008 learning.py:507] global step 237: loss = 0.4167 (3.033 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:07:56.466309 139649090267008 learning.py:507] global step 237: loss = 0.4854 (2.996 sec/step)\n",
            "I0619 17:07:59.439680 139649090267008 learning.py:507] global step 237: loss = 0.4935 (2.972 sec/step)\n",
            "I0619 17:08:02.363644 139649090267008 learning.py:507] global step 237: loss = 0.4364 (2.922 sec/step)\n",
            "I0619 17:08:05.363372 139649090267008 learning.py:507] global step 237: loss = 0.3391 (2.998 sec/step)\n",
            "I0619 17:08:08.363488 139649090267008 learning.py:507] global step 237: loss = 0.5003 (2.998 sec/step)\n",
            "I0619 17:08:11.280479 139649090267008 learning.py:507] global step 238: loss = 0.5121 (2.915 sec/step)\n",
            "I0619 17:08:14.189375 139649090267008 learning.py:507] global step 238: loss = 0.4574 (2.907 sec/step)\n",
            "I0619 17:08:17.155650 139649090267008 learning.py:507] global step 238: loss = 0.5972 (2.965 sec/step)\n",
            "I0619 17:08:20.144405 139649090267008 learning.py:507] global step 238: loss = 0.6085 (2.987 sec/step)\n",
            "I0619 17:08:25.321911 139649090267008 learning.py:507] global step 238: loss = 0.5711 (5.175 sec/step)\n",
            "I0619 17:08:26.158443 139646017689344 supervisor.py:1050] Recording summary at step 238.\n",
            "I0619 17:08:28.427073 139649090267008 learning.py:507] global step 238: loss = 0.4796 (3.100 sec/step)\n",
            "I0619 17:08:31.451297 139649090267008 learning.py:507] global step 238: loss = 0.4718 (3.022 sec/step)\n",
            "I0619 17:08:34.427068 139649090267008 learning.py:507] global step 238: loss = 0.4985 (2.974 sec/step)\n",
            "I0619 17:08:37.418213 139649090267008 learning.py:507] global step 239: loss = 0.4641 (2.989 sec/step)\n",
            "I0619 17:08:40.518248 139649090267008 learning.py:507] global step 239: loss = 0.5870 (3.098 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:08:43.573251 139649090267008 learning.py:507] global step 239: loss = 0.4531 (3.053 sec/step)\n",
            "I0619 17:08:46.603797 139649090267008 learning.py:507] global step 239: loss = 0.6334 (3.029 sec/step)\n",
            "I0619 17:08:49.646445 139649090267008 learning.py:507] global step 239: loss = 0.5463 (3.041 sec/step)\n",
            "I0619 17:08:52.641606 139649090267008 learning.py:507] global step 239: loss = 0.4119 (2.993 sec/step)\n",
            "I0619 17:08:55.657256 139649090267008 learning.py:507] global step 239: loss = 0.4571 (3.014 sec/step)\n",
            "I0619 17:08:58.682187 139649090267008 learning.py:507] global step 239: loss = 0.5430 (3.023 sec/step)\n",
            "I0619 17:09:01.679432 139649090267008 learning.py:507] global step 240: loss = 0.4025 (2.995 sec/step)\n",
            "I0619 17:09:04.685152 139649090267008 learning.py:507] global step 240: loss = 0.4213 (3.004 sec/step)\n",
            "I0619 17:09:07.767274 139649090267008 learning.py:507] global step 240: loss = 0.5382 (3.080 sec/step)\n",
            "I0619 17:09:10.788331 139649090267008 learning.py:507] global step 240: loss = 0.3975 (3.019 sec/step)\n",
            "I0619 17:09:13.812079 139649090267008 learning.py:507] global step 240: loss = 0.4220 (3.022 sec/step)\n",
            "I0619 17:09:16.845539 139649090267008 learning.py:507] global step 240: loss = 0.5225 (3.032 sec/step)\n",
            "I0619 17:09:19.850725 139649090267008 learning.py:507] global step 240: loss = 0.4190 (3.004 sec/step)\n",
            "I0619 17:09:22.873502 139649090267008 learning.py:507] global step 240: loss = 0.4361 (3.021 sec/step)\n",
            "I0619 17:09:25.909439 139649090267008 learning.py:507] global step 241: loss = 0.4628 (3.034 sec/step)\n",
            "I0619 17:09:28.876544 139649090267008 learning.py:507] global step 241: loss = 0.4504 (2.966 sec/step)\n",
            "I0619 17:09:31.822936 139649090267008 learning.py:507] global step 241: loss = 0.4082 (2.945 sec/step)\n",
            "I0619 17:09:34.798884 139649090267008 learning.py:507] global step 241: loss = 0.4891 (2.974 sec/step)\n",
            "I0619 17:09:37.814082 139649090267008 learning.py:507] global step 241: loss = 0.3772 (3.013 sec/step)\n",
            "I0619 17:09:40.770785 139649090267008 learning.py:507] global step 241: loss = 0.6062 (2.955 sec/step)\n",
            "I0619 17:09:43.793699 139649090267008 learning.py:507] global step 241: loss = 0.5256 (3.021 sec/step)\n",
            "I0619 17:09:46.730156 139649090267008 learning.py:507] global step 241: loss = 0.4753 (2.935 sec/step)\n",
            "I0619 17:09:49.819326 139649090267008 learning.py:507] global step 242: loss = 0.4397 (3.087 sec/step)\n",
            "I0619 17:09:52.808594 139649090267008 learning.py:507] global step 242: loss = 0.5022 (2.987 sec/step)\n",
            "I0619 17:09:55.856851 139649090267008 learning.py:507] global step 242: loss = 0.4459 (3.047 sec/step)\n",
            "I0619 17:09:58.823863 139649090267008 learning.py:507] global step 242: loss = 0.4415 (2.965 sec/step)\n",
            "I0619 17:10:01.729485 139649090267008 learning.py:507] global step 242: loss = 0.5143 (2.904 sec/step)\n",
            "I0619 17:10:04.694101 139649090267008 learning.py:507] global step 242: loss = 0.4591 (2.963 sec/step)\n",
            "I0619 17:10:07.734198 139649090267008 learning.py:507] global step 242: loss = 0.4232 (3.038 sec/step)\n",
            "I0619 17:10:10.790234 139649090267008 learning.py:507] global step 242: loss = 0.4071 (3.054 sec/step)\n",
            "I0619 17:10:13.743863 139649090267008 learning.py:507] global step 243: loss = 0.4890 (2.951 sec/step)\n",
            "I0619 17:10:16.810168 139649090267008 learning.py:507] global step 243: loss = 0.4384 (3.065 sec/step)\n",
            "I0619 17:10:19.751327 139649090267008 learning.py:507] global step 243: loss = 0.5205 (2.939 sec/step)\n",
            "I0619 17:10:20.931787 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 17:10:25.823781 139649090267008 learning.py:507] global step 243: loss = 0.5151 (6.064 sec/step)\n",
            "I0619 17:10:27.133793 139646017689344 supervisor.py:1050] Recording summary at step 243.\n",
            "I0619 17:10:29.751274 139649090267008 learning.py:507] global step 243: loss = 0.3918 (3.925 sec/step)\n",
            "I0619 17:10:32.803641 139649090267008 learning.py:507] global step 243: loss = 0.4471 (3.051 sec/step)\n",
            "I0619 17:10:35.982918 139649090267008 learning.py:507] global step 243: loss = 0.4233 (3.178 sec/step)\n",
            "I0619 17:10:39.055183 139649090267008 learning.py:507] global step 243: loss = 0.4144 (3.071 sec/step)\n",
            "I0619 17:10:42.048703 139649090267008 learning.py:507] global step 244: loss = 0.4236 (2.991 sec/step)\n",
            "I0619 17:10:45.036302 139649090267008 learning.py:507] global step 244: loss = 0.4980 (2.986 sec/step)\n",
            "I0619 17:10:48.047286 139649090267008 learning.py:507] global step 244: loss = 0.4332 (3.009 sec/step)\n",
            "I0619 17:10:51.060697 139649090267008 learning.py:507] global step 244: loss = 0.3871 (3.012 sec/step)\n",
            "I0619 17:10:54.329117 139649090267008 learning.py:507] global step 244: loss = 0.4646 (3.267 sec/step)\n",
            "I0619 17:10:57.303380 139649090267008 learning.py:507] global step 244: loss = 0.4488 (2.973 sec/step)\n",
            "I0619 17:11:00.237926 139649090267008 learning.py:507] global step 244: loss = 0.4932 (2.933 sec/step)\n",
            "I0619 17:11:03.218826 139649090267008 learning.py:507] global step 244: loss = 0.4899 (2.979 sec/step)\n",
            "I0619 17:11:06.213349 139649090267008 learning.py:507] global step 245: loss = 0.4902 (2.992 sec/step)\n",
            "I0619 17:11:09.205037 139649090267008 learning.py:507] global step 245: loss = 0.4631 (2.990 sec/step)\n",
            "I0619 17:11:12.592104 139649090267008 learning.py:507] global step 245: loss = 0.5292 (3.385 sec/step)\n",
            "I0619 17:11:15.617200 139649090267008 learning.py:507] global step 245: loss = 0.4994 (3.023 sec/step)\n",
            "I0619 17:11:18.642752 139649090267008 learning.py:507] global step 245: loss = 0.4399 (3.024 sec/step)\n",
            "I0619 17:11:21.673999 139649090267008 learning.py:507] global step 245: loss = 0.4703 (3.029 sec/step)\n",
            "I0619 17:11:24.798463 139649090267008 learning.py:507] global step 245: loss = 0.4157 (3.123 sec/step)\n",
            "I0619 17:11:27.746801 139649090267008 learning.py:507] global step 245: loss = 0.6208 (2.947 sec/step)\n",
            "I0619 17:11:30.699432 139649090267008 learning.py:507] global step 246: loss = 0.3627 (2.950 sec/step)\n",
            "I0619 17:11:33.673712 139649090267008 learning.py:507] global step 246: loss = 0.5385 (2.972 sec/step)\n",
            "I0619 17:11:36.594046 139649090267008 learning.py:507] global step 246: loss = 0.4331 (2.919 sec/step)\n",
            "I0619 17:11:39.543456 139649090267008 learning.py:507] global step 246: loss = 0.3968 (2.948 sec/step)\n",
            "I0619 17:11:42.569086 139649090267008 learning.py:507] global step 246: loss = 0.4273 (3.024 sec/step)\n",
            "I0619 17:11:45.526469 139649090267008 learning.py:507] global step 246: loss = 0.4278 (2.956 sec/step)\n",
            "I0619 17:11:48.488265 139649090267008 learning.py:507] global step 246: loss = 0.5984 (2.960 sec/step)\n",
            "I0619 17:11:51.533220 139649090267008 learning.py:507] global step 246: loss = 0.5219 (3.043 sec/step)\n",
            "I0619 17:11:54.560033 139649090267008 learning.py:507] global step 247: loss = 0.3742 (3.025 sec/step)\n",
            "I0619 17:11:57.569369 139649090267008 learning.py:507] global step 247: loss = 0.4169 (3.008 sec/step)\n",
            "I0619 17:12:00.532658 139649090267008 learning.py:507] global step 247: loss = 0.4157 (2.962 sec/step)\n",
            "I0619 17:12:03.462517 139649090267008 learning.py:507] global step 247: loss = 0.3859 (2.928 sec/step)\n",
            "I0619 17:12:06.557743 139649090267008 learning.py:507] global step 247: loss = 0.4279 (3.094 sec/step)\n",
            "I0619 17:12:09.518952 139649090267008 learning.py:507] global step 247: loss = 0.3964 (2.960 sec/step)\n",
            "I0619 17:12:12.459602 139649090267008 learning.py:507] global step 247: loss = 0.5333 (2.939 sec/step)\n",
            "I0619 17:12:15.488046 139649090267008 learning.py:507] global step 247: loss = 0.4059 (3.027 sec/step)\n",
            "I0619 17:12:18.448180 139649090267008 learning.py:507] global step 248: loss = 0.5165 (2.958 sec/step)\n",
            "I0619 17:12:22.223196 139649090267008 learning.py:507] global step 248: loss = 0.4539 (3.755 sec/step)\n",
            "I0619 17:12:26.057046 139646017689344 supervisor.py:1050] Recording summary at step 248.\n",
            "I0619 17:12:26.802043 139649090267008 learning.py:507] global step 248: loss = 0.6056 (4.577 sec/step)\n",
            "I0619 17:12:29.699311 139649090267008 learning.py:507] global step 248: loss = 0.4703 (2.895 sec/step)\n",
            "I0619 17:12:32.619277 139649090267008 learning.py:507] global step 248: loss = 0.4962 (2.918 sec/step)\n",
            "I0619 17:12:35.569740 139649090267008 learning.py:507] global step 248: loss = 0.4611 (2.949 sec/step)\n",
            "I0619 17:12:38.693325 139649090267008 learning.py:507] global step 248: loss = 0.5322 (3.122 sec/step)\n",
            "I0619 17:12:41.710148 139649090267008 learning.py:507] global step 248: loss = 0.3947 (3.015 sec/step)\n",
            "I0619 17:12:44.676112 139649090267008 learning.py:507] global step 249: loss = 0.3740 (2.963 sec/step)\n",
            "I0619 17:12:47.647939 139649090267008 learning.py:507] global step 249: loss = 0.4361 (2.970 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:12:50.611740 139649090267008 learning.py:507] global step 249: loss = 0.4501 (2.962 sec/step)\n",
            "I0619 17:12:53.595094 139649090267008 learning.py:507] global step 249: loss = 0.4049 (2.981 sec/step)\n",
            "I0619 17:12:56.608304 139649090267008 learning.py:507] global step 249: loss = 0.4734 (3.010 sec/step)\n",
            "I0619 17:12:59.554463 139649090267008 learning.py:507] global step 249: loss = 0.3892 (2.944 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:13:02.538213 139649090267008 learning.py:507] global step 249: loss = 0.4644 (2.982 sec/step)\n",
            "I0619 17:13:05.517980 139649090267008 learning.py:507] global step 249: loss = 0.4899 (2.978 sec/step)\n",
            "I0619 17:13:08.516717 139649090267008 learning.py:507] global step 250: loss = 0.4406 (2.997 sec/step)\n",
            "I0619 17:13:11.491856 139649090267008 learning.py:507] global step 250: loss = 0.5039 (2.973 sec/step)\n",
            "I0619 17:13:14.501033 139649090267008 learning.py:507] global step 250: loss = 0.4862 (3.007 sec/step)\n",
            "I0619 17:13:17.541989 139649090267008 learning.py:507] global step 250: loss = 0.4244 (3.039 sec/step)\n",
            "I0619 17:13:20.530898 139649090267008 learning.py:507] global step 250: loss = 0.4444 (2.987 sec/step)\n",
            "I0619 17:13:23.475347 139649090267008 learning.py:507] global step 250: loss = 0.4630 (2.941 sec/step)\n",
            "I0619 17:13:26.440116 139649090267008 learning.py:507] global step 250: loss = 0.4440 (2.963 sec/step)\n",
            "I0619 17:13:29.404438 139649090267008 learning.py:507] global step 250: loss = 0.5314 (2.963 sec/step)\n",
            "I0619 17:13:32.397489 139649090267008 learning.py:507] global step 251: loss = 0.4567 (2.991 sec/step)\n",
            "I0619 17:13:35.339720 139649090267008 learning.py:507] global step 251: loss = 0.4768 (2.940 sec/step)\n",
            "I0619 17:13:38.328842 139649090267008 learning.py:507] global step 251: loss = 0.5576 (2.987 sec/step)\n",
            "I0619 17:13:41.307495 139649090267008 learning.py:507] global step 251: loss = 0.4727 (2.977 sec/step)\n",
            "I0619 17:13:44.283947 139649090267008 learning.py:507] global step 251: loss = 0.4084 (2.975 sec/step)\n",
            "I0619 17:13:47.199955 139649090267008 learning.py:507] global step 251: loss = 0.5575 (2.914 sec/step)\n",
            "I0619 17:13:50.264041 139649090267008 learning.py:507] global step 251: loss = 0.4683 (3.062 sec/step)\n",
            "I0619 17:13:53.263382 139649090267008 learning.py:507] global step 251: loss = 0.4251 (2.998 sec/step)\n",
            "I0619 17:13:56.265091 139649090267008 learning.py:507] global step 252: loss = 0.4149 (3.000 sec/step)\n",
            "I0619 17:13:59.223410 139649090267008 learning.py:507] global step 252: loss = 0.4396 (2.956 sec/step)\n",
            "I0619 17:14:02.187824 139649090267008 learning.py:507] global step 252: loss = 0.5258 (2.963 sec/step)\n",
            "I0619 17:14:05.251367 139649090267008 learning.py:507] global step 252: loss = 0.4619 (3.062 sec/step)\n",
            "I0619 17:14:08.255275 139649090267008 learning.py:507] global step 252: loss = 0.3802 (3.002 sec/step)\n",
            "I0619 17:14:11.247020 139649090267008 learning.py:507] global step 252: loss = 0.4206 (2.990 sec/step)\n",
            "I0619 17:14:14.289048 139649090267008 learning.py:507] global step 252: loss = 0.3730 (3.040 sec/step)\n",
            "I0619 17:14:17.287860 139649090267008 learning.py:507] global step 252: loss = 0.4226 (2.997 sec/step)\n",
            "I0619 17:14:20.212462 139649090267008 learning.py:507] global step 253: loss = 0.4172 (2.923 sec/step)\n",
            "I0619 17:14:25.458616 139649090267008 learning.py:507] global step 253: loss = 0.4612 (5.242 sec/step)\n",
            "I0619 17:14:26.479347 139646017689344 supervisor.py:1050] Recording summary at step 253.\n",
            "I0619 17:14:28.704724 139649090267008 learning.py:507] global step 253: loss = 0.4125 (3.243 sec/step)\n",
            "I0619 17:14:31.832347 139649090267008 learning.py:507] global step 253: loss = 0.4351 (3.126 sec/step)\n",
            "I0619 17:14:34.825459 139649090267008 learning.py:507] global step 253: loss = 0.3619 (2.992 sec/step)\n",
            "I0619 17:14:37.849747 139649090267008 learning.py:507] global step 253: loss = 0.4042 (3.023 sec/step)\n",
            "I0619 17:14:40.841196 139649090267008 learning.py:507] global step 253: loss = 0.4960 (2.990 sec/step)\n",
            "I0619 17:14:43.856886 139649090267008 learning.py:507] global step 253: loss = 0.6896 (3.014 sec/step)\n",
            "I0619 17:14:46.787083 139649090267008 learning.py:507] global step 254: loss = 0.5194 (2.929 sec/step)\n",
            "I0619 17:14:49.924075 139649090267008 learning.py:507] global step 254: loss = 0.4521 (3.135 sec/step)\n",
            "I0619 17:14:52.967690 139649090267008 learning.py:507] global step 254: loss = 0.4478 (3.042 sec/step)\n",
            "I0619 17:14:55.967247 139649090267008 learning.py:507] global step 254: loss = 0.4037 (2.998 sec/step)\n",
            "I0619 17:14:58.988724 139649090267008 learning.py:507] global step 254: loss = 0.5818 (3.020 sec/step)\n",
            "I0619 17:15:02.074328 139649090267008 learning.py:507] global step 254: loss = 0.4538 (3.084 sec/step)\n",
            "I0619 17:15:05.049725 139649090267008 learning.py:507] global step 254: loss = 0.4258 (2.974 sec/step)\n",
            "I0619 17:15:07.988543 139649090267008 learning.py:507] global step 254: loss = 0.4723 (2.937 sec/step)\n",
            "I0619 17:15:10.919945 139649090267008 learning.py:507] global step 255: loss = 0.4873 (2.929 sec/step)\n",
            "I0619 17:15:13.881595 139649090267008 learning.py:507] global step 255: loss = 0.4604 (2.960 sec/step)\n",
            "I0619 17:15:16.857336 139649090267008 learning.py:507] global step 255: loss = 0.5003 (2.974 sec/step)\n",
            "I0619 17:15:19.841630 139649090267008 learning.py:507] global step 255: loss = 0.5003 (2.983 sec/step)\n",
            "I0619 17:15:22.782314 139649090267008 learning.py:507] global step 255: loss = 0.4408 (2.939 sec/step)\n",
            "I0619 17:15:25.739080 139649090267008 learning.py:507] global step 255: loss = 0.5593 (2.955 sec/step)\n",
            "I0619 17:15:28.728632 139649090267008 learning.py:507] global step 255: loss = 0.4269 (2.988 sec/step)\n",
            "I0619 17:15:31.690173 139649090267008 learning.py:507] global step 255: loss = 0.5741 (2.960 sec/step)\n",
            "I0619 17:15:34.637557 139649090267008 learning.py:507] global step 256: loss = 0.4922 (2.945 sec/step)\n",
            "I0619 17:15:37.722287 139649090267008 learning.py:507] global step 256: loss = 0.4928 (3.078 sec/step)\n",
            "I0619 17:15:40.708986 139649090267008 learning.py:507] global step 256: loss = 0.4716 (2.985 sec/step)\n",
            "I0619 17:15:43.673393 139649090267008 learning.py:507] global step 256: loss = 0.4052 (2.963 sec/step)\n",
            "I0619 17:15:46.613573 139649090267008 learning.py:507] global step 256: loss = 0.4274 (2.939 sec/step)\n",
            "I0619 17:15:49.629445 139649090267008 learning.py:507] global step 256: loss = 0.4526 (3.014 sec/step)\n",
            "I0619 17:15:52.614451 139649090267008 learning.py:507] global step 256: loss = 0.4953 (2.983 sec/step)\n",
            "I0619 17:15:55.619211 139649090267008 learning.py:507] global step 256: loss = 0.3888 (3.003 sec/step)\n",
            "I0619 17:15:58.619228 139649090267008 learning.py:507] global step 257: loss = 0.4443 (2.998 sec/step)\n",
            "I0619 17:16:01.670694 139649090267008 learning.py:507] global step 257: loss = 0.3687 (3.050 sec/step)\n",
            "I0619 17:16:04.612570 139649090267008 learning.py:507] global step 257: loss = 0.4513 (2.940 sec/step)\n",
            "I0619 17:16:07.526012 139649090267008 learning.py:507] global step 257: loss = 0.3871 (2.911 sec/step)\n",
            "I0619 17:16:10.484204 139649090267008 learning.py:507] global step 257: loss = 0.3808 (2.956 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:16:13.436877 139649090267008 learning.py:507] global step 257: loss = 0.5160 (2.951 sec/step)\n",
            "I0619 17:16:16.402520 139649090267008 learning.py:507] global step 257: loss = 0.5777 (2.964 sec/step)\n",
            "I0619 17:16:19.357631 139649090267008 learning.py:507] global step 257: loss = 0.4183 (2.953 sec/step)\n",
            "I0619 17:16:24.066150 139649090267008 learning.py:507] global step 258: loss = 0.4679 (4.700 sec/step)\n",
            "I0619 17:16:26.052537 139646017689344 supervisor.py:1050] Recording summary at step 258.\n",
            "I0619 17:16:27.685597 139649090267008 learning.py:507] global step 258: loss = 0.4451 (3.617 sec/step)\n",
            "I0619 17:16:30.713217 139649090267008 learning.py:507] global step 258: loss = 0.3687 (3.026 sec/step)\n",
            "I0619 17:16:33.716689 139649090267008 learning.py:507] global step 258: loss = 0.3621 (3.002 sec/step)\n",
            "I0619 17:16:36.722953 139649090267008 learning.py:507] global step 258: loss = 0.5821 (3.005 sec/step)\n",
            "I0619 17:16:39.659676 139649090267008 learning.py:507] global step 258: loss = 0.4067 (2.935 sec/step)\n",
            "I0619 17:16:42.609354 139649090267008 learning.py:507] global step 258: loss = 0.4595 (2.948 sec/step)\n",
            "I0619 17:16:45.599343 139649090267008 learning.py:507] global step 258: loss = 0.4354 (2.988 sec/step)\n",
            "I0619 17:16:48.507332 139649090267008 learning.py:507] global step 259: loss = 0.4107 (2.906 sec/step)\n",
            "I0619 17:16:51.552992 139649090267008 learning.py:507] global step 259: loss = 0.4080 (3.044 sec/step)\n",
            "I0619 17:16:54.500923 139649090267008 learning.py:507] global step 259: loss = 0.3914 (2.946 sec/step)\n",
            "I0619 17:16:57.404030 139649090267008 learning.py:507] global step 259: loss = 0.6385 (2.902 sec/step)\n",
            "I0619 17:17:00.321298 139649090267008 learning.py:507] global step 259: loss = 0.3550 (2.915 sec/step)\n",
            "I0619 17:17:03.278199 139649090267008 learning.py:507] global step 259: loss = 0.3932 (2.955 sec/step)\n",
            "I0619 17:17:06.191560 139649090267008 learning.py:507] global step 259: loss = 0.4066 (2.912 sec/step)\n",
            "I0619 17:17:09.284976 139649090267008 learning.py:507] global step 259: loss = 0.4327 (3.092 sec/step)\n",
            "I0619 17:17:12.251392 139649090267008 learning.py:507] global step 260: loss = 0.3465 (2.964 sec/step)\n",
            "I0619 17:17:15.225289 139649090267008 learning.py:507] global step 260: loss = 0.4042 (2.972 sec/step)\n",
            "I0619 17:17:18.191310 139649090267008 learning.py:507] global step 260: loss = 0.4393 (2.964 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:17:21.173752 139649090267008 learning.py:507] global step 260: loss = 0.4332 (2.981 sec/step)\n",
            "I0619 17:17:24.124358 139649090267008 learning.py:507] global step 260: loss = 0.4120 (2.949 sec/step)\n",
            "I0619 17:17:27.071439 139649090267008 learning.py:507] global step 260: loss = 0.5608 (2.945 sec/step)\n",
            "I0619 17:17:29.992762 139649090267008 learning.py:507] global step 260: loss = 0.4051 (2.920 sec/step)\n",
            "I0619 17:17:32.951770 139649090267008 learning.py:507] global step 260: loss = 0.4059 (2.957 sec/step)\n",
            "I0619 17:17:35.905100 139649090267008 learning.py:507] global step 261: loss = 0.4661 (2.951 sec/step)\n",
            "I0619 17:17:38.843863 139649090267008 learning.py:507] global step 261: loss = 0.5384 (2.937 sec/step)\n",
            "I0619 17:17:41.796235 139649090267008 learning.py:507] global step 261: loss = 0.4065 (2.951 sec/step)\n",
            "I0619 17:17:44.816843 139649090267008 learning.py:507] global step 261: loss = 0.4129 (3.019 sec/step)\n",
            "I0619 17:17:47.777343 139649090267008 learning.py:507] global step 261: loss = 0.4473 (2.959 sec/step)\n",
            "I0619 17:17:50.733245 139649090267008 learning.py:507] global step 261: loss = 0.4175 (2.954 sec/step)\n",
            "I0619 17:17:53.782456 139649090267008 learning.py:507] global step 261: loss = 0.4221 (3.048 sec/step)\n",
            "I0619 17:17:56.712988 139649090267008 learning.py:507] global step 261: loss = 0.4791 (2.929 sec/step)\n",
            "I0619 17:17:59.674932 139649090267008 learning.py:507] global step 262: loss = 0.4091 (2.960 sec/step)\n",
            "I0619 17:18:02.721673 139649090267008 learning.py:507] global step 262: loss = 0.5689 (3.045 sec/step)\n",
            "I0619 17:18:05.673166 139649090267008 learning.py:507] global step 262: loss = 0.5089 (2.950 sec/step)\n",
            "I0619 17:18:08.603275 139649090267008 learning.py:507] global step 262: loss = 0.3945 (2.929 sec/step)\n",
            "I0619 17:18:11.537858 139649090267008 learning.py:507] global step 262: loss = 0.3669 (2.933 sec/step)\n",
            "I0619 17:18:14.857630 139649090267008 learning.py:507] global step 262: loss = 0.6577 (3.318 sec/step)\n",
            "I0619 17:18:17.835173 139649090267008 learning.py:507] global step 262: loss = 0.4019 (2.976 sec/step)\n",
            "I0619 17:18:20.822885 139649090267008 learning.py:507] global step 262: loss = 0.4891 (2.986 sec/step)\n",
            "I0619 17:18:25.533526 139646017689344 supervisor.py:1050] Recording summary at step 262.\n",
            "I0619 17:18:26.158903 139649090267008 learning.py:507] global step 263: loss = 0.4604 (5.333 sec/step)\n",
            "I0619 17:18:29.100708 139649090267008 learning.py:507] global step 263: loss = 0.4480 (2.940 sec/step)\n",
            "I0619 17:18:32.618399 139649090267008 learning.py:507] global step 263: loss = 0.4169 (3.516 sec/step)\n",
            "I0619 17:18:35.623164 139649090267008 learning.py:507] global step 263: loss = 0.4521 (3.003 sec/step)\n",
            "I0619 17:18:38.738312 139649090267008 learning.py:507] global step 263: loss = 0.4204 (3.113 sec/step)\n",
            "I0619 17:18:41.787012 139649090267008 learning.py:507] global step 263: loss = 0.4008 (3.047 sec/step)\n",
            "I0619 17:18:44.929021 139649090267008 learning.py:507] global step 263: loss = 0.5108 (3.140 sec/step)\n",
            "I0619 17:18:48.154719 139649090267008 learning.py:507] global step 263: loss = 0.4493 (3.224 sec/step)\n",
            "I0619 17:18:51.163844 139649090267008 learning.py:507] global step 264: loss = 0.4270 (3.007 sec/step)\n",
            "I0619 17:18:55.929556 139649090267008 learning.py:507] global step 264: loss = 0.4569 (4.764 sec/step)\n",
            "I0619 17:18:58.998648 139649090267008 learning.py:507] global step 264: loss = 0.4097 (3.067 sec/step)\n",
            "I0619 17:19:02.153091 139649090267008 learning.py:507] global step 264: loss = 0.4220 (3.152 sec/step)\n",
            "I0619 17:19:05.359915 139649090267008 learning.py:507] global step 264: loss = 0.4314 (3.205 sec/step)\n",
            "I0619 17:19:08.880057 139649090267008 learning.py:507] global step 264: loss = 0.4537 (3.518 sec/step)\n",
            "I0619 17:19:11.882490 139649090267008 learning.py:507] global step 264: loss = 0.4003 (3.001 sec/step)\n",
            "I0619 17:19:15.898160 139649090267008 learning.py:507] global step 264: loss = 0.3241 (4.014 sec/step)\n",
            "I0619 17:19:19.045533 139649090267008 learning.py:507] global step 265: loss = 0.4388 (3.145 sec/step)\n",
            "I0619 17:19:22.061775 139649090267008 learning.py:507] global step 265: loss = 0.5016 (3.015 sec/step)\n",
            "I0619 17:19:25.089095 139649090267008 learning.py:507] global step 265: loss = 0.4311 (3.026 sec/step)\n",
            "I0619 17:19:28.108567 139649090267008 learning.py:507] global step 265: loss = 0.4638 (3.018 sec/step)\n",
            "I0619 17:19:31.137453 139649090267008 learning.py:507] global step 265: loss = 0.4894 (3.027 sec/step)\n",
            "I0619 17:19:34.254410 139649090267008 learning.py:507] global step 265: loss = 0.3857 (3.115 sec/step)\n",
            "I0619 17:19:37.201436 139649090267008 learning.py:507] global step 265: loss = 0.3922 (2.945 sec/step)\n",
            "I0619 17:19:40.141429 139649090267008 learning.py:507] global step 265: loss = 0.3965 (2.937 sec/step)\n",
            "I0619 17:19:43.152348 139649090267008 learning.py:507] global step 266: loss = 0.3834 (3.009 sec/step)\n",
            "I0619 17:19:46.138339 139649090267008 learning.py:507] global step 266: loss = 0.4447 (2.984 sec/step)\n",
            "I0619 17:19:49.411077 139649090267008 learning.py:507] global step 266: loss = 0.4443 (3.271 sec/step)\n",
            "I0619 17:19:52.490604 139649090267008 learning.py:507] global step 266: loss = 0.4781 (3.078 sec/step)\n",
            "I0619 17:19:55.494763 139649090267008 learning.py:507] global step 266: loss = 0.4558 (3.002 sec/step)\n",
            "I0619 17:19:58.525117 139649090267008 learning.py:507] global step 266: loss = 0.4464 (3.028 sec/step)\n",
            "I0619 17:20:01.561861 139649090267008 learning.py:507] global step 266: loss = 0.4190 (3.035 sec/step)\n",
            "I0619 17:20:04.647974 139649090267008 learning.py:507] global step 266: loss = 0.4626 (3.084 sec/step)\n",
            "I0619 17:20:07.764495 139649090267008 learning.py:507] global step 267: loss = 0.3865 (3.114 sec/step)\n",
            "I0619 17:20:10.766817 139649090267008 learning.py:507] global step 267: loss = 0.3502 (3.000 sec/step)\n",
            "I0619 17:20:13.730151 139649090267008 learning.py:507] global step 267: loss = 0.4050 (2.962 sec/step)\n",
            "I0619 17:20:16.747638 139649090267008 learning.py:507] global step 267: loss = 0.3744 (3.016 sec/step)\n",
            "I0619 17:20:19.911769 139649090267008 learning.py:507] global step 267: loss = 0.4937 (3.162 sec/step)\n",
            "I0619 17:20:20.934110 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:20:26.954888 139649090267008 learning.py:507] global step 267: loss = 0.5655 (6.849 sec/step)\n",
            "I0619 17:20:26.955561 139646017689344 supervisor.py:1050] Recording summary at step 267.\n",
            "I0619 17:20:29.976807 139649090267008 learning.py:507] global step 267: loss = 0.4241 (3.014 sec/step)\n",
            "I0619 17:20:32.951100 139649090267008 learning.py:507] global step 267: loss = 0.4218 (2.973 sec/step)\n",
            "I0619 17:20:36.077275 139649090267008 learning.py:507] global step 268: loss = 0.4058 (3.123 sec/step)\n",
            "I0619 17:20:39.067734 139649090267008 learning.py:507] global step 268: loss = 0.5136 (2.989 sec/step)\n",
            "I0619 17:20:42.043640 139649090267008 learning.py:507] global step 268: loss = 0.5556 (2.974 sec/step)\n",
            "I0619 17:20:45.003716 139649090267008 learning.py:507] global step 268: loss = 0.5262 (2.958 sec/step)\n",
            "I0619 17:20:47.954757 139649090267008 learning.py:507] global step 268: loss = 0.4302 (2.949 sec/step)\n",
            "I0619 17:20:50.949798 139649090267008 learning.py:507] global step 268: loss = 0.5901 (2.993 sec/step)\n",
            "I0619 17:20:53.966203 139649090267008 learning.py:507] global step 268: loss = 0.3791 (3.015 sec/step)\n",
            "I0619 17:20:56.968123 139649090267008 learning.py:507] global step 268: loss = 0.4279 (3.000 sec/step)\n",
            "I0619 17:20:59.970392 139649090267008 learning.py:507] global step 269: loss = 0.4287 (2.998 sec/step)\n",
            "I0619 17:21:02.993087 139649090267008 learning.py:507] global step 269: loss = 0.5387 (3.021 sec/step)\n",
            "I0619 17:21:06.005335 139649090267008 learning.py:507] global step 269: loss = 0.6516 (3.010 sec/step)\n",
            "I0619 17:21:08.912276 139649090267008 learning.py:507] global step 269: loss = 0.4453 (2.905 sec/step)\n",
            "I0619 17:21:11.896796 139649090267008 learning.py:507] global step 269: loss = 0.5117 (2.983 sec/step)\n",
            "I0619 17:21:14.942092 139649090267008 learning.py:507] global step 269: loss = 0.4119 (3.044 sec/step)\n",
            "I0619 17:21:17.903277 139649090267008 learning.py:507] global step 269: loss = 0.4262 (2.960 sec/step)\n",
            "I0619 17:21:20.914700 139649090267008 learning.py:507] global step 269: loss = 0.4483 (3.010 sec/step)\n",
            "I0619 17:21:23.938055 139649090267008 learning.py:507] global step 270: loss = 0.4510 (3.021 sec/step)\n",
            "I0619 17:21:26.913319 139649090267008 learning.py:507] global step 270: loss = 0.3949 (2.974 sec/step)\n",
            "I0619 17:21:29.935854 139649090267008 learning.py:507] global step 270: loss = 0.4488 (3.021 sec/step)\n",
            "I0619 17:21:32.832786 139649090267008 learning.py:507] global step 270: loss = 0.4998 (2.895 sec/step)\n",
            "I0619 17:21:35.854500 139649090267008 learning.py:507] global step 270: loss = 0.4524 (3.020 sec/step)\n",
            "I0619 17:21:39.033000 139649090267008 learning.py:507] global step 270: loss = 0.4715 (3.177 sec/step)\n",
            "I0619 17:21:42.064549 139649090267008 learning.py:507] global step 270: loss = 0.3947 (3.030 sec/step)\n",
            "I0619 17:21:45.069632 139649090267008 learning.py:507] global step 270: loss = 0.3927 (3.003 sec/step)\n",
            "I0619 17:21:48.065429 139649090267008 learning.py:507] global step 271: loss = 0.4208 (2.993 sec/step)\n",
            "I0619 17:21:50.979157 139649090267008 learning.py:507] global step 271: loss = 0.5196 (2.912 sec/step)\n",
            "I0619 17:21:54.040749 139649090267008 learning.py:507] global step 271: loss = 0.4514 (3.060 sec/step)\n",
            "I0619 17:21:57.053801 139649090267008 learning.py:507] global step 271: loss = 0.3537 (3.011 sec/step)\n",
            "I0619 17:22:00.050206 139649090267008 learning.py:507] global step 271: loss = 0.4724 (2.995 sec/step)\n",
            "I0619 17:22:03.082561 139649090267008 learning.py:507] global step 271: loss = 0.3405 (3.031 sec/step)\n",
            "I0619 17:22:06.041455 139649090267008 learning.py:507] global step 271: loss = 0.4965 (2.957 sec/step)\n",
            "I0619 17:22:08.964676 139649090267008 learning.py:507] global step 271: loss = 0.3831 (2.922 sec/step)\n",
            "I0619 17:22:12.062414 139649090267008 learning.py:507] global step 272: loss = 0.3505 (3.095 sec/step)\n",
            "I0619 17:22:14.986050 139649090267008 learning.py:507] global step 272: loss = 0.3571 (2.922 sec/step)\n",
            "I0619 17:22:17.915932 139649090267008 learning.py:507] global step 272: loss = 0.3934 (2.928 sec/step)\n",
            "I0619 17:22:20.888833 139649090267008 learning.py:507] global step 272: loss = 0.4000 (2.971 sec/step)\n",
            "I0619 17:22:26.052876 139649090267008 learning.py:507] global step 272: loss = 0.4367 (5.162 sec/step)\n",
            "I0619 17:22:26.679341 139646017689344 supervisor.py:1050] Recording summary at step 272.\n",
            "I0619 17:22:29.062406 139649090267008 learning.py:507] global step 272: loss = 0.4491 (3.007 sec/step)\n",
            "I0619 17:22:32.042429 139649090267008 learning.py:507] global step 272: loss = 0.3921 (2.978 sec/step)\n",
            "I0619 17:22:35.084230 139649090267008 learning.py:507] global step 272: loss = 0.4521 (3.040 sec/step)\n",
            "I0619 17:22:38.080359 139649090267008 learning.py:507] global step 273: loss = 0.4316 (2.994 sec/step)\n",
            "I0619 17:22:41.069203 139649090267008 learning.py:507] global step 273: loss = 0.3713 (2.987 sec/step)\n",
            "I0619 17:22:43.986317 139649090267008 learning.py:507] global step 273: loss = 0.4769 (2.915 sec/step)\n",
            "I0619 17:22:46.918855 139649090267008 learning.py:507] global step 273: loss = 0.5263 (2.931 sec/step)\n",
            "I0619 17:22:49.823091 139649090267008 learning.py:507] global step 273: loss = 0.3786 (2.902 sec/step)\n",
            "I0619 17:22:52.788424 139649090267008 learning.py:507] global step 273: loss = 0.4619 (2.964 sec/step)\n",
            "I0619 17:22:55.811706 139649090267008 learning.py:507] global step 273: loss = 0.4043 (3.022 sec/step)\n",
            "I0619 17:22:58.724425 139649090267008 learning.py:507] global step 273: loss = 0.4776 (2.911 sec/step)\n",
            "I0619 17:23:01.671879 139649090267008 learning.py:507] global step 274: loss = 0.4221 (2.945 sec/step)\n",
            "I0619 17:23:04.657585 139649090267008 learning.py:507] global step 274: loss = 0.5091 (2.984 sec/step)\n",
            "I0619 17:23:07.661192 139649090267008 learning.py:507] global step 274: loss = 0.4050 (3.002 sec/step)\n",
            "I0619 17:23:10.611557 139649090267008 learning.py:507] global step 274: loss = 0.4620 (2.949 sec/step)\n",
            "I0619 17:23:13.542890 139649090267008 learning.py:507] global step 274: loss = 0.4218 (2.930 sec/step)\n",
            "I0619 17:23:16.490916 139649090267008 learning.py:507] global step 274: loss = 0.4822 (2.946 sec/step)\n",
            "I0619 17:23:19.410930 139649090267008 learning.py:507] global step 274: loss = 0.4215 (2.918 sec/step)\n",
            "I0619 17:23:22.468156 139649090267008 learning.py:507] global step 274: loss = 0.3527 (3.056 sec/step)\n",
            "I0619 17:23:25.447312 139649090267008 learning.py:507] global step 275: loss = 0.3387 (2.976 sec/step)\n",
            "I0619 17:23:28.388812 139649090267008 learning.py:507] global step 275: loss = 0.5733 (2.940 sec/step)\n",
            "I0619 17:23:31.293730 139649090267008 learning.py:507] global step 275: loss = 0.5142 (2.901 sec/step)\n",
            "I0619 17:23:34.222650 139649090267008 learning.py:507] global step 275: loss = 0.4938 (2.927 sec/step)\n",
            "I0619 17:23:37.221178 139649090267008 learning.py:507] global step 275: loss = 0.4961 (2.997 sec/step)\n",
            "I0619 17:23:40.322713 139649090267008 learning.py:507] global step 275: loss = 0.4127 (3.100 sec/step)\n",
            "I0619 17:23:43.254232 139649090267008 learning.py:507] global step 275: loss = 0.5448 (2.930 sec/step)\n",
            "I0619 17:23:46.245222 139649090267008 learning.py:507] global step 275: loss = 0.4467 (2.989 sec/step)\n",
            "I0619 17:23:49.143820 139649090267008 learning.py:507] global step 276: loss = 0.3260 (2.896 sec/step)\n",
            "I0619 17:23:52.065150 139649090267008 learning.py:507] global step 276: loss = 0.5469 (2.920 sec/step)\n",
            "I0619 17:23:55.042852 139649090267008 learning.py:507] global step 276: loss = 0.4411 (2.976 sec/step)\n",
            "I0619 17:23:57.996130 139649090267008 learning.py:507] global step 276: loss = 0.4101 (2.952 sec/step)\n",
            "I0619 17:24:01.016252 139649090267008 learning.py:507] global step 276: loss = 0.4975 (3.018 sec/step)\n",
            "I0619 17:24:03.959716 139649090267008 learning.py:507] global step 276: loss = 0.4107 (2.942 sec/step)\n",
            "I0619 17:24:06.904410 139649090267008 learning.py:507] global step 276: loss = 0.5719 (2.943 sec/step)\n",
            "I0619 17:24:10.288427 139649090267008 learning.py:507] global step 276: loss = 0.3934 (3.382 sec/step)\n",
            "I0619 17:24:13.203555 139649090267008 learning.py:507] global step 277: loss = 0.5618 (2.913 sec/step)\n",
            "I0619 17:24:16.096767 139649090267008 learning.py:507] global step 277: loss = 0.4924 (2.892 sec/step)\n",
            "I0619 17:24:19.026751 139649090267008 learning.py:507] global step 277: loss = 0.3740 (2.928 sec/step)\n",
            "I0619 17:24:23.230834 139649090267008 learning.py:507] global step 277: loss = 0.5104 (4.192 sec/step)\n",
            "I0619 17:24:26.324918 139646017689344 supervisor.py:1050] Recording summary at step 277.\n",
            "I0619 17:24:27.504316 139649090267008 learning.py:507] global step 277: loss = 0.4461 (4.264 sec/step)\n",
            "I0619 17:24:30.448082 139649090267008 learning.py:507] global step 277: loss = 0.5379 (2.942 sec/step)\n",
            "I0619 17:24:33.332167 139649090267008 learning.py:507] global step 277: loss = 0.5141 (2.882 sec/step)\n",
            "I0619 17:24:36.249980 139649090267008 learning.py:507] global step 277: loss = 0.4319 (2.916 sec/step)\n",
            "I0619 17:24:39.206064 139649090267008 learning.py:507] global step 278: loss = 0.4696 (2.954 sec/step)\n",
            "I0619 17:24:42.185115 139649090267008 learning.py:507] global step 278: loss = 0.4568 (2.977 sec/step)\n",
            "I0619 17:24:45.120314 139649090267008 learning.py:507] global step 278: loss = 0.4379 (2.933 sec/step)\n",
            "I0619 17:24:48.021826 139649090267008 learning.py:507] global step 278: loss = 0.3821 (2.900 sec/step)\n",
            "I0619 17:24:50.961142 139649090267008 learning.py:507] global step 278: loss = 0.4063 (2.938 sec/step)\n",
            "I0619 17:24:53.911074 139649090267008 learning.py:507] global step 278: loss = 0.3935 (2.948 sec/step)\n",
            "I0619 17:24:56.839582 139649090267008 learning.py:507] global step 278: loss = 0.3617 (2.927 sec/step)\n",
            "I0619 17:24:59.843356 139649090267008 learning.py:507] global step 278: loss = 0.4021 (3.002 sec/step)\n",
            "I0619 17:25:02.765073 139649090267008 learning.py:507] global step 279: loss = 0.5044 (2.919 sec/step)\n",
            "I0619 17:25:05.745646 139649090267008 learning.py:507] global step 279: loss = 0.5800 (2.979 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:25:08.792205 139649090267008 learning.py:507] global step 279: loss = 0.4501 (3.045 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:25:11.714441 139649090267008 learning.py:507] global step 279: loss = 0.5320 (2.921 sec/step)\n",
            "I0619 17:25:14.690055 139649090267008 learning.py:507] global step 279: loss = 0.4872 (2.974 sec/step)\n",
            "I0619 17:25:17.692928 139649090267008 learning.py:507] global step 279: loss = 0.4052 (3.001 sec/step)\n",
            "I0619 17:25:20.687226 139649090267008 learning.py:507] global step 279: loss = 0.4151 (2.993 sec/step)\n",
            "I0619 17:25:23.659744 139649090267008 learning.py:507] global step 279: loss = 0.4351 (2.971 sec/step)\n",
            "I0619 17:25:26.606616 139649090267008 learning.py:507] global step 280: loss = 0.4681 (2.945 sec/step)\n",
            "I0619 17:25:29.524168 139649090267008 learning.py:507] global step 280: loss = 0.4621 (2.916 sec/step)\n",
            "I0619 17:25:32.472866 139649090267008 learning.py:507] global step 280: loss = 0.4907 (2.947 sec/step)\n",
            "I0619 17:25:35.407379 139649090267008 learning.py:507] global step 280: loss = 0.3939 (2.933 sec/step)\n",
            "I0619 17:25:38.353206 139649090267008 learning.py:507] global step 280: loss = 0.4210 (2.944 sec/step)\n",
            "I0619 17:25:41.335925 139649090267008 learning.py:507] global step 280: loss = 0.4739 (2.981 sec/step)\n",
            "I0619 17:25:44.268986 139649090267008 learning.py:507] global step 280: loss = 0.3817 (2.931 sec/step)\n",
            "I0619 17:25:47.202883 139649090267008 learning.py:507] global step 280: loss = 0.4183 (2.932 sec/step)\n",
            "I0619 17:25:50.085361 139649090267008 learning.py:507] global step 281: loss = 0.4128 (2.880 sec/step)\n",
            "I0619 17:25:52.986851 139649090267008 learning.py:507] global step 281: loss = 0.4853 (2.900 sec/step)\n",
            "I0619 17:25:55.958946 139649090267008 learning.py:507] global step 281: loss = 0.4183 (2.970 sec/step)\n",
            "I0619 17:25:58.919574 139649090267008 learning.py:507] global step 281: loss = 0.4130 (2.959 sec/step)\n",
            "I0619 17:26:01.946722 139649090267008 learning.py:507] global step 281: loss = 0.4149 (3.025 sec/step)\n",
            "I0619 17:26:04.942203 139649090267008 learning.py:507] global step 281: loss = 0.4224 (2.994 sec/step)\n",
            "I0619 17:26:07.879993 139649090267008 learning.py:507] global step 281: loss = 0.4045 (2.936 sec/step)\n",
            "I0619 17:26:10.809844 139649090267008 learning.py:507] global step 281: loss = 0.4542 (2.928 sec/step)\n",
            "I0619 17:26:13.741536 139649090267008 learning.py:507] global step 282: loss = 0.4374 (2.929 sec/step)\n",
            "I0619 17:26:16.680820 139649090267008 learning.py:507] global step 282: loss = 0.4927 (2.938 sec/step)\n",
            "I0619 17:26:19.616269 139649090267008 learning.py:507] global step 282: loss = 0.3816 (2.933 sec/step)\n",
            "I0619 17:26:24.325213 139649090267008 learning.py:507] global step 282: loss = 0.4782 (4.699 sec/step)\n",
            "I0619 17:26:26.267173 139646017689344 supervisor.py:1050] Recording summary at step 282.\n",
            "I0619 17:26:27.767606 139649090267008 learning.py:507] global step 282: loss = 0.5005 (3.437 sec/step)\n",
            "I0619 17:26:30.758051 139649090267008 learning.py:507] global step 282: loss = 0.5076 (2.989 sec/step)\n",
            "I0619 17:26:33.725174 139649090267008 learning.py:507] global step 282: loss = 0.4842 (2.966 sec/step)\n",
            "I0619 17:26:36.678040 139649090267008 learning.py:507] global step 282: loss = 0.4361 (2.951 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:26:39.679832 139649090267008 learning.py:507] global step 283: loss = 0.4271 (2.999 sec/step)\n",
            "I0619 17:26:42.681239 139649090267008 learning.py:507] global step 283: loss = 0.4282 (2.999 sec/step)\n",
            "I0619 17:26:45.650794 139649090267008 learning.py:507] global step 283: loss = 0.3911 (2.968 sec/step)\n",
            "I0619 17:26:48.666117 139649090267008 learning.py:507] global step 283: loss = 0.4492 (3.014 sec/step)\n",
            "I0619 17:26:51.705946 139649090267008 learning.py:507] global step 283: loss = 0.3950 (3.038 sec/step)\n",
            "I0619 17:26:54.769134 139649090267008 learning.py:507] global step 283: loss = 0.4891 (3.061 sec/step)\n",
            "I0619 17:26:57.913010 139649090267008 learning.py:507] global step 283: loss = 0.3913 (3.142 sec/step)\n",
            "I0619 17:27:00.822806 139649090267008 learning.py:507] global step 283: loss = 0.4160 (2.908 sec/step)\n",
            "I0619 17:27:03.878931 139649090267008 learning.py:507] global step 284: loss = 0.4394 (3.054 sec/step)\n",
            "I0619 17:27:06.827231 139649090267008 learning.py:507] global step 284: loss = 0.4324 (2.946 sec/step)\n",
            "I0619 17:27:09.757133 139649090267008 learning.py:507] global step 284: loss = 0.4905 (2.928 sec/step)\n",
            "I0619 17:27:12.667828 139649090267008 learning.py:507] global step 284: loss = 0.3536 (2.909 sec/step)\n",
            "I0619 17:27:15.889495 139649090267008 learning.py:507] global step 284: loss = 0.3827 (3.220 sec/step)\n",
            "I0619 17:27:18.889041 139649090267008 learning.py:507] global step 284: loss = 0.4131 (2.998 sec/step)\n",
            "I0619 17:27:21.761266 139649090267008 learning.py:507] global step 284: loss = 0.3702 (2.871 sec/step)\n",
            "I0619 17:27:24.728170 139649090267008 learning.py:507] global step 284: loss = 0.3779 (2.965 sec/step)\n",
            "I0619 17:27:27.612034 139649090267008 learning.py:507] global step 285: loss = 0.4788 (2.882 sec/step)\n",
            "I0619 17:27:30.498687 139649090267008 learning.py:507] global step 285: loss = 0.4461 (2.885 sec/step)\n",
            "I0619 17:27:33.423448 139649090267008 learning.py:507] global step 285: loss = 0.4318 (2.923 sec/step)\n",
            "I0619 17:27:36.359999 139649090267008 learning.py:507] global step 285: loss = 0.4752 (2.935 sec/step)\n",
            "I0619 17:27:39.582901 139649090267008 learning.py:507] global step 285: loss = 0.4303 (3.221 sec/step)\n",
            "I0619 17:27:42.528531 139649090267008 learning.py:507] global step 285: loss = 0.4274 (2.944 sec/step)\n",
            "I0619 17:27:45.494809 139649090267008 learning.py:507] global step 285: loss = 0.4725 (2.965 sec/step)\n",
            "I0619 17:27:48.499884 139649090267008 learning.py:507] global step 285: loss = 0.4150 (3.003 sec/step)\n",
            "I0619 17:27:51.453670 139649090267008 learning.py:507] global step 286: loss = 0.4077 (2.951 sec/step)\n",
            "I0619 17:27:54.398287 139649090267008 learning.py:507] global step 286: loss = 0.4461 (2.943 sec/step)\n",
            "I0619 17:27:57.796504 139649090267008 learning.py:507] global step 286: loss = 0.3819 (3.397 sec/step)\n",
            "I0619 17:28:00.802349 139649090267008 learning.py:507] global step 286: loss = 0.4197 (3.004 sec/step)\n",
            "I0619 17:28:04.329689 139649090267008 learning.py:507] global step 286: loss = 0.4091 (3.526 sec/step)\n",
            "I0619 17:28:07.259572 139649090267008 learning.py:507] global step 286: loss = 0.4411 (2.928 sec/step)\n",
            "I0619 17:28:10.148647 139649090267008 learning.py:507] global step 286: loss = 0.4086 (2.887 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:28:13.276711 139649090267008 learning.py:507] global step 286: loss = 0.5175 (3.126 sec/step)\n",
            "I0619 17:28:16.219414 139649090267008 learning.py:507] global step 287: loss = 0.4217 (2.940 sec/step)\n",
            "I0619 17:28:19.108092 139649090267008 learning.py:507] global step 287: loss = 0.5155 (2.887 sec/step)\n",
            "I0619 17:28:24.157288 139649090267008 learning.py:507] global step 287: loss = 0.3813 (5.039 sec/step)\n",
            "I0619 17:28:26.202309 139646017689344 supervisor.py:1050] Recording summary at step 287.\n",
            "I0619 17:28:27.710919 139649090267008 learning.py:507] global step 287: loss = 0.4568 (3.552 sec/step)\n",
            "I0619 17:28:30.773278 139649090267008 learning.py:507] global step 287: loss = 0.4799 (3.061 sec/step)\n",
            "I0619 17:28:33.688817 139649090267008 learning.py:507] global step 287: loss = 0.3782 (2.914 sec/step)\n",
            "I0619 17:28:36.586736 139649090267008 learning.py:507] global step 287: loss = 0.4201 (2.896 sec/step)\n",
            "I0619 17:28:39.571487 139649090267008 learning.py:507] global step 287: loss = 0.3916 (2.983 sec/step)\n",
            "I0619 17:28:42.526730 139649090267008 learning.py:507] global step 288: loss = 0.4167 (2.953 sec/step)\n",
            "I0619 17:28:45.455477 139649090267008 learning.py:507] global step 288: loss = 0.3548 (2.927 sec/step)\n",
            "I0619 17:28:48.389595 139649090267008 learning.py:507] global step 288: loss = 0.3999 (2.932 sec/step)\n",
            "I0619 17:28:51.377519 139649090267008 learning.py:507] global step 288: loss = 0.3741 (2.986 sec/step)\n",
            "I0619 17:28:54.334309 139649090267008 learning.py:507] global step 288: loss = 0.4318 (2.955 sec/step)\n",
            "I0619 17:28:57.253074 139649090267008 learning.py:507] global step 288: loss = 0.4514 (2.917 sec/step)\n",
            "I0619 17:29:00.208221 139649090267008 learning.py:507] global step 288: loss = 0.4449 (2.953 sec/step)\n",
            "I0619 17:29:03.153404 139649090267008 learning.py:507] global step 288: loss = 0.3762 (2.943 sec/step)\n",
            "I0619 17:29:06.062764 139649090267008 learning.py:507] global step 289: loss = 0.3678 (2.907 sec/step)\n",
            "I0619 17:29:09.086329 139649090267008 learning.py:507] global step 289: loss = 0.4700 (3.022 sec/step)\n",
            "I0619 17:29:12.060797 139649090267008 learning.py:507] global step 289: loss = 0.3710 (2.973 sec/step)\n",
            "I0619 17:29:15.017141 139649090267008 learning.py:507] global step 289: loss = 0.3555 (2.955 sec/step)\n",
            "I0619 17:29:18.026858 139649090267008 learning.py:507] global step 289: loss = 0.4924 (3.008 sec/step)\n",
            "I0619 17:29:21.034686 139649090267008 learning.py:507] global step 289: loss = 0.5632 (3.006 sec/step)\n",
            "I0619 17:29:24.096784 139649090267008 learning.py:507] global step 289: loss = 0.4504 (3.060 sec/step)\n",
            "I0619 17:29:27.046694 139649090267008 learning.py:507] global step 289: loss = 0.4547 (2.948 sec/step)\n",
            "I0619 17:29:30.042777 139649090267008 learning.py:507] global step 290: loss = 0.4285 (2.994 sec/step)\n",
            "I0619 17:29:32.974808 139649090267008 learning.py:507] global step 290: loss = 0.5205 (2.930 sec/step)\n",
            "I0619 17:29:35.837204 139649090267008 learning.py:507] global step 290: loss = 0.4220 (2.861 sec/step)\n",
            "I0619 17:29:38.754348 139649090267008 learning.py:507] global step 290: loss = 0.3819 (2.916 sec/step)\n",
            "I0619 17:29:41.822035 139649090267008 learning.py:507] global step 290: loss = 0.4267 (3.066 sec/step)\n",
            "I0619 17:29:44.812008 139649090267008 learning.py:507] global step 290: loss = 0.4464 (2.988 sec/step)\n",
            "I0619 17:29:47.781589 139649090267008 learning.py:507] global step 290: loss = 0.5358 (2.968 sec/step)\n",
            "I0619 17:29:50.697201 139649090267008 learning.py:507] global step 290: loss = 0.4668 (2.914 sec/step)\n",
            "I0619 17:29:53.653345 139649090267008 learning.py:507] global step 291: loss = 0.4822 (2.954 sec/step)\n",
            "I0619 17:29:56.575931 139649090267008 learning.py:507] global step 291: loss = 0.5067 (2.921 sec/step)\n",
            "I0619 17:29:59.500911 139649090267008 learning.py:507] global step 291: loss = 0.3957 (2.923 sec/step)\n",
            "I0619 17:30:02.577176 139649090267008 learning.py:507] global step 291: loss = 0.4442 (3.075 sec/step)\n",
            "I0619 17:30:05.517601 139649090267008 learning.py:507] global step 291: loss = 0.5096 (2.939 sec/step)\n",
            "I0619 17:30:08.574351 139649090267008 learning.py:507] global step 291: loss = 0.4669 (3.055 sec/step)\n",
            "I0619 17:30:11.509083 139649090267008 learning.py:507] global step 291: loss = 0.4079 (2.933 sec/step)\n",
            "I0619 17:30:14.564331 139649090267008 learning.py:507] global step 291: loss = 0.4812 (3.054 sec/step)\n",
            "I0619 17:30:17.452371 139649090267008 learning.py:507] global step 292: loss = 0.4514 (2.885 sec/step)\n",
            "I0619 17:30:20.372801 139649090267008 learning.py:507] global step 292: loss = 0.3951 (2.919 sec/step)\n",
            "I0619 17:30:20.931278 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 17:30:27.174822 139646017689344 supervisor.py:1050] Recording summary at step 292.\n",
            "I0619 17:30:27.179293 139649090267008 learning.py:507] global step 292: loss = 0.3916 (6.800 sec/step)\n",
            "I0619 17:30:30.267242 139649090267008 learning.py:507] global step 292: loss = 0.4314 (3.085 sec/step)\n",
            "I0619 17:30:33.231290 139649090267008 learning.py:507] global step 292: loss = 0.4605 (2.962 sec/step)\n",
            "I0619 17:30:36.214790 139649090267008 learning.py:507] global step 292: loss = 0.5074 (2.982 sec/step)\n",
            "I0619 17:30:39.142436 139649090267008 learning.py:507] global step 292: loss = 0.4683 (2.926 sec/step)\n",
            "I0619 17:30:42.177934 139649090267008 learning.py:507] global step 292: loss = 0.4114 (3.034 sec/step)\n",
            "I0619 17:30:45.156296 139649090267008 learning.py:507] global step 293: loss = 0.4787 (2.975 sec/step)\n",
            "I0619 17:30:48.377852 139649090267008 learning.py:507] global step 293: loss = 0.3991 (3.220 sec/step)\n",
            "I0619 17:30:51.348935 139649090267008 learning.py:507] global step 293: loss = 0.4092 (2.969 sec/step)\n",
            "I0619 17:30:54.398543 139649090267008 learning.py:507] global step 293: loss = 0.4017 (3.048 sec/step)\n",
            "I0619 17:30:57.366609 139649090267008 learning.py:507] global step 293: loss = 0.4262 (2.966 sec/step)\n",
            "I0619 17:31:00.347035 139649090267008 learning.py:507] global step 293: loss = 0.4200 (2.979 sec/step)\n",
            "I0619 17:31:03.363054 139649090267008 learning.py:507] global step 293: loss = 0.4185 (3.014 sec/step)\n",
            "I0619 17:31:06.350426 139649090267008 learning.py:507] global step 293: loss = 0.4018 (2.986 sec/step)\n",
            "I0619 17:31:09.285692 139649090267008 learning.py:507] global step 294: loss = 0.4209 (2.933 sec/step)\n",
            "I0619 17:31:12.278866 139649090267008 learning.py:507] global step 294: loss = 0.4160 (2.992 sec/step)\n",
            "I0619 17:31:15.218450 139649090267008 learning.py:507] global step 294: loss = 0.4100 (2.936 sec/step)\n",
            "I0619 17:31:18.138117 139649090267008 learning.py:507] global step 294: loss = 0.3783 (2.917 sec/step)\n",
            "I0619 17:31:21.187319 139649090267008 learning.py:507] global step 294: loss = 0.4317 (3.048 sec/step)\n",
            "I0619 17:31:24.197305 139649090267008 learning.py:507] global step 294: loss = 0.3860 (3.008 sec/step)\n",
            "I0619 17:31:27.154109 139649090267008 learning.py:507] global step 294: loss = 0.4092 (2.955 sec/step)\n",
            "I0619 17:31:30.093496 139649090267008 learning.py:507] global step 294: loss = 0.5108 (2.938 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:31:33.048424 139649090267008 learning.py:507] global step 295: loss = 0.3550 (2.952 sec/step)\n",
            "I0619 17:31:35.995507 139649090267008 learning.py:507] global step 295: loss = 0.4277 (2.945 sec/step)\n",
            "I0619 17:31:39.071464 139649090267008 learning.py:507] global step 295: loss = 0.4082 (3.074 sec/step)\n",
            "I0619 17:31:41.970505 139649090267008 learning.py:507] global step 295: loss = 0.4211 (2.897 sec/step)\n",
            "I0619 17:31:44.946637 139649090267008 learning.py:507] global step 295: loss = 0.3795 (2.975 sec/step)\n",
            "I0619 17:31:47.982512 139649090267008 learning.py:507] global step 295: loss = 0.3816 (3.034 sec/step)\n",
            "I0619 17:31:50.922908 139649090267008 learning.py:507] global step 295: loss = 0.4265 (2.939 sec/step)\n",
            "I0619 17:31:53.832846 139649090267008 learning.py:507] global step 295: loss = 0.4090 (2.908 sec/step)\n",
            "I0619 17:31:56.838421 139649090267008 learning.py:507] global step 296: loss = 0.3973 (3.003 sec/step)\n",
            "I0619 17:31:59.886352 139649090267008 learning.py:507] global step 296: loss = 0.4853 (3.046 sec/step)\n",
            "I0619 17:32:02.931232 139649090267008 learning.py:507] global step 296: loss = 0.3808 (3.043 sec/step)\n",
            "I0619 17:32:05.927435 139649090267008 learning.py:507] global step 296: loss = 0.5394 (2.994 sec/step)\n",
            "I0619 17:32:08.869516 139649090267008 learning.py:507] global step 296: loss = 0.4057 (2.940 sec/step)\n",
            "I0619 17:32:11.858822 139649090267008 learning.py:507] global step 296: loss = 0.4441 (2.988 sec/step)\n",
            "I0619 17:32:14.757803 139649090267008 learning.py:507] global step 296: loss = 0.4599 (2.897 sec/step)\n",
            "I0619 17:32:17.852617 139649090267008 learning.py:507] global step 296: loss = 0.4465 (3.093 sec/step)\n",
            "I0619 17:32:20.862740 139649090267008 learning.py:507] global step 297: loss = 0.3945 (3.008 sec/step)\n",
            "I0619 17:32:26.073587 139649090267008 learning.py:507] global step 297: loss = 0.3552 (5.209 sec/step)\n",
            "I0619 17:32:26.550313 139646017689344 supervisor.py:1050] Recording summary at step 297.\n",
            "I0619 17:32:29.121180 139649090267008 learning.py:507] global step 297: loss = 0.5436 (3.045 sec/step)\n",
            "I0619 17:32:32.065521 139649090267008 learning.py:507] global step 297: loss = 0.3700 (2.943 sec/step)\n",
            "I0619 17:32:34.983444 139649090267008 learning.py:507] global step 297: loss = 0.3831 (2.916 sec/step)\n",
            "I0619 17:32:37.937775 139649090267008 learning.py:507] global step 297: loss = 0.3524 (2.953 sec/step)\n",
            "I0619 17:32:40.820603 139649090267008 learning.py:507] global step 297: loss = 0.4743 (2.881 sec/step)\n",
            "I0619 17:32:44.090074 139649090267008 learning.py:507] global step 297: loss = 0.4884 (3.267 sec/step)\n",
            "I0619 17:32:47.049142 139649090267008 learning.py:507] global step 298: loss = 0.5655 (2.957 sec/step)\n",
            "I0619 17:32:50.000207 139649090267008 learning.py:507] global step 298: loss = 0.4777 (2.949 sec/step)\n",
            "I0619 17:32:52.947165 139649090267008 learning.py:507] global step 298: loss = 0.5494 (2.945 sec/step)\n",
            "I0619 17:32:55.910486 139649090267008 learning.py:507] global step 298: loss = 0.3752 (2.962 sec/step)\n",
            "I0619 17:32:58.909842 139649090267008 learning.py:507] global step 298: loss = 0.5423 (2.998 sec/step)\n",
            "I0619 17:33:02.351205 139649090267008 learning.py:507] global step 298: loss = 0.5809 (3.440 sec/step)\n",
            "I0619 17:33:05.353472 139649090267008 learning.py:507] global step 298: loss = 0.3853 (3.001 sec/step)\n",
            "I0619 17:33:08.355472 139649090267008 learning.py:507] global step 298: loss = 0.5594 (3.000 sec/step)\n",
            "I0619 17:33:11.294505 139649090267008 learning.py:507] global step 299: loss = 0.4482 (2.937 sec/step)\n",
            "I0619 17:33:14.204652 139649090267008 learning.py:507] global step 299: loss = 0.5221 (2.908 sec/step)\n",
            "I0619 17:33:17.162772 139649090267008 learning.py:507] global step 299: loss = 0.3834 (2.953 sec/step)\n",
            "I0619 17:33:20.136463 139649090267008 learning.py:507] global step 299: loss = 0.4499 (2.972 sec/step)\n",
            "I0619 17:33:23.150538 139649090267008 learning.py:507] global step 299: loss = 0.4653 (3.012 sec/step)\n",
            "I0619 17:33:26.248145 139649090267008 learning.py:507] global step 299: loss = 0.4496 (3.096 sec/step)\n",
            "I0619 17:33:29.210827 139649090267008 learning.py:507] global step 299: loss = 0.3601 (2.961 sec/step)\n",
            "I0619 17:33:32.109138 139649090267008 learning.py:507] global step 299: loss = 0.4123 (2.897 sec/step)\n",
            "I0619 17:33:35.011686 139649090267008 learning.py:507] global step 300: loss = 0.3710 (2.900 sec/step)\n",
            "I0619 17:33:38.001177 139649090267008 learning.py:507] global step 300: loss = 0.4030 (2.988 sec/step)\n",
            "I0619 17:33:40.939922 139649090267008 learning.py:507] global step 300: loss = 0.4575 (2.937 sec/step)\n",
            "I0619 17:33:44.043488 139649090267008 learning.py:507] global step 300: loss = 0.4003 (3.102 sec/step)\n",
            "I0619 17:33:46.979789 139649090267008 learning.py:507] global step 300: loss = 0.4210 (2.935 sec/step)\n",
            "I0619 17:33:49.968662 139649090267008 learning.py:507] global step 300: loss = 0.4729 (2.987 sec/step)\n",
            "I0619 17:33:52.948938 139649090267008 learning.py:507] global step 300: loss = 0.4256 (2.979 sec/step)\n",
            "I0619 17:33:55.949067 139649090267008 learning.py:507] global step 300: loss = 0.3204 (2.998 sec/step)\n",
            "I0619 17:33:58.980627 139649090267008 learning.py:507] global step 301: loss = 0.4397 (3.030 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:34:01.876029 139649090267008 learning.py:507] global step 301: loss = 0.4434 (2.894 sec/step)\n",
            "I0619 17:34:04.797632 139649090267008 learning.py:507] global step 301: loss = 0.4694 (2.920 sec/step)\n",
            "I0619 17:34:07.752561 139649090267008 learning.py:507] global step 301: loss = 0.4544 (2.953 sec/step)\n",
            "I0619 17:34:10.705177 139649090267008 learning.py:507] global step 301: loss = 0.5714 (2.947 sec/step)\n",
            "I0619 17:34:13.655610 139649090267008 learning.py:507] global step 301: loss = 0.5903 (2.949 sec/step)\n",
            "I0619 17:34:16.588464 139649090267008 learning.py:507] global step 301: loss = 0.3872 (2.931 sec/step)\n",
            "I0619 17:34:19.565737 139649090267008 learning.py:507] global step 301: loss = 0.3859 (2.975 sec/step)\n",
            "I0619 17:34:24.320184 139649090267008 learning.py:507] global step 302: loss = 0.3614 (4.750 sec/step)\n",
            "I0619 17:34:26.091567 139646017689344 supervisor.py:1050] Recording summary at step 302.\n",
            "I0619 17:34:27.767071 139649090267008 learning.py:507] global step 302: loss = 0.4319 (3.441 sec/step)\n",
            "I0619 17:34:30.726220 139649090267008 learning.py:507] global step 302: loss = 0.4387 (2.957 sec/step)\n",
            "I0619 17:34:33.659717 139649090267008 learning.py:507] global step 302: loss = 0.4083 (2.932 sec/step)\n",
            "I0619 17:34:36.625855 139649090267008 learning.py:507] global step 302: loss = 0.4116 (2.964 sec/step)\n",
            "I0619 17:34:39.728048 139649090267008 learning.py:507] global step 302: loss = 0.3847 (3.101 sec/step)\n",
            "I0619 17:34:42.730087 139649090267008 learning.py:507] global step 302: loss = 0.4024 (3.000 sec/step)\n",
            "I0619 17:34:45.748207 139649090267008 learning.py:507] global step 302: loss = 0.5623 (3.017 sec/step)\n",
            "I0619 17:34:48.748233 139649090267008 learning.py:507] global step 303: loss = 0.4052 (2.998 sec/step)\n",
            "I0619 17:34:51.765262 139649090267008 learning.py:507] global step 303: loss = 0.3748 (3.015 sec/step)\n",
            "I0619 17:34:54.727860 139649090267008 learning.py:507] global step 303: loss = 0.3563 (2.960 sec/step)\n",
            "I0619 17:34:57.840511 139649090267008 learning.py:507] global step 303: loss = 0.4116 (3.111 sec/step)\n",
            "I0619 17:35:00.947747 139649090267008 learning.py:507] global step 303: loss = 0.4382 (3.105 sec/step)\n",
            "I0619 17:35:03.989057 139649090267008 learning.py:507] global step 303: loss = 0.4326 (3.040 sec/step)\n",
            "I0619 17:35:06.942622 139649090267008 learning.py:507] global step 303: loss = 0.5200 (2.952 sec/step)\n",
            "I0619 17:35:09.850830 139649090267008 learning.py:507] global step 303: loss = 0.4274 (2.906 sec/step)\n",
            "I0619 17:35:12.851920 139649090267008 learning.py:507] global step 304: loss = 0.4169 (2.999 sec/step)\n",
            "I0619 17:35:15.866918 139649090267008 learning.py:507] global step 304: loss = 0.4793 (3.013 sec/step)\n",
            "I0619 17:35:19.048501 139649090267008 learning.py:507] global step 304: loss = 0.3809 (3.179 sec/step)\n",
            "I0619 17:35:22.174449 139649090267008 learning.py:507] global step 304: loss = 0.3893 (3.124 sec/step)\n",
            "I0619 17:35:25.150140 139649090267008 learning.py:507] global step 304: loss = 0.3991 (2.974 sec/step)\n",
            "I0619 17:35:28.146143 139649090267008 learning.py:507] global step 304: loss = 0.3833 (2.994 sec/step)\n",
            "I0619 17:35:31.070245 139649090267008 learning.py:507] global step 304: loss = 0.4111 (2.922 sec/step)\n",
            "I0619 17:35:33.998065 139649090267008 learning.py:507] global step 304: loss = 0.4242 (2.926 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:35:36.895032 139649090267008 learning.py:507] global step 305: loss = 0.5422 (2.895 sec/step)\n",
            "I0619 17:35:39.927074 139649090267008 learning.py:507] global step 305: loss = 0.4662 (3.030 sec/step)\n",
            "I0619 17:35:43.139020 139649090267008 learning.py:507] global step 305: loss = 0.4043 (3.210 sec/step)\n",
            "I0619 17:35:46.034648 139649090267008 learning.py:507] global step 305: loss = 0.4376 (2.894 sec/step)\n",
            "I0619 17:35:48.912630 139649090267008 learning.py:507] global step 305: loss = 0.4019 (2.876 sec/step)\n",
            "I0619 17:35:51.880238 139649090267008 learning.py:507] global step 305: loss = 0.4948 (2.961 sec/step)\n",
            "I0619 17:35:54.823109 139649090267008 learning.py:507] global step 305: loss = 0.4348 (2.941 sec/step)\n",
            "I0619 17:35:57.934599 139649090267008 learning.py:507] global step 305: loss = 0.5096 (3.110 sec/step)\n",
            "I0619 17:36:01.180235 139649090267008 learning.py:507] global step 306: loss = 0.4475 (3.243 sec/step)\n",
            "I0619 17:36:04.209764 139649090267008 learning.py:507] global step 306: loss = 0.4063 (3.028 sec/step)\n",
            "I0619 17:36:07.225741 139649090267008 learning.py:507] global step 306: loss = 0.4327 (3.014 sec/step)\n",
            "I0619 17:36:10.266217 139649090267008 learning.py:507] global step 306: loss = 0.3987 (3.039 sec/step)\n",
            "I0619 17:36:13.152529 139649090267008 learning.py:507] global step 306: loss = 0.4018 (2.884 sec/step)\n",
            "I0619 17:36:16.080713 139649090267008 learning.py:507] global step 306: loss = 0.3979 (2.926 sec/step)\n",
            "I0619 17:36:18.985156 139649090267008 learning.py:507] global step 306: loss = 0.4001 (2.903 sec/step)\n",
            "I0619 17:36:23.115878 139649090267008 learning.py:507] global step 306: loss = 0.3853 (4.120 sec/step)\n",
            "I0619 17:36:26.105907 139646017689344 supervisor.py:1050] Recording summary at step 306.\n",
            "I0619 17:36:27.213019 139649090267008 learning.py:507] global step 307: loss = 0.4533 (4.085 sec/step)\n",
            "I0619 17:36:30.149218 139649090267008 learning.py:507] global step 307: loss = 0.3930 (2.934 sec/step)\n",
            "I0619 17:36:33.150384 139649090267008 learning.py:507] global step 307: loss = 0.4176 (3.000 sec/step)\n",
            "I0619 17:36:36.149790 139649090267008 learning.py:507] global step 307: loss = 0.3895 (2.997 sec/step)\n",
            "I0619 17:36:39.110183 139649090267008 learning.py:507] global step 307: loss = 0.5045 (2.959 sec/step)\n",
            "I0619 17:36:42.016872 139649090267008 learning.py:507] global step 307: loss = 0.4675 (2.905 sec/step)\n",
            "I0619 17:36:44.992639 139649090267008 learning.py:507] global step 307: loss = 0.3812 (2.974 sec/step)\n",
            "I0619 17:36:47.935186 139649090267008 learning.py:507] global step 307: loss = 0.4225 (2.941 sec/step)\n",
            "I0619 17:36:51.036275 139649090267008 learning.py:507] global step 308: loss = 0.4379 (3.099 sec/step)\n",
            "I0619 17:36:54.007089 139649090267008 learning.py:507] global step 308: loss = 0.4124 (2.969 sec/step)\n",
            "I0619 17:36:56.941246 139649090267008 learning.py:507] global step 308: loss = 0.4248 (2.933 sec/step)\n",
            "I0619 17:37:00.039606 139649090267008 learning.py:507] global step 308: loss = 0.4158 (3.097 sec/step)\n",
            "I0619 17:37:03.045332 139649090267008 learning.py:507] global step 308: loss = 0.3892 (3.004 sec/step)\n",
            "I0619 17:37:06.048035 139649090267008 learning.py:507] global step 308: loss = 0.4333 (3.001 sec/step)\n",
            "I0619 17:37:09.081430 139649090267008 learning.py:507] global step 308: loss = 0.3608 (3.031 sec/step)\n",
            "I0619 17:37:12.061325 139649090267008 learning.py:507] global step 308: loss = 0.3698 (2.978 sec/step)\n",
            "I0619 17:37:14.996685 139649090267008 learning.py:507] global step 309: loss = 0.4439 (2.927 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:37:18.147816 139649090267008 learning.py:507] global step 309: loss = 0.4099 (3.149 sec/step)\n",
            "I0619 17:37:21.096369 139649090267008 learning.py:507] global step 309: loss = 0.3824 (2.947 sec/step)\n",
            "I0619 17:37:24.076423 139649090267008 learning.py:507] global step 309: loss = 0.4687 (2.978 sec/step)\n",
            "I0619 17:37:27.077733 139649090267008 learning.py:507] global step 309: loss = 0.4794 (3.000 sec/step)\n",
            "I0619 17:37:30.072367 139649090267008 learning.py:507] global step 309: loss = 0.3904 (2.993 sec/step)\n",
            "I0619 17:37:33.106818 139649090267008 learning.py:507] global step 309: loss = 0.3798 (3.033 sec/step)\n",
            "I0619 17:37:36.072633 139649090267008 learning.py:507] global step 309: loss = 0.5214 (2.964 sec/step)\n",
            "I0619 17:37:38.991761 139649090267008 learning.py:507] global step 310: loss = 0.4668 (2.916 sec/step)\n",
            "I0619 17:37:41.891844 139649090267008 learning.py:507] global step 310: loss = 0.3464 (2.899 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:37:44.831410 139649090267008 learning.py:507] global step 310: loss = 0.5254 (2.938 sec/step)\n",
            "I0619 17:37:47.824678 139649090267008 learning.py:507] global step 310: loss = 0.5166 (2.992 sec/step)\n",
            "I0619 17:37:50.820737 139649090267008 learning.py:507] global step 310: loss = 0.5372 (2.994 sec/step)\n",
            "I0619 17:37:53.730072 139649090267008 learning.py:507] global step 310: loss = 0.4834 (2.908 sec/step)\n",
            "I0619 17:37:56.654058 139649090267008 learning.py:507] global step 310: loss = 0.3433 (2.922 sec/step)\n",
            "I0619 17:37:59.574442 139649090267008 learning.py:507] global step 310: loss = 0.4386 (2.919 sec/step)\n",
            "I0619 17:38:02.527560 139649090267008 learning.py:507] global step 311: loss = 0.4575 (2.951 sec/step)\n",
            "I0619 17:38:05.518437 139649090267008 learning.py:507] global step 311: loss = 0.5617 (2.989 sec/step)\n",
            "I0619 17:38:08.504916 139649090267008 learning.py:507] global step 311: loss = 0.4445 (2.985 sec/step)\n",
            "I0619 17:38:11.415169 139649090267008 learning.py:507] global step 311: loss = 0.4474 (2.909 sec/step)\n",
            "I0619 17:38:14.307307 139649090267008 learning.py:507] global step 311: loss = 0.4487 (2.891 sec/step)\n",
            "I0619 17:38:17.218604 139649090267008 learning.py:507] global step 311: loss = 0.5594 (2.909 sec/step)\n",
            "I0619 17:38:20.381936 139649090267008 learning.py:507] global step 311: loss = 0.4495 (3.162 sec/step)\n",
            "I0619 17:38:25.475424 139649090267008 learning.py:507] global step 311: loss = 0.5585 (5.091 sec/step)\n",
            "I0619 17:38:26.309717 139646017689344 supervisor.py:1050] Recording summary at step 311.\n",
            "I0619 17:38:28.554370 139649090267008 learning.py:507] global step 312: loss = 0.4805 (3.076 sec/step)\n",
            "I0619 17:38:31.467541 139649090267008 learning.py:507] global step 312: loss = 0.4482 (2.911 sec/step)\n",
            "I0619 17:38:34.334062 139649090267008 learning.py:507] global step 312: loss = 0.5254 (2.865 sec/step)\n",
            "I0619 17:38:37.643081 139649090267008 learning.py:507] global step 312: loss = 0.3853 (3.307 sec/step)\n",
            "I0619 17:38:40.669253 139649090267008 learning.py:507] global step 312: loss = 0.3571 (3.024 sec/step)\n",
            "I0619 17:38:43.651084 139649090267008 learning.py:507] global step 312: loss = 0.3659 (2.980 sec/step)\n",
            "I0619 17:38:46.901316 139649090267008 learning.py:507] global step 312: loss = 0.4603 (3.248 sec/step)\n",
            "I0619 17:38:49.890707 139649090267008 learning.py:507] global step 312: loss = 0.3923 (2.988 sec/step)\n",
            "I0619 17:38:52.810631 139649090267008 learning.py:507] global step 313: loss = 0.3628 (2.918 sec/step)\n",
            "I0619 17:38:55.769814 139649090267008 learning.py:507] global step 313: loss = 0.3671 (2.957 sec/step)\n",
            "I0619 17:38:58.707403 139649090267008 learning.py:507] global step 313: loss = 0.4792 (2.936 sec/step)\n",
            "I0619 17:39:01.647376 139649090267008 learning.py:507] global step 313: loss = 0.4819 (2.938 sec/step)\n",
            "I0619 17:39:04.969665 139649090267008 learning.py:507] global step 313: loss = 0.4956 (3.321 sec/step)\n",
            "I0619 17:39:07.910680 139649090267008 learning.py:507] global step 313: loss = 0.4758 (2.939 sec/step)\n",
            "I0619 17:39:10.885303 139649090267008 learning.py:507] global step 313: loss = 0.3608 (2.973 sec/step)\n",
            "I0619 17:39:13.829296 139649090267008 learning.py:507] global step 313: loss = 0.4774 (2.942 sec/step)\n",
            "I0619 17:39:16.831004 139649090267008 learning.py:507] global step 314: loss = 0.3614 (2.999 sec/step)\n",
            "I0619 17:39:19.910457 139649090267008 learning.py:507] global step 314: loss = 0.3343 (3.078 sec/step)\n",
            "I0619 17:39:22.875020 139649090267008 learning.py:507] global step 314: loss = 0.4471 (2.963 sec/step)\n",
            "I0619 17:39:25.836399 139649090267008 learning.py:507] global step 314: loss = 0.4269 (2.959 sec/step)\n",
            "I0619 17:39:28.765322 139649090267008 learning.py:507] global step 314: loss = 0.4740 (2.927 sec/step)\n",
            "I0619 17:39:31.880089 139649090267008 learning.py:507] global step 314: loss = 0.4070 (3.113 sec/step)\n",
            "I0619 17:39:34.941412 139649090267008 learning.py:507] global step 314: loss = 0.5025 (3.060 sec/step)\n",
            "I0619 17:39:38.003847 139649090267008 learning.py:507] global step 314: loss = 0.3650 (3.060 sec/step)\n",
            "I0619 17:39:40.956496 139649090267008 learning.py:507] global step 315: loss = 0.5263 (2.951 sec/step)\n",
            "I0619 17:39:43.906903 139649090267008 learning.py:507] global step 315: loss = 0.4624 (2.949 sec/step)\n",
            "I0619 17:39:46.847078 139649090267008 learning.py:507] global step 315: loss = 0.4892 (2.938 sec/step)\n",
            "I0619 17:39:50.432120 139649090267008 learning.py:507] global step 315: loss = 0.3527 (3.583 sec/step)\n",
            "I0619 17:39:53.435502 139649090267008 learning.py:507] global step 315: loss = 0.4504 (3.002 sec/step)\n",
            "I0619 17:39:56.443392 139649090267008 learning.py:507] global step 315: loss = 0.3782 (3.006 sec/step)\n",
            "I0619 17:39:59.517256 139649090267008 learning.py:507] global step 315: loss = 0.4368 (3.072 sec/step)\n",
            "I0619 17:40:02.481815 139649090267008 learning.py:507] global step 315: loss = 0.4462 (2.963 sec/step)\n",
            "I0619 17:40:05.468915 139649090267008 learning.py:507] global step 316: loss = 0.4251 (2.985 sec/step)\n",
            "I0619 17:40:08.845994 139649090267008 learning.py:507] global step 316: loss = 0.3761 (3.375 sec/step)\n",
            "I0619 17:40:11.877078 139649090267008 learning.py:507] global step 316: loss = 0.4845 (3.029 sec/step)\n",
            "I0619 17:40:14.851644 139649090267008 learning.py:507] global step 316: loss = 0.4815 (2.973 sec/step)\n",
            "I0619 17:40:17.848937 139649090267008 learning.py:507] global step 316: loss = 0.5259 (2.995 sec/step)\n",
            "I0619 17:40:20.830671 139649090267008 learning.py:507] global step 316: loss = 0.5190 (2.980 sec/step)\n",
            "I0619 17:40:20.931997 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 17:40:27.380944 139646017689344 supervisor.py:1050] Recording summary at step 316.\n",
            "I0619 17:40:27.837719 139649090267008 learning.py:507] global step 316: loss = 0.3805 (7.005 sec/step)\n",
            "I0619 17:40:31.038282 139649090267008 learning.py:507] global step 316: loss = 0.3910 (3.199 sec/step)\n",
            "I0619 17:40:34.026293 139649090267008 learning.py:507] global step 317: loss = 0.3968 (2.986 sec/step)\n",
            "I0619 17:40:36.966312 139649090267008 learning.py:507] global step 317: loss = 0.3783 (2.938 sec/step)\n",
            "I0619 17:40:39.982853 139649090267008 learning.py:507] global step 317: loss = 0.3541 (3.015 sec/step)\n",
            "I0619 17:40:42.993788 139649090267008 learning.py:507] global step 317: loss = 0.4370 (3.009 sec/step)\n",
            "I0619 17:40:45.917408 139649090267008 learning.py:507] global step 317: loss = 0.4256 (2.921 sec/step)\n",
            "I0619 17:40:48.905532 139649090267008 learning.py:507] global step 317: loss = 0.6097 (2.986 sec/step)\n",
            "I0619 17:40:51.854875 139649090267008 learning.py:507] global step 317: loss = 0.4291 (2.948 sec/step)\n",
            "I0619 17:40:54.853976 139649090267008 learning.py:507] global step 317: loss = 0.3898 (2.997 sec/step)\n",
            "I0619 17:40:57.786337 139649090267008 learning.py:507] global step 318: loss = 0.4050 (2.930 sec/step)\n",
            "I0619 17:41:00.759412 139649090267008 learning.py:507] global step 318: loss = 0.4240 (2.971 sec/step)\n",
            "I0619 17:41:03.680872 139649090267008 learning.py:507] global step 318: loss = 0.4374 (2.920 sec/step)\n",
            "I0619 17:41:06.654488 139649090267008 learning.py:507] global step 318: loss = 0.4177 (2.972 sec/step)\n",
            "I0619 17:41:09.588813 139649090267008 learning.py:507] global step 318: loss = 0.3688 (2.933 sec/step)\n",
            "I0619 17:41:12.485657 139649090267008 learning.py:507] global step 318: loss = 0.4577 (2.895 sec/step)\n",
            "I0619 17:41:15.497240 139649090267008 learning.py:507] global step 318: loss = 0.4032 (3.010 sec/step)\n",
            "I0619 17:41:18.504635 139649090267008 learning.py:507] global step 318: loss = 0.3894 (3.006 sec/step)\n",
            "I0619 17:41:21.450336 139649090267008 learning.py:507] global step 319: loss = 0.4278 (2.943 sec/step)\n",
            "I0619 17:41:24.380877 139649090267008 learning.py:507] global step 319: loss = 0.3454 (2.929 sec/step)\n",
            "I0619 17:41:27.383019 139649090267008 learning.py:507] global step 319: loss = 0.3844 (3.001 sec/step)\n",
            "I0619 17:41:30.456368 139649090267008 learning.py:507] global step 319: loss = 0.4358 (3.072 sec/step)\n",
            "I0619 17:41:33.384261 139649090267008 learning.py:507] global step 319: loss = 0.4248 (2.926 sec/step)\n",
            "I0619 17:41:36.342549 139649090267008 learning.py:507] global step 319: loss = 0.4588 (2.957 sec/step)\n",
            "I0619 17:41:39.306294 139649090267008 learning.py:507] global step 319: loss = 0.4270 (2.962 sec/step)\n",
            "I0619 17:41:42.264952 139649090267008 learning.py:507] global step 319: loss = 0.4298 (2.957 sec/step)\n",
            "I0619 17:41:45.278878 139649090267008 learning.py:507] global step 320: loss = 0.5402 (3.011 sec/step)\n",
            "I0619 17:41:48.389183 139649090267008 learning.py:507] global step 320: loss = 0.6272 (3.109 sec/step)\n",
            "I0619 17:41:51.344234 139649090267008 learning.py:507] global step 320: loss = 0.4222 (2.953 sec/step)\n",
            "I0619 17:41:54.330850 139649090267008 learning.py:507] global step 320: loss = 0.4184 (2.985 sec/step)\n",
            "I0619 17:41:57.309201 139649090267008 learning.py:507] global step 320: loss = 0.3585 (2.977 sec/step)\n",
            "I0619 17:42:00.209161 139649090267008 learning.py:507] global step 320: loss = 0.4168 (2.897 sec/step)\n",
            "I0619 17:42:03.142915 139649090267008 learning.py:507] global step 320: loss = 0.3536 (2.932 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:42:06.134754 139649090267008 learning.py:507] global step 320: loss = 0.6203 (2.990 sec/step)\n",
            "I0619 17:42:09.101664 139649090267008 learning.py:507] global step 321: loss = 0.4741 (2.965 sec/step)\n",
            "I0619 17:42:12.058691 139649090267008 learning.py:507] global step 321: loss = 0.3664 (2.955 sec/step)\n",
            "I0619 17:42:14.986858 139649090267008 learning.py:507] global step 321: loss = 0.4431 (2.926 sec/step)\n",
            "I0619 17:42:18.013748 139649090267008 learning.py:507] global step 321: loss = 0.4196 (3.025 sec/step)\n",
            "I0619 17:42:21.031042 139649090267008 learning.py:507] global step 321: loss = 0.3834 (3.016 sec/step)\n",
            "I0619 17:42:26.255172 139649090267008 learning.py:507] global step 321: loss = 0.4393 (5.221 sec/step)\n",
            "I0619 17:42:26.273469 139646017689344 supervisor.py:1050] Recording summary at step 321.\n",
            "I0619 17:42:29.271280 139649090267008 learning.py:507] global step 321: loss = 0.3470 (3.013 sec/step)\n",
            "I0619 17:42:32.252177 139649090267008 learning.py:507] global step 321: loss = 0.4321 (2.979 sec/step)\n",
            "I0619 17:42:35.238124 139649090267008 learning.py:507] global step 322: loss = 0.4032 (2.983 sec/step)\n",
            "I0619 17:42:38.253671 139649090267008 learning.py:507] global step 322: loss = 0.3555 (3.014 sec/step)\n",
            "I0619 17:42:41.237862 139649090267008 learning.py:507] global step 322: loss = 0.4176 (2.982 sec/step)\n",
            "I0619 17:42:44.201451 139649090267008 learning.py:507] global step 322: loss = 0.4341 (2.962 sec/step)\n",
            "I0619 17:42:47.209048 139649090267008 learning.py:507] global step 322: loss = 0.4295 (3.006 sec/step)\n",
            "I0619 17:42:50.182873 139649090267008 learning.py:507] global step 322: loss = 0.5139 (2.972 sec/step)\n",
            "I0619 17:42:53.124084 139649090267008 learning.py:507] global step 322: loss = 0.4180 (2.940 sec/step)\n",
            "I0619 17:42:56.055283 139649090267008 learning.py:507] global step 322: loss = 0.4092 (2.929 sec/step)\n",
            "I0619 17:42:59.030337 139649090267008 learning.py:507] global step 323: loss = 0.4659 (2.972 sec/step)\n",
            "I0619 17:43:02.015936 139649090267008 learning.py:507] global step 323: loss = 0.4341 (2.983 sec/step)\n",
            "I0619 17:43:04.985054 139649090267008 learning.py:507] global step 323: loss = 0.3859 (2.967 sec/step)\n",
            "I0619 17:43:07.858764 139649090267008 learning.py:507] global step 323: loss = 0.4354 (2.872 sec/step)\n",
            "I0619 17:43:10.838287 139649090267008 learning.py:507] global step 323: loss = 0.3805 (2.978 sec/step)\n",
            "I0619 17:43:13.736048 139649090267008 learning.py:507] global step 323: loss = 0.3489 (2.896 sec/step)\n",
            "I0619 17:43:16.669922 139649090267008 learning.py:507] global step 323: loss = 0.4553 (2.932 sec/step)\n",
            "I0619 17:43:19.628549 139649090267008 learning.py:507] global step 323: loss = 0.3989 (2.957 sec/step)\n",
            "I0619 17:43:22.595113 139649090267008 learning.py:507] global step 324: loss = 0.4786 (2.964 sec/step)\n",
            "I0619 17:43:25.540933 139649090267008 learning.py:507] global step 324: loss = 0.4401 (2.944 sec/step)\n",
            "I0619 17:43:28.487727 139649090267008 learning.py:507] global step 324: loss = 0.4925 (2.945 sec/step)\n",
            "I0619 17:43:31.464417 139649090267008 learning.py:507] global step 324: loss = 0.3876 (2.975 sec/step)\n",
            "I0619 17:43:34.365571 139649090267008 learning.py:507] global step 324: loss = 0.3713 (2.899 sec/step)\n",
            "I0619 17:43:37.421939 139649090267008 learning.py:507] global step 324: loss = 0.3444 (3.054 sec/step)\n",
            "I0619 17:43:40.398005 139649090267008 learning.py:507] global step 324: loss = 0.4420 (2.975 sec/step)\n",
            "I0619 17:43:43.304152 139649090267008 learning.py:507] global step 324: loss = 0.4506 (2.905 sec/step)\n",
            "I0619 17:43:46.282665 139649090267008 learning.py:507] global step 325: loss = 0.3604 (2.977 sec/step)\n",
            "I0619 17:43:49.295520 139649090267008 learning.py:507] global step 325: loss = 0.4551 (3.011 sec/step)\n",
            "I0619 17:43:52.395760 139649090267008 learning.py:507] global step 325: loss = 0.4732 (3.099 sec/step)\n",
            "I0619 17:43:55.410836 139649090267008 learning.py:507] global step 325: loss = 0.4263 (3.013 sec/step)\n",
            "I0619 17:43:58.427121 139649090267008 learning.py:507] global step 325: loss = 0.3964 (3.015 sec/step)\n",
            "I0619 17:44:01.482309 139649090267008 learning.py:507] global step 325: loss = 0.4528 (3.054 sec/step)\n",
            "I0619 17:44:04.547114 139649090267008 learning.py:507] global step 325: loss = 0.4129 (3.063 sec/step)\n",
            "I0619 17:44:07.532844 139649090267008 learning.py:507] global step 325: loss = 0.4732 (2.984 sec/step)\n",
            "I0619 17:44:10.638408 139649090267008 learning.py:507] global step 326: loss = 0.3500 (3.104 sec/step)\n",
            "I0619 17:44:13.634414 139649090267008 learning.py:507] global step 326: loss = 0.4016 (2.994 sec/step)\n",
            "I0619 17:44:16.582565 139649090267008 learning.py:507] global step 326: loss = 0.4821 (2.944 sec/step)\n",
            "I0619 17:44:19.498458 139649090267008 learning.py:507] global step 326: loss = 0.4075 (2.914 sec/step)\n",
            "I0619 17:44:24.756326 139649090267008 learning.py:507] global step 326: loss = 0.3754 (5.251 sec/step)\n",
            "I0619 17:44:25.992238 139646017689344 supervisor.py:1050] Recording summary at step 326.\n",
            "I0619 17:44:27.904148 139649090267008 learning.py:507] global step 326: loss = 0.5943 (3.146 sec/step)\n",
            "I0619 17:44:31.014957 139649090267008 learning.py:507] global step 326: loss = 0.4010 (3.109 sec/step)\n",
            "I0619 17:44:33.940665 139649090267008 learning.py:507] global step 326: loss = 0.4044 (2.924 sec/step)\n",
            "I0619 17:44:36.877363 139649090267008 learning.py:507] global step 327: loss = 0.4594 (2.934 sec/step)\n",
            "I0619 17:44:39.893074 139649090267008 learning.py:507] global step 327: loss = 0.4195 (3.014 sec/step)\n",
            "I0619 17:44:42.986544 139649090267008 learning.py:507] global step 327: loss = 0.3463 (3.092 sec/step)\n",
            "I0619 17:44:45.947944 139649090267008 learning.py:507] global step 327: loss = 0.3899 (2.960 sec/step)\n",
            "I0619 17:44:48.807913 139649090267008 learning.py:507] global step 327: loss = 0.3327 (2.858 sec/step)\n",
            "I0619 17:44:51.752788 139649090267008 learning.py:507] global step 327: loss = 0.4390 (2.943 sec/step)\n",
            "I0619 17:44:54.698935 139649090267008 learning.py:507] global step 327: loss = 0.4340 (2.945 sec/step)\n",
            "I0619 17:44:57.627660 139649090267008 learning.py:507] global step 327: loss = 0.5093 (2.927 sec/step)\n",
            "I0619 17:45:00.761796 139649090267008 learning.py:507] global step 328: loss = 0.3756 (3.132 sec/step)\n",
            "I0619 17:45:03.707031 139649090267008 learning.py:507] global step 328: loss = 0.3799 (2.943 sec/step)\n",
            "I0619 17:45:06.609989 139649090267008 learning.py:507] global step 328: loss = 0.4338 (2.901 sec/step)\n",
            "I0619 17:45:09.592436 139649090267008 learning.py:507] global step 328: loss = 0.4313 (2.981 sec/step)\n",
            "I0619 17:45:12.529241 139649090267008 learning.py:507] global step 328: loss = 0.3972 (2.935 sec/step)\n",
            "I0619 17:45:15.495697 139649090267008 learning.py:507] global step 328: loss = 0.4594 (2.965 sec/step)\n",
            "I0619 17:45:18.413783 139649090267008 learning.py:507] global step 328: loss = 0.3161 (2.916 sec/step)\n",
            "I0619 17:45:21.338269 139649090267008 learning.py:507] global step 328: loss = 0.3733 (2.923 sec/step)\n",
            "I0619 17:45:24.322262 139649090267008 learning.py:507] global step 329: loss = 0.3468 (2.981 sec/step)\n",
            "I0619 17:45:27.344605 139649090267008 learning.py:507] global step 329: loss = 0.4069 (3.021 sec/step)\n",
            "I0619 17:45:30.332008 139649090267008 learning.py:507] global step 329: loss = 0.4307 (2.986 sec/step)\n",
            "I0619 17:45:33.291793 139649090267008 learning.py:507] global step 329: loss = 0.3907 (2.958 sec/step)\n",
            "I0619 17:45:36.257351 139649090267008 learning.py:507] global step 329: loss = 0.4174 (2.964 sec/step)\n",
            "I0619 17:45:39.141976 139649090267008 learning.py:507] global step 329: loss = 0.3712 (2.883 sec/step)\n",
            "I0619 17:45:42.015104 139649090267008 learning.py:507] global step 329: loss = 0.5211 (2.871 sec/step)\n",
            "I0619 17:45:44.996896 139649090267008 learning.py:507] global step 329: loss = 0.3626 (2.980 sec/step)\n",
            "I0619 17:45:47.945698 139649090267008 learning.py:507] global step 330: loss = 0.4888 (2.946 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:45:50.884028 139649090267008 learning.py:507] global step 330: loss = 0.3425 (2.937 sec/step)\n",
            "I0619 17:45:53.818159 139649090267008 learning.py:507] global step 330: loss = 0.3888 (2.933 sec/step)\n",
            "I0619 17:45:56.757796 139649090267008 learning.py:507] global step 330: loss = 0.3581 (2.938 sec/step)\n",
            "I0619 17:45:59.699180 139649090267008 learning.py:507] global step 330: loss = 0.3139 (2.939 sec/step)\n",
            "I0619 17:46:02.640925 139649090267008 learning.py:507] global step 330: loss = 0.3474 (2.940 sec/step)\n",
            "I0619 17:46:05.652941 139649090267008 learning.py:507] global step 330: loss = 0.4270 (3.010 sec/step)\n",
            "I0619 17:46:08.585803 139649090267008 learning.py:507] global step 330: loss = 0.3997 (2.931 sec/step)\n",
            "I0619 17:46:11.522437 139649090267008 learning.py:507] global step 331: loss = 0.4808 (2.933 sec/step)\n",
            "I0619 17:46:14.482989 139649090267008 learning.py:507] global step 331: loss = 0.3271 (2.958 sec/step)\n",
            "I0619 17:46:17.485990 139649090267008 learning.py:507] global step 331: loss = 0.3628 (3.001 sec/step)\n",
            "I0619 17:46:20.430812 139649090267008 learning.py:507] global step 331: loss = 0.4753 (2.943 sec/step)\n",
            "I0619 17:46:25.573349 139649090267008 learning.py:507] global step 331: loss = 0.3122 (5.136 sec/step)\n",
            "I0619 17:46:26.421184 139646017689344 supervisor.py:1050] Recording summary at step 331.\n",
            "I0619 17:46:28.684173 139649090267008 learning.py:507] global step 331: loss = 0.4260 (3.107 sec/step)\n",
            "I0619 17:46:31.595263 139649090267008 learning.py:507] global step 331: loss = 0.3763 (2.909 sec/step)\n",
            "I0619 17:46:34.523879 139649090267008 learning.py:507] global step 331: loss = 0.5547 (2.927 sec/step)\n",
            "I0619 17:46:37.582762 139649090267008 learning.py:507] global step 332: loss = 0.3804 (3.057 sec/step)\n",
            "I0619 17:46:40.790508 139649090267008 learning.py:507] global step 332: loss = 0.4763 (3.206 sec/step)\n",
            "I0619 17:46:43.797356 139649090267008 learning.py:507] global step 332: loss = 0.4309 (3.005 sec/step)\n",
            "I0619 17:46:46.821680 139649090267008 learning.py:507] global step 332: loss = 0.3870 (3.023 sec/step)\n",
            "I0619 17:46:49.847559 139649090267008 learning.py:507] global step 332: loss = 0.4664 (3.024 sec/step)\n",
            "I0619 17:46:52.777990 139649090267008 learning.py:507] global step 332: loss = 0.3849 (2.929 sec/step)\n",
            "I0619 17:46:55.713292 139649090267008 learning.py:507] global step 332: loss = 0.5297 (2.933 sec/step)\n",
            "I0619 17:46:58.884251 139649090267008 learning.py:507] global step 332: loss = 0.3999 (3.169 sec/step)\n",
            "I0619 17:47:01.876738 139649090267008 learning.py:507] global step 333: loss = 0.4847 (2.990 sec/step)\n",
            "I0619 17:47:04.948595 139649090267008 learning.py:507] global step 333: loss = 0.6029 (3.070 sec/step)\n",
            "I0619 17:47:07.935696 139649090267008 learning.py:507] global step 333: loss = 0.3915 (2.985 sec/step)\n",
            "I0619 17:47:10.860152 139649090267008 learning.py:507] global step 333: loss = 0.3922 (2.923 sec/step)\n",
            "I0619 17:47:13.769689 139649090267008 learning.py:507] global step 333: loss = 0.4545 (2.908 sec/step)\n",
            "I0619 17:47:16.719516 139649090267008 learning.py:507] global step 333: loss = 0.4045 (2.948 sec/step)\n",
            "I0619 17:47:19.714850 139649090267008 learning.py:507] global step 333: loss = 0.3395 (2.994 sec/step)\n",
            "I0619 17:47:22.879911 139649090267008 learning.py:507] global step 333: loss = 0.4192 (3.164 sec/step)\n",
            "I0619 17:47:25.855768 139649090267008 learning.py:507] global step 334: loss = 0.4768 (2.974 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:47:28.905516 139649090267008 learning.py:507] global step 334: loss = 0.4421 (3.048 sec/step)\n",
            "I0619 17:47:31.908079 139649090267008 learning.py:507] global step 334: loss = 0.3844 (3.001 sec/step)\n",
            "I0619 17:47:34.924798 139649090267008 learning.py:507] global step 334: loss = 0.4636 (3.015 sec/step)\n",
            "I0619 17:47:38.327378 139649090267008 learning.py:507] global step 334: loss = 0.4684 (3.401 sec/step)\n",
            "I0619 17:47:41.268516 139649090267008 learning.py:507] global step 334: loss = 0.4439 (2.939 sec/step)\n",
            "I0619 17:47:44.237361 139649090267008 learning.py:507] global step 334: loss = 0.3738 (2.967 sec/step)\n",
            "I0619 17:47:47.174596 139649090267008 learning.py:507] global step 334: loss = 0.3942 (2.935 sec/step)\n",
            "I0619 17:47:50.131569 139649090267008 learning.py:507] global step 335: loss = 0.4111 (2.954 sec/step)\n",
            "I0619 17:47:53.036940 139649090267008 learning.py:507] global step 335: loss = 0.3936 (2.904 sec/step)\n",
            "I0619 17:47:56.407871 139649090267008 learning.py:507] global step 335: loss = 0.4623 (3.369 sec/step)\n",
            "I0619 17:47:59.346431 139649090267008 learning.py:507] global step 335: loss = 0.5104 (2.937 sec/step)\n",
            "I0619 17:48:02.328625 139649090267008 learning.py:507] global step 335: loss = 0.4697 (2.981 sec/step)\n",
            "I0619 17:48:05.286995 139649090267008 learning.py:507] global step 335: loss = 0.4678 (2.956 sec/step)\n",
            "I0619 17:48:08.252150 139649090267008 learning.py:507] global step 335: loss = 0.4185 (2.964 sec/step)\n",
            "I0619 17:48:11.251158 139649090267008 learning.py:507] global step 335: loss = 0.4708 (2.997 sec/step)\n",
            "I0619 17:48:14.269744 139649090267008 learning.py:507] global step 336: loss = 0.4768 (3.016 sec/step)\n",
            "I0619 17:48:17.246759 139649090267008 learning.py:507] global step 336: loss = 0.5463 (2.975 sec/step)\n",
            "I0619 17:48:20.160188 139649090267008 learning.py:507] global step 336: loss = 0.4137 (2.912 sec/step)\n",
            "I0619 17:48:25.272443 139649090267008 learning.py:507] global step 336: loss = 0.4411 (5.106 sec/step)\n",
            "I0619 17:48:26.018649 139646017689344 supervisor.py:1050] Recording summary at step 336.\n",
            "I0619 17:48:28.288015 139649090267008 learning.py:507] global step 336: loss = 0.4316 (3.014 sec/step)\n",
            "I0619 17:48:31.279200 139649090267008 learning.py:507] global step 336: loss = 0.3329 (2.989 sec/step)\n",
            "I0619 17:48:34.158492 139649090267008 learning.py:507] global step 336: loss = 0.3830 (2.878 sec/step)\n",
            "I0619 17:48:37.111794 139649090267008 learning.py:507] global step 336: loss = 0.4298 (2.952 sec/step)\n",
            "I0619 17:48:40.050139 139649090267008 learning.py:507] global step 337: loss = 0.3220 (2.936 sec/step)\n",
            "I0619 17:48:43.009261 139649090267008 learning.py:507] global step 337: loss = 0.4036 (2.957 sec/step)\n",
            "I0619 17:48:45.945072 139649090267008 learning.py:507] global step 337: loss = 0.3912 (2.934 sec/step)\n",
            "I0619 17:48:48.884348 139649090267008 learning.py:507] global step 337: loss = 0.3889 (2.938 sec/step)\n",
            "I0619 17:48:51.931303 139649090267008 learning.py:507] global step 337: loss = 0.4178 (3.045 sec/step)\n",
            "I0619 17:48:54.844158 139649090267008 learning.py:507] global step 337: loss = 0.4925 (2.911 sec/step)\n",
            "I0619 17:48:57.790692 139649090267008 learning.py:507] global step 337: loss = 0.3630 (2.945 sec/step)\n",
            "I0619 17:49:00.771613 139649090267008 learning.py:507] global step 337: loss = 0.5059 (2.979 sec/step)\n",
            "I0619 17:49:03.733559 139649090267008 learning.py:507] global step 338: loss = 0.3964 (2.960 sec/step)\n",
            "I0619 17:49:06.776128 139649090267008 learning.py:507] global step 338: loss = 0.4159 (3.041 sec/step)\n",
            "I0619 17:49:09.757338 139649090267008 learning.py:507] global step 338: loss = 0.4180 (2.980 sec/step)\n",
            "I0619 17:49:12.702807 139649090267008 learning.py:507] global step 338: loss = 0.4422 (2.944 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:49:15.687647 139649090267008 learning.py:507] global step 338: loss = 0.4781 (2.983 sec/step)\n",
            "I0619 17:49:18.681440 139649090267008 learning.py:507] global step 338: loss = 0.3477 (2.992 sec/step)\n",
            "I0619 17:49:21.621361 139649090267008 learning.py:507] global step 338: loss = 0.3885 (2.938 sec/step)\n",
            "I0619 17:49:24.658019 139649090267008 learning.py:507] global step 338: loss = 0.4856 (3.035 sec/step)\n",
            "I0619 17:49:27.746070 139649090267008 learning.py:507] global step 339: loss = 0.4621 (3.085 sec/step)\n",
            "I0619 17:49:30.747673 139649090267008 learning.py:507] global step 339: loss = 0.4597 (3.000 sec/step)\n",
            "I0619 17:49:33.774702 139649090267008 learning.py:507] global step 339: loss = 0.3361 (3.025 sec/step)\n",
            "I0619 17:49:36.743735 139649090267008 learning.py:507] global step 339: loss = 0.4505 (2.967 sec/step)\n",
            "I0619 17:49:39.691578 139649090267008 learning.py:507] global step 339: loss = 0.4907 (2.946 sec/step)\n",
            "I0619 17:49:42.588993 139649090267008 learning.py:507] global step 339: loss = 0.4086 (2.896 sec/step)\n",
            "I0619 17:49:45.718134 139649090267008 learning.py:507] global step 339: loss = 0.3937 (3.127 sec/step)\n",
            "I0619 17:49:48.641573 139649090267008 learning.py:507] global step 339: loss = 0.4165 (2.922 sec/step)\n",
            "I0619 17:49:51.657312 139649090267008 learning.py:507] global step 340: loss = 0.3961 (3.013 sec/step)\n",
            "I0619 17:49:54.608406 139649090267008 learning.py:507] global step 340: loss = 0.3410 (2.949 sec/step)\n",
            "I0619 17:49:57.552753 139649090267008 learning.py:507] global step 340: loss = 0.3706 (2.943 sec/step)\n",
            "I0619 17:50:00.509168 139649090267008 learning.py:507] global step 340: loss = 0.4742 (2.955 sec/step)\n",
            "I0619 17:50:03.422014 139649090267008 learning.py:507] global step 340: loss = 0.3754 (2.911 sec/step)\n",
            "I0619 17:50:06.301531 139649090267008 learning.py:507] global step 340: loss = 0.4791 (2.878 sec/step)\n",
            "I0619 17:50:09.180835 139649090267008 learning.py:507] global step 340: loss = 0.4424 (2.878 sec/step)\n",
            "I0619 17:50:12.128232 139649090267008 learning.py:507] global step 340: loss = 0.3976 (2.946 sec/step)\n",
            "I0619 17:50:15.139138 139649090267008 learning.py:507] global step 341: loss = 0.5012 (3.009 sec/step)\n",
            "I0619 17:50:18.102614 139649090267008 learning.py:507] global step 341: loss = 0.3343 (2.962 sec/step)\n",
            "I0619 17:50:20.931283 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 17:50:21.195680 139649090267008 learning.py:507] global step 341: loss = 0.4121 (3.091 sec/step)\n",
            "I0619 17:50:27.403983 139646017689344 supervisor.py:1050] Recording summary at step 341.\n",
            "I0619 17:50:27.963072 139649090267008 learning.py:507] global step 341: loss = 0.3813 (6.766 sec/step)\n",
            "I0619 17:50:30.959102 139649090267008 learning.py:507] global step 341: loss = 0.3853 (2.994 sec/step)\n",
            "I0619 17:50:34.003320 139649090267008 learning.py:507] global step 341: loss = 0.4549 (3.042 sec/step)\n",
            "I0619 17:50:36.998750 139649090267008 learning.py:507] global step 341: loss = 0.4638 (2.994 sec/step)\n",
            "I0619 17:50:40.070702 139649090267008 learning.py:507] global step 341: loss = 0.4883 (3.070 sec/step)\n",
            "I0619 17:50:43.050590 139649090267008 learning.py:507] global step 342: loss = 0.4818 (2.978 sec/step)\n",
            "I0619 17:50:46.093077 139649090267008 learning.py:507] global step 342: loss = 0.4942 (3.041 sec/step)\n",
            "I0619 17:50:49.079858 139649090267008 learning.py:507] global step 342: loss = 0.4032 (2.985 sec/step)\n",
            "I0619 17:50:52.117458 139649090267008 learning.py:507] global step 342: loss = 0.3867 (3.036 sec/step)\n",
            "I0619 17:50:55.038908 139649090267008 learning.py:507] global step 342: loss = 0.3452 (2.920 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:50:58.007274 139649090267008 learning.py:507] global step 342: loss = 0.3457 (2.966 sec/step)\n",
            "I0619 17:51:01.119389 139649090267008 learning.py:507] global step 342: loss = 0.3322 (3.110 sec/step)\n",
            "I0619 17:51:04.077342 139649090267008 learning.py:507] global step 342: loss = 0.3934 (2.956 sec/step)\n",
            "I0619 17:51:07.126624 139649090267008 learning.py:507] global step 343: loss = 0.4293 (3.047 sec/step)\n",
            "I0619 17:51:10.145812 139649090267008 learning.py:507] global step 343: loss = 0.4452 (3.017 sec/step)\n",
            "I0619 17:51:13.133085 139649090267008 learning.py:507] global step 343: loss = 0.3718 (2.985 sec/step)\n",
            "I0619 17:51:16.093841 139649090267008 learning.py:507] global step 343: loss = 0.4999 (2.959 sec/step)\n",
            "I0619 17:51:19.281477 139649090267008 learning.py:507] global step 343: loss = 0.3988 (3.186 sec/step)\n",
            "I0619 17:51:22.229063 139649090267008 learning.py:507] global step 343: loss = 0.3958 (2.946 sec/step)\n",
            "I0619 17:51:25.187429 139649090267008 learning.py:507] global step 343: loss = 0.3313 (2.957 sec/step)\n",
            "I0619 17:51:28.212666 139649090267008 learning.py:507] global step 343: loss = 0.3732 (3.023 sec/step)\n",
            "I0619 17:51:31.152360 139649090267008 learning.py:507] global step 344: loss = 0.4042 (2.937 sec/step)\n",
            "I0619 17:51:34.179116 139649090267008 learning.py:507] global step 344: loss = 0.4464 (3.025 sec/step)\n",
            "I0619 17:51:37.146972 139649090267008 learning.py:507] global step 344: loss = 0.4058 (2.966 sec/step)\n",
            "I0619 17:51:40.090069 139649090267008 learning.py:507] global step 344: loss = 0.3994 (2.941 sec/step)\n",
            "I0619 17:51:43.098982 139649090267008 learning.py:507] global step 344: loss = 0.4166 (3.007 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:51:46.072263 139649090267008 learning.py:507] global step 344: loss = 0.3725 (2.972 sec/step)\n",
            "I0619 17:51:49.093618 139649090267008 learning.py:507] global step 344: loss = 0.4455 (3.020 sec/step)\n",
            "I0619 17:51:52.022695 139649090267008 learning.py:507] global step 344: loss = 0.3216 (2.927 sec/step)\n",
            "I0619 17:51:54.964246 139649090267008 learning.py:507] global step 345: loss = 0.3462 (2.940 sec/step)\n",
            "I0619 17:51:57.894060 139649090267008 learning.py:507] global step 345: loss = 0.5663 (2.928 sec/step)\n",
            "I0619 17:52:00.885239 139649090267008 learning.py:507] global step 345: loss = 0.3163 (2.990 sec/step)\n",
            "I0619 17:52:03.903951 139649090267008 learning.py:507] global step 345: loss = 0.4340 (3.017 sec/step)\n",
            "I0619 17:52:07.062129 139649090267008 learning.py:507] global step 345: loss = 0.3539 (3.157 sec/step)\n",
            "I0619 17:52:10.080414 139649090267008 learning.py:507] global step 345: loss = 0.4416 (3.017 sec/step)\n",
            "I0619 17:52:13.077873 139649090267008 learning.py:507] global step 345: loss = 0.4560 (2.996 sec/step)\n",
            "I0619 17:52:16.012310 139649090267008 learning.py:507] global step 345: loss = 0.5005 (2.933 sec/step)\n",
            "I0619 17:52:19.008139 139649090267008 learning.py:507] global step 346: loss = 0.4475 (2.994 sec/step)\n",
            "I0619 17:52:23.362049 139649090267008 learning.py:507] global step 346: loss = 0.4936 (4.341 sec/step)\n",
            "I0619 17:52:26.267414 139646017689344 supervisor.py:1050] Recording summary at step 346.\n",
            "I0619 17:52:27.282481 139649090267008 learning.py:507] global step 346: loss = 0.4261 (3.911 sec/step)\n",
            "I0619 17:52:30.246383 139649090267008 learning.py:507] global step 346: loss = 0.3695 (2.962 sec/step)\n",
            "I0619 17:52:33.224276 139649090267008 learning.py:507] global step 346: loss = 0.4236 (2.976 sec/step)\n",
            "I0619 17:52:36.184096 139649090267008 learning.py:507] global step 346: loss = 0.4218 (2.958 sec/step)\n",
            "I0619 17:52:39.228547 139649090267008 learning.py:507] global step 346: loss = 0.3056 (3.043 sec/step)\n",
            "I0619 17:52:42.217351 139649090267008 learning.py:507] global step 346: loss = 0.6742 (2.987 sec/step)\n",
            "I0619 17:52:45.281199 139649090267008 learning.py:507] global step 347: loss = 0.4159 (3.062 sec/step)\n",
            "I0619 17:52:48.292935 139649090267008 learning.py:507] global step 347: loss = 0.3301 (3.010 sec/step)\n",
            "I0619 17:52:51.243719 139649090267008 learning.py:507] global step 347: loss = 0.5176 (2.949 sec/step)\n",
            "I0619 17:52:54.196672 139649090267008 learning.py:507] global step 347: loss = 0.4028 (2.951 sec/step)\n",
            "I0619 17:52:57.231610 139649090267008 learning.py:507] global step 347: loss = 0.4076 (3.033 sec/step)\n",
            "I0619 17:53:00.197928 139649090267008 learning.py:507] global step 347: loss = 0.4559 (2.964 sec/step)\n",
            "I0619 17:53:03.152458 139649090267008 learning.py:507] global step 347: loss = 0.4388 (2.953 sec/step)\n",
            "I0619 17:53:06.163502 139649090267008 learning.py:507] global step 347: loss = 0.3576 (3.009 sec/step)\n",
            "I0619 17:53:09.126445 139649090267008 learning.py:507] global step 348: loss = 0.4290 (2.961 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:53:12.176537 139649090267008 learning.py:507] global step 348: loss = 0.3806 (3.048 sec/step)\n",
            "I0619 17:53:15.187937 139649090267008 learning.py:507] global step 348: loss = 0.4089 (3.010 sec/step)\n",
            "I0619 17:53:18.203548 139649090267008 learning.py:507] global step 348: loss = 0.4129 (3.014 sec/step)\n",
            "I0619 17:53:21.188736 139649090267008 learning.py:507] global step 348: loss = 0.4112 (2.984 sec/step)\n",
            "I0619 17:53:24.160053 139649090267008 learning.py:507] global step 348: loss = 0.3092 (2.969 sec/step)\n",
            "I0619 17:53:27.170874 139649090267008 learning.py:507] global step 348: loss = 0.3560 (3.009 sec/step)\n",
            "I0619 17:53:30.120237 139649090267008 learning.py:507] global step 348: loss = 0.4639 (2.947 sec/step)\n",
            "I0619 17:53:33.181913 139649090267008 learning.py:507] global step 349: loss = 0.3723 (3.059 sec/step)\n",
            "I0619 17:53:36.207489 139649090267008 learning.py:507] global step 349: loss = 0.3539 (3.024 sec/step)\n",
            "I0619 17:53:39.094467 139649090267008 learning.py:507] global step 349: loss = 0.3324 (2.885 sec/step)\n",
            "I0619 17:53:42.070982 139649090267008 learning.py:507] global step 349: loss = 0.4035 (2.975 sec/step)\n",
            "I0619 17:53:45.107945 139649090267008 learning.py:507] global step 349: loss = 0.3991 (3.035 sec/step)\n",
            "I0619 17:53:47.998472 139649090267008 learning.py:507] global step 349: loss = 0.3539 (2.889 sec/step)\n",
            "I0619 17:53:51.048393 139649090267008 learning.py:507] global step 349: loss = 0.3771 (3.048 sec/step)\n",
            "I0619 17:53:54.233455 139649090267008 learning.py:507] global step 349: loss = 0.4526 (3.183 sec/step)\n",
            "I0619 17:53:57.158757 139649090267008 learning.py:507] global step 350: loss = 0.4628 (2.923 sec/step)\n",
            "I0619 17:54:00.119487 139649090267008 learning.py:507] global step 350: loss = 0.3723 (2.959 sec/step)\n",
            "I0619 17:54:03.057509 139649090267008 learning.py:507] global step 350: loss = 0.3273 (2.936 sec/step)\n",
            "I0619 17:54:06.021335 139649090267008 learning.py:507] global step 350: loss = 0.4997 (2.962 sec/step)\n",
            "I0619 17:54:08.988309 139649090267008 learning.py:507] global step 350: loss = 0.3265 (2.965 sec/step)\n",
            "I0619 17:54:12.074641 139649090267008 learning.py:507] global step 350: loss = 0.4566 (3.084 sec/step)\n",
            "I0619 17:54:15.017126 139649090267008 learning.py:507] global step 350: loss = 0.4899 (2.940 sec/step)\n",
            "I0619 17:54:18.002429 139649090267008 learning.py:507] global step 350: loss = 0.3385 (2.984 sec/step)\n",
            "I0619 17:54:20.970740 139649090267008 learning.py:507] global step 351: loss = 0.3760 (2.967 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:54:25.579622 139646017689344 supervisor.py:1050] Recording summary at step 351.\n",
            "I0619 17:54:26.130069 139649090267008 learning.py:507] global step 351: loss = 0.3829 (5.157 sec/step)\n",
            "I0619 17:54:29.055352 139649090267008 learning.py:507] global step 351: loss = 0.3915 (2.923 sec/step)\n",
            "I0619 17:54:32.017898 139649090267008 learning.py:507] global step 351: loss = 0.5014 (2.961 sec/step)\n",
            "I0619 17:54:34.983814 139649090267008 learning.py:507] global step 351: loss = 0.4520 (2.964 sec/step)\n",
            "I0619 17:54:37.953389 139649090267008 learning.py:507] global step 351: loss = 0.4030 (2.968 sec/step)\n",
            "I0619 17:54:40.964059 139649090267008 learning.py:507] global step 351: loss = 0.4147 (3.009 sec/step)\n",
            "I0619 17:54:43.984540 139649090267008 learning.py:507] global step 351: loss = 0.4145 (3.019 sec/step)\n",
            "I0619 17:54:46.968009 139649090267008 learning.py:507] global step 352: loss = 0.4498 (2.982 sec/step)\n",
            "I0619 17:54:49.943783 139649090267008 learning.py:507] global step 352: loss = 0.3887 (2.974 sec/step)\n",
            "I0619 17:54:52.870690 139649090267008 learning.py:507] global step 352: loss = 0.3715 (2.925 sec/step)\n",
            "I0619 17:54:55.898032 139649090267008 learning.py:507] global step 352: loss = 0.3489 (3.026 sec/step)\n",
            "I0619 17:54:58.992011 139649090267008 learning.py:507] global step 352: loss = 0.4425 (3.092 sec/step)\n",
            "I0619 17:55:01.917532 139649090267008 learning.py:507] global step 352: loss = 0.4806 (2.924 sec/step)\n",
            "I0619 17:55:04.874710 139649090267008 learning.py:507] global step 352: loss = 0.3268 (2.955 sec/step)\n",
            "I0619 17:55:07.840548 139649090267008 learning.py:507] global step 352: loss = 0.3420 (2.964 sec/step)\n",
            "I0619 17:55:10.767571 139649090267008 learning.py:507] global step 353: loss = 0.3799 (2.925 sec/step)\n",
            "I0619 17:55:13.805536 139649090267008 learning.py:507] global step 353: loss = 0.3140 (3.036 sec/step)\n",
            "I0619 17:55:16.756380 139649090267008 learning.py:507] global step 353: loss = 0.3998 (2.949 sec/step)\n",
            "I0619 17:55:19.697026 139649090267008 learning.py:507] global step 353: loss = 0.3827 (2.938 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:55:22.641679 139649090267008 learning.py:507] global step 353: loss = 0.4636 (2.943 sec/step)\n",
            "I0619 17:55:25.645758 139649090267008 learning.py:507] global step 353: loss = 0.3615 (3.002 sec/step)\n",
            "I0619 17:55:28.602394 139649090267008 learning.py:507] global step 353: loss = 0.3547 (2.955 sec/step)\n",
            "I0619 17:55:31.724075 139649090267008 learning.py:507] global step 353: loss = 0.3398 (3.120 sec/step)\n",
            "I0619 17:55:34.660466 139649090267008 learning.py:507] global step 354: loss = 0.3976 (2.934 sec/step)\n",
            "I0619 17:55:37.740103 139649090267008 learning.py:507] global step 354: loss = 0.3891 (3.078 sec/step)\n",
            "I0619 17:55:40.703757 139649090267008 learning.py:507] global step 354: loss = 0.4176 (2.962 sec/step)\n",
            "I0619 17:55:43.608801 139649090267008 learning.py:507] global step 354: loss = 0.3625 (2.903 sec/step)\n",
            "I0619 17:55:46.572491 139649090267008 learning.py:507] global step 354: loss = 0.3065 (2.962 sec/step)\n",
            "I0619 17:55:49.497004 139649090267008 learning.py:507] global step 354: loss = 0.4140 (2.923 sec/step)\n",
            "I0619 17:55:52.443823 139649090267008 learning.py:507] global step 354: loss = 0.4694 (2.945 sec/step)\n",
            "I0619 17:55:55.387296 139649090267008 learning.py:507] global step 354: loss = 0.4429 (2.942 sec/step)\n",
            "I0619 17:55:58.356678 139649090267008 learning.py:507] global step 355: loss = 0.4183 (2.966 sec/step)\n",
            "I0619 17:56:01.331862 139649090267008 learning.py:507] global step 355: loss = 0.4381 (2.973 sec/step)\n",
            "I0619 17:56:04.318465 139649090267008 learning.py:507] global step 355: loss = 0.4078 (2.985 sec/step)\n",
            "I0619 17:56:07.346435 139649090267008 learning.py:507] global step 355: loss = 0.5198 (3.026 sec/step)\n",
            "I0619 17:56:10.336985 139649090267008 learning.py:507] global step 355: loss = 0.4434 (2.989 sec/step)\n",
            "I0619 17:56:13.269324 139649090267008 learning.py:507] global step 355: loss = 0.3943 (2.931 sec/step)\n",
            "I0619 17:56:16.276484 139649090267008 learning.py:507] global step 355: loss = 0.4283 (3.006 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 17:56:19.245193 139649090267008 learning.py:507] global step 355: loss = 0.3953 (2.967 sec/step)\n",
            "I0619 17:56:23.808511 139649090267008 learning.py:507] global step 356: loss = 0.4789 (4.553 sec/step)\n",
            "I0619 17:56:26.345667 139646017689344 supervisor.py:1050] Recording summary at step 356.\n",
            "I0619 17:56:27.574040 139649090267008 learning.py:507] global step 356: loss = 0.3493 (3.757 sec/step)\n",
            "I0619 17:56:30.509561 139649090267008 learning.py:507] global step 356: loss = 0.4377 (2.934 sec/step)\n",
            "I0619 17:56:33.484686 139649090267008 learning.py:507] global step 356: loss = 0.4434 (2.974 sec/step)\n",
            "I0619 17:56:36.426901 139649090267008 learning.py:507] global step 356: loss = 0.3462 (2.940 sec/step)\n",
            "I0619 17:56:39.409864 139649090267008 learning.py:507] global step 356: loss = 0.3186 (2.981 sec/step)\n",
            "I0619 17:56:42.352928 139649090267008 learning.py:507] global step 356: loss = 0.3986 (2.941 sec/step)\n",
            "I0619 17:56:45.340146 139649090267008 learning.py:507] global step 356: loss = 0.3613 (2.985 sec/step)\n",
            "I0619 17:56:48.253228 139649090267008 learning.py:507] global step 357: loss = 0.3759 (2.911 sec/step)\n",
            "I0619 17:56:51.284845 139649090267008 learning.py:507] global step 357: loss = 0.4412 (3.030 sec/step)\n",
            "I0619 17:56:54.228522 139649090267008 learning.py:507] global step 357: loss = 0.5255 (2.942 sec/step)\n",
            "I0619 17:56:57.171530 139649090267008 learning.py:507] global step 357: loss = 0.4318 (2.941 sec/step)\n",
            "I0619 17:57:00.124843 139649090267008 learning.py:507] global step 357: loss = 0.4305 (2.952 sec/step)\n",
            "I0619 17:57:03.134512 139649090267008 learning.py:507] global step 357: loss = 0.4174 (3.008 sec/step)\n",
            "I0619 17:57:06.081992 139649090267008 learning.py:507] global step 357: loss = 0.4516 (2.946 sec/step)\n",
            "I0619 17:57:09.041310 139649090267008 learning.py:507] global step 357: loss = 0.3635 (2.957 sec/step)\n",
            "I0619 17:57:12.515659 139649090267008 learning.py:507] global step 358: loss = 0.3265 (3.472 sec/step)\n",
            "I0619 17:57:15.510738 139649090267008 learning.py:507] global step 358: loss = 0.4635 (2.993 sec/step)\n",
            "I0619 17:57:18.522471 139649090267008 learning.py:507] global step 358: loss = 0.3857 (3.010 sec/step)\n",
            "I0619 17:57:21.512099 139649090267008 learning.py:507] global step 358: loss = 0.4028 (2.988 sec/step)\n",
            "I0619 17:57:24.528600 139649090267008 learning.py:507] global step 358: loss = 0.3971 (3.015 sec/step)\n",
            "I0619 17:57:27.552479 139649090267008 learning.py:507] global step 358: loss = 0.4595 (3.022 sec/step)\n",
            "I0619 17:57:31.098103 139649090267008 learning.py:507] global step 358: loss = 0.3602 (3.544 sec/step)\n",
            "I0619 17:57:34.070536 139649090267008 learning.py:507] global step 358: loss = 0.4228 (2.971 sec/step)\n",
            "I0619 17:57:37.212359 139649090267008 learning.py:507] global step 359: loss = 0.3839 (3.139 sec/step)\n",
            "I0619 17:57:40.209391 139649090267008 learning.py:507] global step 359: loss = 0.4900 (2.995 sec/step)\n",
            "I0619 17:57:43.164438 139649090267008 learning.py:507] global step 359: loss = 0.3919 (2.953 sec/step)\n",
            "I0619 17:57:46.178874 139649090267008 learning.py:507] global step 359: loss = 0.5429 (3.013 sec/step)\n",
            "I0619 17:57:49.256446 139649090267008 learning.py:507] global step 359: loss = 0.4669 (3.076 sec/step)\n",
            "I0619 17:57:52.320803 139649090267008 learning.py:507] global step 359: loss = 0.3625 (3.063 sec/step)\n",
            "I0619 17:57:55.507037 139649090267008 learning.py:507] global step 359: loss = 0.4197 (3.185 sec/step)\n",
            "I0619 17:57:58.598338 139649090267008 learning.py:507] global step 359: loss = 0.3579 (3.090 sec/step)\n",
            "I0619 17:58:01.656940 139649090267008 learning.py:507] global step 360: loss = 0.4823 (3.056 sec/step)\n",
            "I0619 17:58:04.671864 139649090267008 learning.py:507] global step 360: loss = 0.3828 (3.013 sec/step)\n",
            "I0619 17:58:07.685503 139649090267008 learning.py:507] global step 360: loss = 0.4443 (3.012 sec/step)\n",
            "I0619 17:58:10.721131 139649090267008 learning.py:507] global step 360: loss = 0.3892 (3.034 sec/step)\n",
            "I0619 17:58:13.712644 139649090267008 learning.py:507] global step 360: loss = 0.4933 (2.990 sec/step)\n",
            "I0619 17:58:16.755183 139649090267008 learning.py:507] global step 360: loss = 0.3824 (3.041 sec/step)\n",
            "I0619 17:58:19.734160 139649090267008 learning.py:507] global step 360: loss = 0.4006 (2.977 sec/step)\n",
            "I0619 17:58:24.892154 139649090267008 learning.py:507] global step 360: loss = 0.3786 (5.145 sec/step)\n",
            "I0619 17:58:26.080886 139646017689344 supervisor.py:1050] Recording summary at step 360.\n",
            "I0619 17:58:28.164786 139649090267008 learning.py:507] global step 361: loss = 0.3422 (3.266 sec/step)\n",
            "I0619 17:58:31.192662 139649090267008 learning.py:507] global step 361: loss = 0.3818 (3.026 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 17:58:34.198628 139649090267008 learning.py:507] global step 361: loss = 0.3866 (3.004 sec/step)\n",
            "I0619 17:58:37.203083 139649090267008 learning.py:507] global step 361: loss = 0.3485 (3.003 sec/step)\n",
            "I0619 17:58:40.314396 139649090267008 learning.py:507] global step 361: loss = 0.4037 (3.109 sec/step)\n",
            "I0619 17:58:43.468380 139649090267008 learning.py:507] global step 361: loss = 0.4072 (3.152 sec/step)\n",
            "I0619 17:58:46.488370 139649090267008 learning.py:507] global step 361: loss = 0.3488 (3.018 sec/step)\n",
            "I0619 17:58:49.561300 139649090267008 learning.py:507] global step 361: loss = 0.4310 (3.071 sec/step)\n",
            "I0619 17:58:52.555350 139649090267008 learning.py:507] global step 362: loss = 0.3748 (2.992 sec/step)\n",
            "I0619 17:58:55.619448 139649090267008 learning.py:507] global step 362: loss = 0.4401 (3.062 sec/step)\n",
            "I0619 17:58:58.591440 139649090267008 learning.py:507] global step 362: loss = 0.4115 (2.970 sec/step)\n",
            "I0619 17:59:01.777347 139649090267008 learning.py:507] global step 362: loss = 0.4955 (3.184 sec/step)\n",
            "I0619 17:59:04.810355 139649090267008 learning.py:507] global step 362: loss = 0.3831 (3.031 sec/step)\n",
            "I0619 17:59:07.859081 139649090267008 learning.py:507] global step 362: loss = 0.3995 (3.047 sec/step)\n",
            "I0619 17:59:10.865725 139649090267008 learning.py:507] global step 362: loss = 0.4693 (3.005 sec/step)\n",
            "I0619 17:59:13.912402 139649090267008 learning.py:507] global step 362: loss = 0.3498 (3.045 sec/step)\n",
            "I0619 17:59:16.889823 139649090267008 learning.py:507] global step 363: loss = 0.4085 (2.976 sec/step)\n",
            "I0619 17:59:19.891126 139649090267008 learning.py:507] global step 363: loss = 0.3606 (3.000 sec/step)\n",
            "I0619 17:59:22.945385 139649090267008 learning.py:507] global step 363: loss = 0.4742 (3.053 sec/step)\n",
            "I0619 17:59:25.990994 139649090267008 learning.py:507] global step 363: loss = 0.4395 (3.044 sec/step)\n",
            "I0619 17:59:28.987662 139649090267008 learning.py:507] global step 363: loss = 0.3768 (2.995 sec/step)\n",
            "I0619 17:59:31.971228 139649090267008 learning.py:507] global step 363: loss = 0.3459 (2.982 sec/step)\n",
            "I0619 17:59:34.928330 139649090267008 learning.py:507] global step 363: loss = 0.4018 (2.955 sec/step)\n",
            "I0619 17:59:37.892493 139649090267008 learning.py:507] global step 363: loss = 0.3476 (2.962 sec/step)\n",
            "I0619 17:59:40.978471 139649090267008 learning.py:507] global step 364: loss = 0.3570 (3.084 sec/step)\n",
            "I0619 17:59:43.980288 139649090267008 learning.py:507] global step 364: loss = 0.4238 (3.000 sec/step)\n",
            "I0619 17:59:46.927354 139649090267008 learning.py:507] global step 364: loss = 0.3607 (2.945 sec/step)\n",
            "I0619 17:59:49.952158 139649090267008 learning.py:507] global step 364: loss = 0.3788 (3.023 sec/step)\n",
            "I0619 17:59:52.901334 139649090267008 learning.py:507] global step 364: loss = 0.5015 (2.948 sec/step)\n",
            "I0619 17:59:56.057247 139649090267008 learning.py:507] global step 364: loss = 0.4112 (3.154 sec/step)\n",
            "I0619 17:59:59.055123 139649090267008 learning.py:507] global step 364: loss = 0.4157 (2.996 sec/step)\n",
            "I0619 18:00:02.026697 139649090267008 learning.py:507] global step 364: loss = 0.3387 (2.970 sec/step)\n",
            "I0619 18:00:05.075194 139649090267008 learning.py:507] global step 365: loss = 0.3020 (3.046 sec/step)\n",
            "I0619 18:00:08.111886 139649090267008 learning.py:507] global step 365: loss = 0.3812 (3.034 sec/step)\n",
            "I0619 18:00:11.129824 139649090267008 learning.py:507] global step 365: loss = 0.4241 (3.016 sec/step)\n",
            "I0619 18:00:14.360153 139649090267008 learning.py:507] global step 365: loss = 0.3503 (3.228 sec/step)\n",
            "I0619 18:00:17.398154 139649090267008 learning.py:507] global step 365: loss = 0.3975 (3.036 sec/step)\n",
            "I0619 18:00:20.538350 139649090267008 learning.py:507] global step 365: loss = 0.3233 (3.138 sec/step)\n",
            "I0619 18:00:20.931274 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:00:27.127065 139649090267008 learning.py:507] global step 365: loss = 0.3251 (6.587 sec/step)\n",
            "I0619 18:00:27.725888 139646017689344 supervisor.py:1050] Recording summary at step 365.\n",
            "I0619 18:00:30.167919 139649090267008 learning.py:507] global step 365: loss = 0.3514 (3.039 sec/step)\n",
            "I0619 18:00:33.157629 139649090267008 learning.py:507] global step 366: loss = 0.3699 (2.988 sec/step)\n",
            "I0619 18:00:36.254116 139649090267008 learning.py:507] global step 366: loss = 0.3571 (3.095 sec/step)\n",
            "I0619 18:00:39.429237 139649090267008 learning.py:507] global step 366: loss = 0.4176 (3.173 sec/step)\n",
            "I0619 18:00:42.411978 139649090267008 learning.py:507] global step 366: loss = 0.4178 (2.981 sec/step)\n",
            "I0619 18:00:45.470349 139649090267008 learning.py:507] global step 366: loss = 0.3978 (3.057 sec/step)\n",
            "I0619 18:00:48.489528 139649090267008 learning.py:507] global step 366: loss = 0.2969 (3.017 sec/step)\n",
            "I0619 18:00:51.656425 139649090267008 learning.py:507] global step 366: loss = 0.3566 (3.165 sec/step)\n",
            "I0619 18:00:54.631218 139649090267008 learning.py:507] global step 366: loss = 0.3549 (2.973 sec/step)\n",
            "I0619 18:00:57.578245 139649090267008 learning.py:507] global step 367: loss = 0.4028 (2.945 sec/step)\n",
            "I0619 18:01:00.685824 139649090267008 learning.py:507] global step 367: loss = 0.3839 (3.106 sec/step)\n",
            "I0619 18:01:03.696500 139649090267008 learning.py:507] global step 367: loss = 0.3472 (3.009 sec/step)\n",
            "I0619 18:01:06.704665 139649090267008 learning.py:507] global step 367: loss = 0.3447 (3.007 sec/step)\n",
            "I0619 18:01:09.970384 139649090267008 learning.py:507] global step 367: loss = 0.3377 (3.264 sec/step)\n",
            "I0619 18:01:12.931788 139649090267008 learning.py:507] global step 367: loss = 0.3081 (2.960 sec/step)\n",
            "I0619 18:01:15.910327 139649090267008 learning.py:507] global step 367: loss = 0.3697 (2.977 sec/step)\n",
            "I0619 18:01:19.157997 139649090267008 learning.py:507] global step 367: loss = 0.4256 (3.246 sec/step)\n",
            "I0619 18:01:22.086375 139649090267008 learning.py:507] global step 368: loss = 0.3998 (2.926 sec/step)\n",
            "I0619 18:01:25.108472 139649090267008 learning.py:507] global step 368: loss = 0.4289 (3.020 sec/step)\n",
            "I0619 18:01:28.076512 139649090267008 learning.py:507] global step 368: loss = 0.3631 (2.966 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:01:31.110588 139649090267008 learning.py:507] global step 368: loss = 0.4160 (3.032 sec/step)\n",
            "I0619 18:01:34.141956 139649090267008 learning.py:507] global step 368: loss = 0.3911 (3.030 sec/step)\n",
            "I0619 18:01:37.098339 139649090267008 learning.py:507] global step 368: loss = 0.3700 (2.955 sec/step)\n",
            "I0619 18:01:40.082464 139649090267008 learning.py:507] global step 368: loss = 0.3388 (2.982 sec/step)\n",
            "I0619 18:01:43.032877 139649090267008 learning.py:507] global step 368: loss = 0.4783 (2.949 sec/step)\n",
            "I0619 18:01:46.181763 139649090267008 learning.py:507] global step 369: loss = 0.3772 (3.146 sec/step)\n",
            "I0619 18:01:49.204336 139649090267008 learning.py:507] global step 369: loss = 0.3464 (3.021 sec/step)\n",
            "I0619 18:01:52.247692 139649090267008 learning.py:507] global step 369: loss = 0.3873 (3.042 sec/step)\n",
            "I0619 18:01:55.326599 139649090267008 learning.py:507] global step 369: loss = 0.3365 (3.077 sec/step)\n",
            "I0619 18:01:58.302923 139649090267008 learning.py:507] global step 369: loss = 0.3612 (2.975 sec/step)\n",
            "I0619 18:02:01.295357 139649090267008 learning.py:507] global step 369: loss = 0.3821 (2.991 sec/step)\n",
            "I0619 18:02:04.450145 139649090267008 learning.py:507] global step 369: loss = 0.3305 (3.153 sec/step)\n",
            "I0619 18:02:07.485815 139649090267008 learning.py:507] global step 369: loss = 0.3810 (3.034 sec/step)\n",
            "I0619 18:02:10.608514 139649090267008 learning.py:507] global step 370: loss = 0.3801 (3.120 sec/step)\n",
            "I0619 18:02:13.934818 139649090267008 learning.py:507] global step 370: loss = 0.4325 (3.325 sec/step)\n",
            "I0619 18:02:16.992020 139649090267008 learning.py:507] global step 370: loss = 0.6668 (3.055 sec/step)\n",
            "I0619 18:02:20.020955 139649090267008 learning.py:507] global step 370: loss = 0.3976 (3.027 sec/step)\n",
            "I0619 18:02:25.076243 139649090267008 learning.py:507] global step 370: loss = 0.5070 (5.049 sec/step)\n",
            "I0619 18:02:26.441669 139646017689344 supervisor.py:1050] Recording summary at step 370.\n",
            "I0619 18:02:28.473920 139649090267008 learning.py:507] global step 370: loss = 0.3448 (3.395 sec/step)\n",
            "I0619 18:02:31.865882 139649090267008 learning.py:507] global step 370: loss = 0.3364 (3.390 sec/step)\n",
            "I0619 18:02:34.944096 139649090267008 learning.py:507] global step 370: loss = 0.3936 (3.076 sec/step)\n",
            "I0619 18:02:37.971068 139649090267008 learning.py:507] global step 371: loss = 0.3605 (3.025 sec/step)\n",
            "I0619 18:02:41.015136 139649090267008 learning.py:507] global step 371: loss = 0.5238 (3.042 sec/step)\n",
            "I0619 18:02:44.069594 139649090267008 learning.py:507] global step 371: loss = 0.3343 (3.053 sec/step)\n",
            "I0619 18:02:47.126298 139649090267008 learning.py:507] global step 371: loss = 0.4502 (3.055 sec/step)\n",
            "I0619 18:02:50.190737 139649090267008 learning.py:507] global step 371: loss = 0.3305 (3.063 sec/step)\n",
            "I0619 18:02:53.336843 139649090267008 learning.py:507] global step 371: loss = 0.4136 (3.145 sec/step)\n",
            "I0619 18:02:56.359577 139649090267008 learning.py:507] global step 371: loss = 0.4225 (3.021 sec/step)\n",
            "I0619 18:02:59.412787 139649090267008 learning.py:507] global step 371: loss = 0.3961 (3.051 sec/step)\n",
            "I0619 18:03:02.553489 139649090267008 learning.py:507] global step 372: loss = 0.3225 (3.138 sec/step)\n",
            "I0619 18:03:05.615894 139649090267008 learning.py:507] global step 372: loss = 0.3520 (3.061 sec/step)\n",
            "I0619 18:03:08.734379 139649090267008 learning.py:507] global step 372: loss = 0.4307 (3.117 sec/step)\n",
            "I0619 18:03:11.765477 139649090267008 learning.py:507] global step 372: loss = 0.3021 (3.029 sec/step)\n",
            "I0619 18:03:14.852658 139649090267008 learning.py:507] global step 372: loss = 0.3053 (3.085 sec/step)\n",
            "I0619 18:03:17.832532 139649090267008 learning.py:507] global step 372: loss = 0.3633 (2.978 sec/step)\n",
            "I0619 18:03:20.848538 139649090267008 learning.py:507] global step 372: loss = 0.3723 (3.014 sec/step)\n",
            "I0619 18:03:23.898161 139649090267008 learning.py:507] global step 372: loss = 0.3894 (3.048 sec/step)\n",
            "I0619 18:03:26.933265 139649090267008 learning.py:507] global step 373: loss = 0.3878 (3.033 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:03:29.906846 139649090267008 learning.py:507] global step 373: loss = 0.3183 (2.972 sec/step)\n",
            "I0619 18:03:33.014535 139649090267008 learning.py:507] global step 373: loss = 0.4159 (3.106 sec/step)\n",
            "I0619 18:03:36.034556 139649090267008 learning.py:507] global step 373: loss = 0.3838 (3.016 sec/step)\n",
            "I0619 18:03:39.074902 139649090267008 learning.py:507] global step 373: loss = 0.4110 (3.037 sec/step)\n",
            "I0619 18:03:42.090811 139649090267008 learning.py:507] global step 373: loss = 0.5009 (3.014 sec/step)\n",
            "I0619 18:03:45.124235 139649090267008 learning.py:507] global step 373: loss = 0.6075 (3.032 sec/step)\n",
            "I0619 18:03:48.130769 139649090267008 learning.py:507] global step 373: loss = 0.3409 (3.005 sec/step)\n",
            "I0619 18:03:51.130646 139649090267008 learning.py:507] global step 374: loss = 0.4240 (2.998 sec/step)\n",
            "I0619 18:03:54.338865 139649090267008 learning.py:507] global step 374: loss = 0.3040 (3.206 sec/step)\n",
            "I0619 18:03:57.328300 139649090267008 learning.py:507] global step 374: loss = 0.3208 (2.987 sec/step)\n",
            "I0619 18:04:00.343486 139649090267008 learning.py:507] global step 374: loss = 0.3192 (3.013 sec/step)\n",
            "I0619 18:04:03.351006 139649090267008 learning.py:507] global step 374: loss = 0.4005 (3.006 sec/step)\n",
            "I0619 18:04:06.434767 139649090267008 learning.py:507] global step 374: loss = 0.3333 (3.082 sec/step)\n",
            "I0619 18:04:09.459331 139649090267008 learning.py:507] global step 374: loss = 0.4560 (3.023 sec/step)\n",
            "I0619 18:04:12.715225 139649090267008 learning.py:507] global step 374: loss = 0.3582 (3.254 sec/step)\n",
            "I0619 18:04:15.735939 139649090267008 learning.py:507] global step 375: loss = 0.4031 (3.019 sec/step)\n",
            "I0619 18:04:18.699878 139649090267008 learning.py:507] global step 375: loss = 0.4490 (2.962 sec/step)\n",
            "I0619 18:04:22.568059 139649090267008 learning.py:507] global step 375: loss = 0.3198 (3.841 sec/step)\n",
            "I0619 18:04:26.209568 139646017689344 supervisor.py:1050] Recording summary at step 375.\n",
            "I0619 18:04:27.021065 139649090267008 learning.py:507] global step 375: loss = 0.4682 (4.441 sec/step)\n",
            "I0619 18:04:30.000297 139649090267008 learning.py:507] global step 375: loss = 0.3465 (2.977 sec/step)\n",
            "I0619 18:04:33.000165 139649090267008 learning.py:507] global step 375: loss = 0.3737 (2.998 sec/step)\n",
            "I0619 18:04:35.977809 139649090267008 learning.py:507] global step 375: loss = 0.3718 (2.976 sec/step)\n",
            "I0619 18:04:38.979630 139649090267008 learning.py:507] global step 375: loss = 0.4117 (3.000 sec/step)\n",
            "I0619 18:04:41.904409 139649090267008 learning.py:507] global step 376: loss = 0.4509 (2.923 sec/step)\n",
            "I0619 18:04:44.981779 139649090267008 learning.py:507] global step 376: loss = 0.3795 (3.076 sec/step)\n",
            "I0619 18:04:48.007812 139649090267008 learning.py:507] global step 376: loss = 0.3559 (3.024 sec/step)\n",
            "I0619 18:04:50.967052 139649090267008 learning.py:507] global step 376: loss = 0.3952 (2.958 sec/step)\n",
            "I0619 18:04:53.951850 139649090267008 learning.py:507] global step 376: loss = 0.3403 (2.983 sec/step)\n",
            "I0619 18:04:56.962643 139649090267008 learning.py:507] global step 376: loss = 0.3894 (3.009 sec/step)\n",
            "I0619 18:04:59.934722 139649090267008 learning.py:507] global step 376: loss = 0.3634 (2.969 sec/step)\n",
            "I0619 18:05:03.174706 139649090267008 learning.py:507] global step 376: loss = 0.3365 (3.238 sec/step)\n",
            "I0619 18:05:06.136152 139649090267008 learning.py:507] global step 377: loss = 0.4214 (2.959 sec/step)\n",
            "I0619 18:05:09.161745 139649090267008 learning.py:507] global step 377: loss = 0.4567 (3.023 sec/step)\n",
            "I0619 18:05:12.154064 139649090267008 learning.py:507] global step 377: loss = 0.4826 (2.991 sec/step)\n",
            "I0619 18:05:15.134288 139649090267008 learning.py:507] global step 377: loss = 0.4187 (2.979 sec/step)\n",
            "I0619 18:05:18.264029 139649090267008 learning.py:507] global step 377: loss = 0.4251 (3.128 sec/step)\n",
            "I0619 18:05:21.340499 139649090267008 learning.py:507] global step 377: loss = 0.3550 (3.075 sec/step)\n",
            "I0619 18:05:24.372523 139649090267008 learning.py:507] global step 377: loss = 0.4519 (3.030 sec/step)\n",
            "I0619 18:05:27.410268 139649090267008 learning.py:507] global step 377: loss = 0.3483 (3.036 sec/step)\n",
            "I0619 18:05:30.452122 139649090267008 learning.py:507] global step 378: loss = 0.4204 (3.039 sec/step)\n",
            "I0619 18:05:33.467612 139649090267008 learning.py:507] global step 378: loss = 0.4198 (3.014 sec/step)\n",
            "I0619 18:05:36.700284 139649090267008 learning.py:507] global step 378: loss = 0.4557 (3.231 sec/step)\n",
            "I0619 18:05:39.753946 139649090267008 learning.py:507] global step 378: loss = 0.4319 (3.052 sec/step)\n",
            "I0619 18:05:42.787683 139649090267008 learning.py:507] global step 378: loss = 0.4000 (3.032 sec/step)\n",
            "I0619 18:05:45.750826 139649090267008 learning.py:507] global step 378: loss = 0.3480 (2.961 sec/step)\n",
            "I0619 18:05:48.880235 139649090267008 learning.py:507] global step 378: loss = 0.4931 (3.128 sec/step)\n",
            "I0619 18:05:51.968314 139649090267008 learning.py:507] global step 378: loss = 0.3892 (3.086 sec/step)\n",
            "I0619 18:05:55.123656 139649090267008 learning.py:507] global step 379: loss = 0.3714 (3.153 sec/step)\n",
            "I0619 18:05:58.154740 139649090267008 learning.py:507] global step 379: loss = 0.4231 (3.029 sec/step)\n",
            "I0619 18:06:01.169583 139649090267008 learning.py:507] global step 379: loss = 0.3796 (3.013 sec/step)\n",
            "I0619 18:06:04.194250 139649090267008 learning.py:507] global step 379: loss = 0.3515 (3.023 sec/step)\n",
            "I0619 18:06:07.241866 139649090267008 learning.py:507] global step 379: loss = 0.4241 (3.046 sec/step)\n",
            "I0619 18:06:10.272361 139649090267008 learning.py:507] global step 379: loss = 0.3976 (3.029 sec/step)\n",
            "I0619 18:06:13.275868 139649090267008 learning.py:507] global step 379: loss = 0.4557 (3.002 sec/step)\n",
            "I0619 18:06:16.286570 139649090267008 learning.py:507] global step 379: loss = 0.3544 (3.009 sec/step)\n",
            "I0619 18:06:19.329272 139649090267008 learning.py:507] global step 380: loss = 0.3792 (3.041 sec/step)\n",
            "I0619 18:06:24.119238 139649090267008 learning.py:507] global step 380: loss = 0.3674 (4.784 sec/step)\n",
            "I0619 18:06:26.439335 139646017689344 supervisor.py:1050] Recording summary at step 380.\n",
            "I0619 18:06:27.802425 139649090267008 learning.py:507] global step 380: loss = 0.3128 (3.681 sec/step)\n",
            "I0619 18:06:30.859902 139649090267008 learning.py:507] global step 380: loss = 0.3259 (3.056 sec/step)\n",
            "I0619 18:06:33.879904 139649090267008 learning.py:507] global step 380: loss = 0.4754 (3.018 sec/step)\n",
            "I0619 18:06:36.916218 139649090267008 learning.py:507] global step 380: loss = 0.3379 (3.034 sec/step)\n",
            "I0619 18:06:40.000433 139649090267008 learning.py:507] global step 380: loss = 0.3993 (3.082 sec/step)\n",
            "I0619 18:06:43.028910 139649090267008 learning.py:507] global step 380: loss = 0.4836 (3.027 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:06:46.118375 139649090267008 learning.py:507] global step 381: loss = 0.3331 (3.086 sec/step)\n",
            "I0619 18:06:49.176035 139649090267008 learning.py:507] global step 381: loss = 0.3437 (3.055 sec/step)\n",
            "I0619 18:06:52.263893 139649090267008 learning.py:507] global step 381: loss = 0.3821 (3.086 sec/step)\n",
            "I0619 18:06:55.384268 139649090267008 learning.py:507] global step 381: loss = 0.3232 (3.119 sec/step)\n",
            "I0619 18:06:58.518643 139649090267008 learning.py:507] global step 381: loss = 0.3347 (3.133 sec/step)\n",
            "I0619 18:07:01.676889 139649090267008 learning.py:507] global step 381: loss = 0.4483 (3.156 sec/step)\n",
            "I0619 18:07:04.655461 139649090267008 learning.py:507] global step 381: loss = 0.3667 (2.977 sec/step)\n",
            "I0619 18:07:07.700102 139649090267008 learning.py:507] global step 381: loss = 0.3579 (3.043 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:07:10.829509 139649090267008 learning.py:507] global step 382: loss = 0.4380 (3.128 sec/step)\n",
            "I0619 18:07:14.177608 139649090267008 learning.py:507] global step 382: loss = 0.3872 (3.346 sec/step)\n",
            "I0619 18:07:17.214122 139649090267008 learning.py:507] global step 382: loss = 0.3807 (3.035 sec/step)\n",
            "I0619 18:07:20.265418 139649090267008 learning.py:507] global step 382: loss = 0.3764 (3.049 sec/step)\n",
            "I0619 18:07:23.362305 139649090267008 learning.py:507] global step 382: loss = 0.5568 (3.095 sec/step)\n",
            "I0619 18:07:26.435415 139649090267008 learning.py:507] global step 382: loss = 0.3342 (3.071 sec/step)\n",
            "I0619 18:07:29.571854 139649090267008 learning.py:507] global step 382: loss = 0.4477 (3.135 sec/step)\n",
            "I0619 18:07:33.040645 139649090267008 learning.py:507] global step 382: loss = 0.3899 (3.467 sec/step)\n",
            "I0619 18:07:36.094815 139649090267008 learning.py:507] global step 383: loss = 0.4324 (3.051 sec/step)\n",
            "I0619 18:07:39.172944 139649090267008 learning.py:507] global step 383: loss = 0.3468 (3.077 sec/step)\n",
            "I0619 18:07:42.198725 139649090267008 learning.py:507] global step 383: loss = 0.3599 (3.024 sec/step)\n",
            "I0619 18:07:45.287438 139649090267008 learning.py:507] global step 383: loss = 0.4245 (3.087 sec/step)\n",
            "I0619 18:07:48.418089 139649090267008 learning.py:507] global step 383: loss = 0.3704 (3.129 sec/step)\n",
            "I0619 18:07:51.512428 139649090267008 learning.py:507] global step 383: loss = 0.3325 (3.092 sec/step)\n",
            "I0619 18:07:54.835493 139649090267008 learning.py:507] global step 383: loss = 0.3271 (3.321 sec/step)\n",
            "I0619 18:07:57.904242 139649090267008 learning.py:507] global step 383: loss = 0.3619 (3.067 sec/step)\n",
            "I0619 18:08:00.948022 139649090267008 learning.py:507] global step 384: loss = 0.4457 (3.041 sec/step)\n",
            "I0619 18:08:04.024620 139649090267008 learning.py:507] global step 384: loss = 0.3664 (3.075 sec/step)\n",
            "I0619 18:08:07.116505 139649090267008 learning.py:507] global step 384: loss = 0.3567 (3.090 sec/step)\n",
            "I0619 18:08:10.722906 139649090267008 learning.py:507] global step 384: loss = 0.3676 (3.604 sec/step)\n",
            "I0619 18:08:13.912655 139649090267008 learning.py:507] global step 384: loss = 0.3832 (3.188 sec/step)\n",
            "I0619 18:08:16.980513 139649090267008 learning.py:507] global step 384: loss = 0.3926 (3.066 sec/step)\n",
            "I0619 18:08:20.111701 139649090267008 learning.py:507] global step 384: loss = 0.3527 (3.129 sec/step)\n",
            "I0619 18:08:25.398502 139649090267008 learning.py:507] global step 384: loss = 0.3595 (5.284 sec/step)\n",
            "I0619 18:08:27.185206 139646017689344 supervisor.py:1050] Recording summary at step 384.\n",
            "I0619 18:08:29.309424 139649090267008 learning.py:507] global step 385: loss = 0.3125 (3.905 sec/step)\n",
            "I0619 18:08:32.334542 139649090267008 learning.py:507] global step 385: loss = 0.4408 (3.023 sec/step)\n",
            "I0619 18:08:35.435167 139649090267008 learning.py:507] global step 385: loss = 0.4385 (3.099 sec/step)\n",
            "I0619 18:08:38.561304 139649090267008 learning.py:507] global step 385: loss = 0.3219 (3.124 sec/step)\n",
            "I0619 18:08:41.596483 139649090267008 learning.py:507] global step 385: loss = 0.3652 (3.033 sec/step)\n",
            "I0619 18:08:44.866046 139649090267008 learning.py:507] global step 385: loss = 0.4451 (3.267 sec/step)\n",
            "I0619 18:08:47.865548 139649090267008 learning.py:507] global step 385: loss = 0.3468 (2.998 sec/step)\n",
            "I0619 18:08:50.916733 139649090267008 learning.py:507] global step 385: loss = 0.3198 (3.049 sec/step)\n",
            "I0619 18:08:53.999655 139649090267008 learning.py:507] global step 386: loss = 0.3879 (3.080 sec/step)\n",
            "I0619 18:08:57.141750 139649090267008 learning.py:507] global step 386: loss = 0.3348 (3.140 sec/step)\n",
            "I0619 18:09:00.194129 139649090267008 learning.py:507] global step 386: loss = 0.6621 (3.050 sec/step)\n",
            "I0619 18:09:03.655644 139649090267008 learning.py:507] global step 386: loss = 0.3205 (3.460 sec/step)\n",
            "I0619 18:09:06.767281 139649090267008 learning.py:507] global step 386: loss = 0.3680 (3.110 sec/step)\n",
            "I0619 18:09:09.898623 139649090267008 learning.py:507] global step 386: loss = 0.3907 (3.130 sec/step)\n",
            "I0619 18:09:12.923072 139649090267008 learning.py:507] global step 386: loss = 0.4096 (3.023 sec/step)\n",
            "I0619 18:09:15.939328 139649090267008 learning.py:507] global step 386: loss = 0.4484 (3.014 sec/step)\n",
            "I0619 18:09:19.038875 139649090267008 learning.py:507] global step 387: loss = 0.3719 (3.098 sec/step)\n",
            "I0619 18:09:22.136773 139649090267008 learning.py:507] global step 387: loss = 0.4164 (3.095 sec/step)\n",
            "I0619 18:09:25.243044 139649090267008 learning.py:507] global step 387: loss = 0.4256 (3.104 sec/step)\n",
            "I0619 18:09:28.320450 139649090267008 learning.py:507] global step 387: loss = 0.4296 (3.076 sec/step)\n",
            "I0619 18:09:31.412254 139649090267008 learning.py:507] global step 387: loss = 0.5543 (3.090 sec/step)\n",
            "I0619 18:09:34.483132 139649090267008 learning.py:507] global step 387: loss = 0.3636 (3.069 sec/step)\n",
            "I0619 18:09:37.645246 139649090267008 learning.py:507] global step 387: loss = 0.3931 (3.160 sec/step)\n",
            "I0619 18:09:40.673443 139649090267008 learning.py:507] global step 387: loss = 0.3584 (3.026 sec/step)\n",
            "I0619 18:09:43.788462 139649090267008 learning.py:507] global step 388: loss = 0.5117 (3.113 sec/step)\n",
            "I0619 18:09:46.900632 139649090267008 learning.py:507] global step 388: loss = 0.3367 (3.110 sec/step)\n",
            "I0619 18:09:49.928746 139649090267008 learning.py:507] global step 388: loss = 0.3590 (3.026 sec/step)\n",
            "I0619 18:09:52.988340 139649090267008 learning.py:507] global step 388: loss = 0.3388 (3.058 sec/step)\n",
            "I0619 18:09:56.293064 139649090267008 learning.py:507] global step 388: loss = 0.3660 (3.303 sec/step)\n",
            "I0619 18:09:59.348629 139649090267008 learning.py:507] global step 388: loss = 0.4331 (3.054 sec/step)\n",
            "I0619 18:10:02.409039 139649090267008 learning.py:507] global step 388: loss = 0.4254 (3.058 sec/step)\n",
            "I0619 18:10:05.463087 139649090267008 learning.py:507] global step 388: loss = 0.4148 (3.052 sec/step)\n",
            "I0619 18:10:08.580290 139649090267008 learning.py:507] global step 389: loss = 0.4017 (3.115 sec/step)\n",
            "I0619 18:10:11.627321 139649090267008 learning.py:507] global step 389: loss = 0.5024 (3.045 sec/step)\n",
            "I0619 18:10:14.796273 139649090267008 learning.py:507] global step 389: loss = 0.3275 (3.167 sec/step)\n",
            "I0619 18:10:17.825165 139649090267008 learning.py:507] global step 389: loss = 0.3225 (3.027 sec/step)\n",
            "I0619 18:10:20.885032 139649090267008 learning.py:507] global step 389: loss = 0.3316 (3.056 sec/step)\n",
            "I0619 18:10:20.931265 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:10:27.668392 139649090267008 learning.py:507] global step 389: loss = 0.3867 (6.780 sec/step)\n",
            "I0619 18:10:28.414374 139646017689344 supervisor.py:1050] Recording summary at step 389.\n",
            "I0619 18:10:30.899636 139649090267008 learning.py:507] global step 389: loss = 0.3073 (3.227 sec/step)\n",
            "I0619 18:10:33.976761 139649090267008 learning.py:507] global step 389: loss = 0.3973 (3.075 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:10:37.140276 139649090267008 learning.py:507] global step 390: loss = 0.4350 (3.161 sec/step)\n",
            "I0619 18:10:40.309735 139649090267008 learning.py:507] global step 390: loss = 0.3416 (3.168 sec/step)\n",
            "I0619 18:10:43.427592 139649090267008 learning.py:507] global step 390: loss = 0.3426 (3.116 sec/step)\n",
            "I0619 18:10:46.525827 139649090267008 learning.py:507] global step 390: loss = 0.3413 (3.096 sec/step)\n",
            "I0619 18:10:49.640022 139649090267008 learning.py:507] global step 390: loss = 0.4023 (3.113 sec/step)\n",
            "I0619 18:10:53.164459 139649090267008 learning.py:507] global step 390: loss = 0.3918 (3.523 sec/step)\n",
            "I0619 18:10:56.305642 139649090267008 learning.py:507] global step 390: loss = 0.4733 (3.139 sec/step)\n",
            "I0619 18:10:59.407712 139649090267008 learning.py:507] global step 390: loss = 0.5163 (3.100 sec/step)\n",
            "I0619 18:11:02.509360 139649090267008 learning.py:507] global step 391: loss = 0.3895 (3.099 sec/step)\n",
            "I0619 18:11:05.537156 139649090267008 learning.py:507] global step 391: loss = 0.3377 (3.026 sec/step)\n",
            "I0619 18:11:08.641139 139649090267008 learning.py:507] global step 391: loss = 0.3647 (3.102 sec/step)\n",
            "I0619 18:11:12.217986 139649090267008 learning.py:507] global step 391: loss = 0.3398 (3.575 sec/step)\n",
            "I0619 18:11:15.278984 139649090267008 learning.py:507] global step 391: loss = 0.3916 (3.059 sec/step)\n",
            "I0619 18:11:18.619870 139649090267008 learning.py:507] global step 391: loss = 0.3560 (3.339 sec/step)\n",
            "I0619 18:11:21.759155 139649090267008 learning.py:507] global step 391: loss = 0.3639 (3.138 sec/step)\n",
            "I0619 18:11:24.989683 139649090267008 learning.py:507] global step 391: loss = 0.4629 (3.229 sec/step)\n",
            "I0619 18:11:28.128536 139649090267008 learning.py:507] global step 392: loss = 0.3912 (3.137 sec/step)\n",
            "I0619 18:11:31.228815 139649090267008 learning.py:507] global step 392: loss = 0.3845 (3.099 sec/step)\n",
            "I0619 18:11:34.366684 139649090267008 learning.py:507] global step 392: loss = 0.3572 (3.136 sec/step)\n",
            "I0619 18:11:37.690952 139649090267008 learning.py:507] global step 392: loss = 0.3107 (3.322 sec/step)\n",
            "I0619 18:11:40.805012 139649090267008 learning.py:507] global step 392: loss = 0.3455 (3.112 sec/step)\n",
            "I0619 18:11:44.065502 139649090267008 learning.py:507] global step 392: loss = 0.3720 (3.259 sec/step)\n",
            "I0619 18:11:47.171260 139649090267008 learning.py:507] global step 392: loss = 0.3317 (3.104 sec/step)\n",
            "I0619 18:11:50.413418 139649090267008 learning.py:507] global step 392: loss = 0.3955 (3.240 sec/step)\n",
            "I0619 18:11:53.512329 139649090267008 learning.py:507] global step 393: loss = 0.4984 (3.097 sec/step)\n",
            "I0619 18:11:56.557461 139649090267008 learning.py:507] global step 393: loss = 0.3106 (3.043 sec/step)\n",
            "I0619 18:11:59.635037 139649090267008 learning.py:507] global step 393: loss = 0.3706 (3.075 sec/step)\n",
            "I0619 18:12:02.749701 139649090267008 learning.py:507] global step 393: loss = 0.3483 (3.113 sec/step)\n",
            "I0619 18:12:05.809702 139649090267008 learning.py:507] global step 393: loss = 0.4795 (3.058 sec/step)\n",
            "I0619 18:12:09.089803 139649090267008 learning.py:507] global step 393: loss = 0.4219 (3.278 sec/step)\n",
            "I0619 18:12:12.143472 139649090267008 learning.py:507] global step 393: loss = 0.4235 (3.052 sec/step)\n",
            "I0619 18:12:15.271512 139649090267008 learning.py:507] global step 393: loss = 0.3285 (3.126 sec/step)\n",
            "I0619 18:12:18.401098 139649090267008 learning.py:507] global step 394: loss = 0.4051 (3.128 sec/step)\n",
            "I0619 18:12:21.686708 139649090267008 learning.py:507] global step 394: loss = 0.3631 (3.036 sec/step)\n",
            "I0619 18:12:26.246257 139646017689344 supervisor.py:1050] Recording summary at step 394.\n",
            "I0619 18:12:26.858158 139649090267008 learning.py:507] global step 394: loss = 0.4482 (5.045 sec/step)\n",
            "I0619 18:12:29.942321 139649090267008 learning.py:507] global step 394: loss = 0.3767 (3.082 sec/step)\n",
            "I0619 18:12:32.985168 139649090267008 learning.py:507] global step 394: loss = 0.3481 (3.041 sec/step)\n",
            "I0619 18:12:36.203063 139649090267008 learning.py:507] global step 394: loss = 0.4137 (3.216 sec/step)\n",
            "I0619 18:12:39.255275 139649090267008 learning.py:507] global step 394: loss = 0.3414 (3.050 sec/step)\n",
            "I0619 18:12:42.342875 139649090267008 learning.py:507] global step 394: loss = 0.3657 (3.086 sec/step)\n",
            "I0619 18:12:45.415608 139649090267008 learning.py:507] global step 395: loss = 0.3579 (3.070 sec/step)\n",
            "I0619 18:12:48.458039 139649090267008 learning.py:507] global step 395: loss = 0.4379 (3.041 sec/step)\n",
            "I0619 18:12:51.524883 139649090267008 learning.py:507] global step 395: loss = 0.3574 (3.065 sec/step)\n",
            "I0619 18:12:54.660663 139649090267008 learning.py:507] global step 395: loss = 0.4749 (3.134 sec/step)\n",
            "I0619 18:12:57.827277 139649090267008 learning.py:507] global step 395: loss = 0.3732 (3.165 sec/step)\n",
            "I0619 18:13:00.910549 139649090267008 learning.py:507] global step 395: loss = 0.3641 (3.081 sec/step)\n",
            "I0619 18:13:04.025185 139649090267008 learning.py:507] global step 395: loss = 0.4119 (3.113 sec/step)\n",
            "I0619 18:13:07.086649 139649090267008 learning.py:507] global step 395: loss = 0.3213 (3.060 sec/step)\n",
            "I0619 18:13:10.303504 139649090267008 learning.py:507] global step 396: loss = 0.3218 (3.214 sec/step)\n",
            "I0619 18:13:13.350502 139649090267008 learning.py:507] global step 396: loss = 0.5020 (3.045 sec/step)\n",
            "I0619 18:13:16.435699 139649090267008 learning.py:507] global step 396: loss = 0.3155 (3.083 sec/step)\n",
            "I0619 18:13:19.469935 139649090267008 learning.py:507] global step 396: loss = 0.3513 (3.032 sec/step)\n",
            "I0619 18:13:22.544480 139649090267008 learning.py:507] global step 396: loss = 0.3346 (3.073 sec/step)\n",
            "I0619 18:13:25.640200 139649090267008 learning.py:507] global step 396: loss = 0.4772 (3.094 sec/step)\n",
            "I0619 18:13:28.755396 139649090267008 learning.py:507] global step 396: loss = 0.3205 (3.113 sec/step)\n",
            "I0619 18:13:31.871250 139649090267008 learning.py:507] global step 396: loss = 0.3764 (3.114 sec/step)\n",
            "I0619 18:13:34.973396 139649090267008 learning.py:507] global step 397: loss = 0.4718 (3.100 sec/step)\n",
            "I0619 18:13:38.061852 139649090267008 learning.py:507] global step 397: loss = 0.3850 (3.087 sec/step)\n",
            "I0619 18:13:41.085379 139649090267008 learning.py:507] global step 397: loss = 0.4142 (3.022 sec/step)\n",
            "I0619 18:13:44.080781 139649090267008 learning.py:507] global step 397: loss = 0.3346 (2.993 sec/step)\n",
            "I0619 18:13:47.129011 139649090267008 learning.py:507] global step 397: loss = 0.3490 (3.046 sec/step)\n",
            "I0619 18:13:50.120842 139649090267008 learning.py:507] global step 397: loss = 0.3139 (2.990 sec/step)\n",
            "I0619 18:13:53.116651 139649090267008 learning.py:507] global step 397: loss = 0.2937 (2.994 sec/step)\n",
            "I0619 18:13:56.142927 139649090267008 learning.py:507] global step 397: loss = 0.4394 (3.024 sec/step)\n",
            "I0619 18:13:59.247352 139649090267008 learning.py:507] global step 398: loss = 0.3763 (3.102 sec/step)\n",
            "I0619 18:14:02.250787 139649090267008 learning.py:507] global step 398: loss = 0.3076 (3.002 sec/step)\n",
            "I0619 18:14:05.286820 139649090267008 learning.py:507] global step 398: loss = 0.3589 (3.034 sec/step)\n",
            "I0619 18:14:08.338158 139649090267008 learning.py:507] global step 398: loss = 0.3850 (3.050 sec/step)\n",
            "I0619 18:14:11.465000 139649090267008 learning.py:507] global step 398: loss = 0.3808 (3.125 sec/step)\n",
            "I0619 18:14:14.451548 139649090267008 learning.py:507] global step 398: loss = 0.5215 (2.985 sec/step)\n",
            "I0619 18:14:17.555744 139649090267008 learning.py:507] global step 398: loss = 0.3020 (3.103 sec/step)\n",
            "I0619 18:14:20.584066 139649090267008 learning.py:507] global step 398: loss = 0.3304 (3.027 sec/step)\n",
            "I0619 18:14:25.839775 139649090267008 learning.py:507] global step 399: loss = 0.2909 (5.251 sec/step)\n",
            "I0619 18:14:26.723327 139646017689344 supervisor.py:1050] Recording summary at step 399.\n",
            "I0619 18:14:28.985033 139649090267008 learning.py:507] global step 399: loss = 0.3811 (3.142 sec/step)\n",
            "I0619 18:14:32.111270 139649090267008 learning.py:507] global step 399: loss = 0.3796 (3.124 sec/step)\n",
            "I0619 18:14:35.149760 139649090267008 learning.py:507] global step 399: loss = 0.3373 (3.037 sec/step)\n",
            "I0619 18:14:38.195163 139649090267008 learning.py:507] global step 399: loss = 0.3941 (3.044 sec/step)\n",
            "I0619 18:14:41.333179 139649090267008 learning.py:507] global step 399: loss = 0.4830 (3.136 sec/step)\n",
            "I0619 18:14:44.428440 139649090267008 learning.py:507] global step 399: loss = 0.3026 (3.093 sec/step)\n",
            "I0619 18:14:47.412261 139649090267008 learning.py:507] global step 399: loss = 0.3887 (2.982 sec/step)\n",
            "I0619 18:14:50.631520 139649090267008 learning.py:507] global step 400: loss = 0.3731 (3.217 sec/step)\n",
            "I0619 18:14:54.213454 139649090267008 learning.py:507] global step 400: loss = 0.4096 (3.580 sec/step)\n",
            "I0619 18:14:57.303438 139649090267008 learning.py:507] global step 400: loss = 0.3428 (3.088 sec/step)\n",
            "I0619 18:15:01.127666 139649090267008 learning.py:507] global step 400: loss = 0.3894 (3.822 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:15:04.231872 139649090267008 learning.py:507] global step 400: loss = 0.4853 (3.102 sec/step)\n",
            "I0619 18:15:07.330080 139649090267008 learning.py:507] global step 400: loss = 0.4120 (3.097 sec/step)\n",
            "I0619 18:15:10.375736 139649090267008 learning.py:507] global step 400: loss = 0.5545 (3.044 sec/step)\n",
            "I0619 18:15:14.834999 139649090267008 learning.py:507] global step 400: loss = 0.3339 (4.457 sec/step)\n",
            "I0619 18:15:17.972007 139649090267008 learning.py:507] global step 401: loss = 0.3390 (3.135 sec/step)\n",
            "I0619 18:15:21.544196 139649090267008 learning.py:507] global step 401: loss = 0.3166 (3.570 sec/step)\n",
            "I0619 18:15:24.532034 139649090267008 learning.py:507] global step 401: loss = 0.3417 (2.986 sec/step)\n",
            "I0619 18:15:27.534185 139649090267008 learning.py:507] global step 401: loss = 0.3404 (3.000 sec/step)\n",
            "I0619 18:15:30.847309 139649090267008 learning.py:507] global step 401: loss = 0.4133 (3.311 sec/step)\n",
            "I0619 18:15:34.279649 139649090267008 learning.py:507] global step 401: loss = 0.3810 (3.431 sec/step)\n",
            "I0619 18:15:37.261069 139649090267008 learning.py:507] global step 401: loss = 0.3895 (2.980 sec/step)\n",
            "I0619 18:15:40.319507 139649090267008 learning.py:507] global step 401: loss = 0.4218 (3.057 sec/step)\n",
            "I0619 18:15:43.397358 139649090267008 learning.py:507] global step 402: loss = 0.4798 (3.076 sec/step)\n",
            "I0619 18:15:46.350324 139649090267008 learning.py:507] global step 402: loss = 0.3861 (2.951 sec/step)\n",
            "I0619 18:15:49.609216 139649090267008 learning.py:507] global step 402: loss = 0.4729 (3.257 sec/step)\n",
            "I0619 18:15:52.706883 139649090267008 learning.py:507] global step 402: loss = 0.4427 (3.096 sec/step)\n",
            "I0619 18:15:55.753118 139649090267008 learning.py:507] global step 402: loss = 0.3631 (3.044 sec/step)\n",
            "I0619 18:15:58.770549 139649090267008 learning.py:507] global step 402: loss = 0.3879 (3.016 sec/step)\n",
            "I0619 18:16:01.854139 139649090267008 learning.py:507] global step 402: loss = 0.3297 (3.082 sec/step)\n",
            "I0619 18:16:04.940917 139649090267008 learning.py:507] global step 402: loss = 0.4092 (3.085 sec/step)\n",
            "I0619 18:16:07.922192 139649090267008 learning.py:507] global step 403: loss = 0.3528 (2.978 sec/step)\n",
            "I0619 18:16:10.964090 139649090267008 learning.py:507] global step 403: loss = 0.3430 (3.040 sec/step)\n",
            "I0619 18:16:14.030524 139649090267008 learning.py:507] global step 403: loss = 0.4177 (3.065 sec/step)\n",
            "I0619 18:16:17.033874 139649090267008 learning.py:507] global step 403: loss = 0.3351 (3.001 sec/step)\n",
            "I0619 18:16:20.024731 139649090267008 learning.py:507] global step 403: loss = 0.3430 (2.989 sec/step)\n",
            "I0619 18:16:25.263638 139649090267008 learning.py:507] global step 403: loss = 0.3073 (5.232 sec/step)\n",
            "I0619 18:16:26.456520 139646017689344 supervisor.py:1050] Recording summary at step 403.\n",
            "I0619 18:16:28.504978 139649090267008 learning.py:507] global step 403: loss = 0.3181 (3.239 sec/step)\n",
            "I0619 18:16:31.522083 139649090267008 learning.py:507] global step 403: loss = 0.3904 (3.015 sec/step)\n",
            "I0619 18:16:34.555534 139649090267008 learning.py:507] global step 404: loss = 0.3384 (3.031 sec/step)\n",
            "I0619 18:16:37.578631 139649090267008 learning.py:507] global step 404: loss = 0.4531 (3.022 sec/step)\n",
            "I0619 18:16:40.588006 139649090267008 learning.py:507] global step 404: loss = 0.3396 (3.008 sec/step)\n",
            "I0619 18:16:43.591260 139649090267008 learning.py:507] global step 404: loss = 0.3163 (3.002 sec/step)\n",
            "I0619 18:16:46.619324 139649090267008 learning.py:507] global step 404: loss = 0.3111 (3.027 sec/step)\n",
            "I0619 18:16:49.655523 139649090267008 learning.py:507] global step 404: loss = 0.3800 (3.034 sec/step)\n",
            "I0619 18:16:52.685233 139649090267008 learning.py:507] global step 404: loss = 0.4092 (3.028 sec/step)\n",
            "I0619 18:16:55.719996 139649090267008 learning.py:507] global step 404: loss = 0.3484 (3.033 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:16:58.762380 139649090267008 learning.py:507] global step 405: loss = 0.3638 (3.040 sec/step)\n",
            "I0619 18:17:01.737844 139649090267008 learning.py:507] global step 405: loss = 0.3966 (2.974 sec/step)\n",
            "I0619 18:17:04.788238 139649090267008 learning.py:507] global step 405: loss = 0.3379 (3.049 sec/step)\n",
            "I0619 18:17:07.773063 139649090267008 learning.py:507] global step 405: loss = 0.3926 (2.983 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:17:10.773909 139649090267008 learning.py:507] global step 405: loss = 0.3671 (2.999 sec/step)\n",
            "I0619 18:17:13.832741 139649090267008 learning.py:507] global step 405: loss = 0.4130 (3.057 sec/step)\n",
            "I0619 18:17:16.913907 139649090267008 learning.py:507] global step 405: loss = 0.4515 (3.079 sec/step)\n",
            "I0619 18:17:19.924694 139649090267008 learning.py:507] global step 405: loss = 0.3630 (3.009 sec/step)\n",
            "I0619 18:17:23.098477 139649090267008 learning.py:507] global step 406: loss = 0.3797 (3.172 sec/step)\n",
            "I0619 18:17:26.087529 139649090267008 learning.py:507] global step 406: loss = 0.3392 (2.987 sec/step)\n",
            "I0619 18:17:29.123905 139649090267008 learning.py:507] global step 406: loss = 0.3201 (3.034 sec/step)\n",
            "I0619 18:17:32.143093 139649090267008 learning.py:507] global step 406: loss = 0.3104 (3.017 sec/step)\n",
            "I0619 18:17:35.186174 139649090267008 learning.py:507] global step 406: loss = 0.3215 (3.041 sec/step)\n",
            "I0619 18:17:38.399427 139649090267008 learning.py:507] global step 406: loss = 0.3361 (3.211 sec/step)\n",
            "I0619 18:17:41.396354 139649090267008 learning.py:507] global step 406: loss = 0.3671 (2.995 sec/step)\n",
            "I0619 18:17:44.366697 139649090267008 learning.py:507] global step 406: loss = 0.4040 (2.969 sec/step)\n",
            "I0619 18:17:47.378550 139649090267008 learning.py:507] global step 407: loss = 0.2936 (3.009 sec/step)\n",
            "I0619 18:17:50.351131 139649090267008 learning.py:507] global step 407: loss = 0.3003 (2.971 sec/step)\n",
            "I0619 18:17:53.305613 139649090267008 learning.py:507] global step 407: loss = 0.3368 (2.952 sec/step)\n",
            "I0619 18:17:56.572112 139649090267008 learning.py:507] global step 407: loss = 0.3346 (3.265 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:17:59.540794 139649090267008 learning.py:507] global step 407: loss = 0.4537 (2.967 sec/step)\n",
            "I0619 18:18:02.518903 139649090267008 learning.py:507] global step 407: loss = 0.3676 (2.976 sec/step)\n",
            "I0619 18:18:05.498971 139649090267008 learning.py:507] global step 407: loss = 0.3279 (2.978 sec/step)\n",
            "I0619 18:18:08.532171 139649090267008 learning.py:507] global step 407: loss = 0.3185 (3.031 sec/step)\n",
            "I0619 18:18:11.556834 139649090267008 learning.py:507] global step 408: loss = 0.3273 (3.022 sec/step)\n",
            "I0619 18:18:14.546090 139649090267008 learning.py:507] global step 408: loss = 0.3494 (2.988 sec/step)\n",
            "I0619 18:18:17.554173 139649090267008 learning.py:507] global step 408: loss = 0.3694 (3.006 sec/step)\n",
            "I0619 18:18:20.530596 139649090267008 learning.py:507] global step 408: loss = 0.3818 (2.974 sec/step)\n",
            "I0619 18:18:25.822763 139649090267008 learning.py:507] global step 408: loss = 0.4915 (5.289 sec/step)\n",
            "I0619 18:18:25.989748 139646017689344 supervisor.py:1050] Recording summary at step 408.\n",
            "I0619 18:18:28.828986 139649090267008 learning.py:507] global step 408: loss = 0.3722 (3.004 sec/step)\n",
            "I0619 18:18:31.889002 139649090267008 learning.py:507] global step 408: loss = 0.3172 (3.058 sec/step)\n",
            "I0619 18:18:34.919029 139649090267008 learning.py:507] global step 408: loss = 0.3795 (3.028 sec/step)\n",
            "I0619 18:18:37.930617 139649090267008 learning.py:507] global step 409: loss = 0.3688 (3.009 sec/step)\n",
            "I0619 18:18:40.996129 139649090267008 learning.py:507] global step 409: loss = 0.3753 (3.064 sec/step)\n",
            "I0619 18:18:44.022922 139649090267008 learning.py:507] global step 409: loss = 0.4994 (3.025 sec/step)\n",
            "I0619 18:18:47.051079 139649090267008 learning.py:507] global step 409: loss = 0.3008 (3.027 sec/step)\n",
            "I0619 18:18:50.060083 139649090267008 learning.py:507] global step 409: loss = 0.4226 (3.007 sec/step)\n",
            "I0619 18:18:53.078154 139649090267008 learning.py:507] global step 409: loss = 0.4065 (3.016 sec/step)\n",
            "I0619 18:18:56.137000 139649090267008 learning.py:507] global step 409: loss = 0.3584 (3.057 sec/step)\n",
            "I0619 18:18:59.145484 139649090267008 learning.py:507] global step 409: loss = 0.4023 (3.007 sec/step)\n",
            "I0619 18:19:02.175591 139649090267008 learning.py:507] global step 410: loss = 0.3013 (3.027 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:19:05.166773 139649090267008 learning.py:507] global step 410: loss = 0.4214 (2.989 sec/step)\n",
            "I0619 18:19:08.193140 139649090267008 learning.py:507] global step 410: loss = 0.3466 (3.025 sec/step)\n",
            "I0619 18:19:11.224915 139649090267008 learning.py:507] global step 410: loss = 0.3094 (3.030 sec/step)\n",
            "I0619 18:19:14.242260 139649090267008 learning.py:507] global step 410: loss = 0.4270 (3.016 sec/step)\n",
            "I0619 18:19:17.230228 139649090267008 learning.py:507] global step 410: loss = 0.3052 (2.986 sec/step)\n",
            "I0619 18:19:20.288120 139649090267008 learning.py:507] global step 410: loss = 0.3702 (3.056 sec/step)\n",
            "I0619 18:19:23.274907 139649090267008 learning.py:507] global step 410: loss = 0.3464 (2.985 sec/step)\n",
            "I0619 18:19:26.301252 139649090267008 learning.py:507] global step 411: loss = 0.4170 (3.024 sec/step)\n",
            "I0619 18:19:29.287099 139649090267008 learning.py:507] global step 411: loss = 0.3713 (2.984 sec/step)\n",
            "I0619 18:19:32.369597 139649090267008 learning.py:507] global step 411: loss = 0.3614 (3.081 sec/step)\n",
            "I0619 18:19:35.371871 139649090267008 learning.py:507] global step 411: loss = 0.3806 (3.000 sec/step)\n",
            "I0619 18:19:38.338912 139649090267008 learning.py:507] global step 411: loss = 0.3306 (2.965 sec/step)\n",
            "I0619 18:19:41.392440 139649090267008 learning.py:507] global step 411: loss = 0.3253 (3.052 sec/step)\n",
            "I0619 18:19:44.358577 139649090267008 learning.py:507] global step 411: loss = 0.3271 (2.965 sec/step)\n",
            "I0619 18:19:47.341156 139649090267008 learning.py:507] global step 411: loss = 0.4041 (2.981 sec/step)\n",
            "I0619 18:19:50.308121 139649090267008 learning.py:507] global step 412: loss = 0.3114 (2.965 sec/step)\n",
            "I0619 18:19:53.334809 139649090267008 learning.py:507] global step 412: loss = 0.4169 (3.025 sec/step)\n",
            "I0619 18:19:56.301068 139649090267008 learning.py:507] global step 412: loss = 0.3457 (2.965 sec/step)\n",
            "I0619 18:19:59.315052 139649090267008 learning.py:507] global step 412: loss = 0.4533 (3.012 sec/step)\n",
            "I0619 18:20:02.342092 139649090267008 learning.py:507] global step 412: loss = 0.4604 (3.025 sec/step)\n",
            "I0619 18:20:05.364100 139649090267008 learning.py:507] global step 412: loss = 0.3299 (3.019 sec/step)\n",
            "I0619 18:20:08.366473 139649090267008 learning.py:507] global step 412: loss = 0.5110 (3.001 sec/step)\n",
            "I0619 18:20:11.429859 139649090267008 learning.py:507] global step 412: loss = 0.3816 (3.062 sec/step)\n",
            "I0619 18:20:14.443385 139649090267008 learning.py:507] global step 413: loss = 0.4219 (3.011 sec/step)\n",
            "I0619 18:20:17.395731 139649090267008 learning.py:507] global step 413: loss = 0.3622 (2.951 sec/step)\n",
            "I0619 18:20:20.409930 139649090267008 learning.py:507] global step 413: loss = 0.3337 (3.012 sec/step)\n",
            "I0619 18:20:20.931470 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:20:27.127418 139649090267008 learning.py:507] global step 413: loss = 0.4169 (6.715 sec/step)\n",
            "I0619 18:20:28.282238 139646017689344 supervisor.py:1050] Recording summary at step 413.\n",
            "I0619 18:20:30.257386 139649090267008 learning.py:507] global step 413: loss = 0.3410 (3.126 sec/step)\n",
            "I0619 18:20:33.286901 139649090267008 learning.py:507] global step 413: loss = 0.3456 (3.028 sec/step)\n",
            "I0619 18:20:36.252709 139649090267008 learning.py:507] global step 413: loss = 0.3359 (2.964 sec/step)\n",
            "I0619 18:20:39.216392 139649090267008 learning.py:507] global step 413: loss = 0.3108 (2.962 sec/step)\n",
            "I0619 18:20:42.402595 139649090267008 learning.py:507] global step 414: loss = 0.4252 (3.184 sec/step)\n",
            "I0619 18:20:45.335031 139649090267008 learning.py:507] global step 414: loss = 0.4018 (2.931 sec/step)\n",
            "I0619 18:20:48.325957 139649090267008 learning.py:507] global step 414: loss = 0.3289 (2.989 sec/step)\n",
            "I0619 18:20:51.299457 139649090267008 learning.py:507] global step 414: loss = 0.3460 (2.972 sec/step)\n",
            "I0619 18:20:54.360536 139649090267008 learning.py:507] global step 414: loss = 0.3355 (3.059 sec/step)\n",
            "I0619 18:20:57.409034 139649090267008 learning.py:507] global step 414: loss = 0.5036 (3.045 sec/step)\n",
            "I0619 18:21:00.428727 139649090267008 learning.py:507] global step 414: loss = 0.3812 (3.018 sec/step)\n",
            "I0619 18:21:03.448699 139649090267008 learning.py:507] global step 414: loss = 0.3412 (3.018 sec/step)\n",
            "I0619 18:21:06.475231 139649090267008 learning.py:507] global step 415: loss = 0.3176 (3.024 sec/step)\n",
            "I0619 18:21:10.608265 139649090267008 learning.py:507] global step 415: loss = 0.4010 (4.131 sec/step)\n",
            "I0619 18:21:13.612030 139649090267008 learning.py:507] global step 415: loss = 0.3560 (3.002 sec/step)\n",
            "I0619 18:21:16.734863 139649090267008 learning.py:507] global step 415: loss = 0.3241 (3.121 sec/step)\n",
            "I0619 18:21:19.751060 139649090267008 learning.py:507] global step 415: loss = 0.4016 (3.014 sec/step)\n",
            "I0619 18:21:22.739934 139649090267008 learning.py:507] global step 415: loss = 0.3385 (2.987 sec/step)\n",
            "I0619 18:21:25.765751 139649090267008 learning.py:507] global step 415: loss = 0.3469 (3.024 sec/step)\n",
            "I0619 18:21:29.237879 139649090267008 learning.py:507] global step 415: loss = 0.3222 (3.470 sec/step)\n",
            "I0619 18:21:32.194530 139649090267008 learning.py:507] global step 416: loss = 0.3930 (2.954 sec/step)\n",
            "I0619 18:21:35.224166 139649090267008 learning.py:507] global step 416: loss = 0.3798 (3.028 sec/step)\n",
            "I0619 18:21:38.280088 139649090267008 learning.py:507] global step 416: loss = 0.3734 (3.054 sec/step)\n",
            "I0619 18:21:41.280880 139649090267008 learning.py:507] global step 416: loss = 0.5088 (2.999 sec/step)\n",
            "I0619 18:21:44.284421 139649090267008 learning.py:507] global step 416: loss = 0.3357 (3.002 sec/step)\n",
            "I0619 18:21:47.289880 139649090267008 learning.py:507] global step 416: loss = 0.2959 (3.004 sec/step)\n",
            "I0619 18:21:50.242929 139649090267008 learning.py:507] global step 416: loss = 0.5191 (2.952 sec/step)\n",
            "I0619 18:21:53.262675 139649090267008 learning.py:507] global step 416: loss = 0.3889 (3.018 sec/step)\n",
            "I0619 18:21:56.320626 139649090267008 learning.py:507] global step 417: loss = 0.4375 (3.055 sec/step)\n",
            "I0619 18:21:59.294957 139649090267008 learning.py:507] global step 417: loss = 0.3681 (2.972 sec/step)\n",
            "I0619 18:22:02.246478 139649090267008 learning.py:507] global step 417: loss = 0.3400 (2.949 sec/step)\n",
            "I0619 18:22:05.232184 139649090267008 learning.py:507] global step 417: loss = 0.4284 (2.984 sec/step)\n",
            "I0619 18:22:08.194363 139649090267008 learning.py:507] global step 417: loss = 0.3518 (2.960 sec/step)\n",
            "I0619 18:22:11.139800 139649090267008 learning.py:507] global step 417: loss = 0.4233 (2.944 sec/step)\n",
            "I0619 18:22:14.154297 139649090267008 learning.py:507] global step 417: loss = 0.3780 (3.013 sec/step)\n",
            "I0619 18:22:17.151502 139649090267008 learning.py:507] global step 417: loss = 0.3043 (2.995 sec/step)\n",
            "I0619 18:22:20.137536 139649090267008 learning.py:507] global step 418: loss = 0.3151 (2.984 sec/step)\n",
            "I0619 18:22:25.255531 139649090267008 learning.py:507] global step 418: loss = 0.4089 (5.113 sec/step)\n",
            "I0619 18:22:26.068036 139646017689344 supervisor.py:1050] Recording summary at step 418.\n",
            "I0619 18:22:28.442063 139649090267008 learning.py:507] global step 418: loss = 0.3063 (3.183 sec/step)\n",
            "I0619 18:22:31.383400 139649090267008 learning.py:507] global step 418: loss = 0.3756 (2.940 sec/step)\n",
            "I0619 18:22:34.310791 139649090267008 learning.py:507] global step 418: loss = 0.4289 (2.926 sec/step)\n",
            "I0619 18:22:37.346472 139649090267008 learning.py:507] global step 418: loss = 0.4269 (3.034 sec/step)\n",
            "I0619 18:22:40.305233 139649090267008 learning.py:507] global step 418: loss = 0.3314 (2.957 sec/step)\n",
            "I0619 18:22:43.306952 139649090267008 learning.py:507] global step 418: loss = 0.3840 (3.000 sec/step)\n",
            "I0619 18:22:46.335371 139649090267008 learning.py:507] global step 419: loss = 0.3877 (3.026 sec/step)\n",
            "I0619 18:22:49.331058 139649090267008 learning.py:507] global step 419: loss = 0.4481 (2.994 sec/step)\n",
            "I0619 18:22:52.404142 139649090267008 learning.py:507] global step 419: loss = 0.3909 (3.072 sec/step)\n",
            "I0619 18:22:55.430354 139649090267008 learning.py:507] global step 419: loss = 0.3865 (3.024 sec/step)\n",
            "I0619 18:22:58.454219 139649090267008 learning.py:507] global step 419: loss = 0.3858 (3.022 sec/step)\n",
            "I0619 18:23:01.572323 139649090267008 learning.py:507] global step 419: loss = 0.7573 (3.116 sec/step)\n",
            "I0619 18:23:04.533715 139649090267008 learning.py:507] global step 419: loss = 0.3456 (2.959 sec/step)\n",
            "I0619 18:23:07.641830 139649090267008 learning.py:507] global step 419: loss = 0.3429 (3.106 sec/step)\n",
            "I0619 18:23:10.619248 139649090267008 learning.py:507] global step 420: loss = 0.4030 (2.975 sec/step)\n",
            "I0619 18:23:13.560260 139649090267008 learning.py:507] global step 420: loss = 0.5843 (2.939 sec/step)\n",
            "I0619 18:23:16.538563 139649090267008 learning.py:507] global step 420: loss = 0.3351 (2.977 sec/step)\n",
            "I0619 18:23:19.675208 139649090267008 learning.py:507] global step 420: loss = 0.3817 (3.135 sec/step)\n",
            "I0619 18:23:22.630686 139649090267008 learning.py:507] global step 420: loss = 0.4641 (2.954 sec/step)\n",
            "I0619 18:23:25.752915 139649090267008 learning.py:507] global step 420: loss = 0.3207 (3.121 sec/step)\n",
            "I0619 18:23:28.754476 139649090267008 learning.py:507] global step 420: loss = 0.3549 (3.000 sec/step)\n",
            "I0619 18:23:31.677905 139649090267008 learning.py:507] global step 420: loss = 0.3220 (2.922 sec/step)\n",
            "I0619 18:23:34.705258 139649090267008 learning.py:507] global step 421: loss = 0.3668 (3.025 sec/step)\n",
            "I0619 18:23:37.674378 139649090267008 learning.py:507] global step 421: loss = 0.3298 (2.967 sec/step)\n",
            "I0619 18:23:40.665093 139649090267008 learning.py:507] global step 421: loss = 0.3018 (2.989 sec/step)\n",
            "I0619 18:23:43.747027 139649090267008 learning.py:507] global step 421: loss = 0.4210 (3.080 sec/step)\n",
            "I0619 18:23:46.807212 139649090267008 learning.py:507] global step 421: loss = 0.3282 (3.058 sec/step)\n",
            "I0619 18:23:49.863484 139649090267008 learning.py:507] global step 421: loss = 0.3229 (3.054 sec/step)\n",
            "I0619 18:23:52.827674 139649090267008 learning.py:507] global step 421: loss = 0.4637 (2.963 sec/step)\n",
            "I0619 18:23:55.810993 139649090267008 learning.py:507] global step 421: loss = 0.3444 (2.982 sec/step)\n",
            "I0619 18:23:58.767886 139649090267008 learning.py:507] global step 422: loss = 0.4390 (2.955 sec/step)\n",
            "I0619 18:24:01.772303 139649090267008 learning.py:507] global step 422: loss = 0.3173 (3.002 sec/step)\n",
            "I0619 18:24:04.723812 139649090267008 learning.py:507] global step 422: loss = 0.3462 (2.950 sec/step)\n",
            "I0619 18:24:07.694332 139649090267008 learning.py:507] global step 422: loss = 0.2875 (2.969 sec/step)\n",
            "I0619 18:24:10.702282 139649090267008 learning.py:507] global step 422: loss = 0.3181 (3.006 sec/step)\n",
            "I0619 18:24:13.727494 139649090267008 learning.py:507] global step 422: loss = 0.3209 (3.023 sec/step)\n",
            "I0619 18:24:16.795833 139649090267008 learning.py:507] global step 422: loss = 0.4088 (3.066 sec/step)\n",
            "I0619 18:24:19.947749 139649090267008 learning.py:507] global step 422: loss = 0.3495 (3.150 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:24:25.064146 139649090267008 learning.py:507] global step 423: loss = 0.3429 (5.114 sec/step)\n",
            "I0619 18:24:26.643807 139646017689344 supervisor.py:1050] Recording summary at step 423.\n",
            "I0619 18:24:28.308877 139649090267008 learning.py:507] global step 423: loss = 0.3148 (3.239 sec/step)\n",
            "I0619 18:24:31.245034 139649090267008 learning.py:507] global step 423: loss = 0.3439 (2.935 sec/step)\n",
            "I0619 18:24:34.239476 139649090267008 learning.py:507] global step 423: loss = 0.4327 (2.993 sec/step)\n",
            "I0619 18:24:37.320033 139649090267008 learning.py:507] global step 423: loss = 0.3481 (3.079 sec/step)\n",
            "I0619 18:24:40.331188 139649090267008 learning.py:507] global step 423: loss = 0.3751 (3.009 sec/step)\n",
            "I0619 18:24:43.574384 139649090267008 learning.py:507] global step 423: loss = 0.3081 (3.240 sec/step)\n",
            "I0619 18:24:46.483494 139649090267008 learning.py:507] global step 423: loss = 0.3883 (2.907 sec/step)\n",
            "I0619 18:24:49.401440 139649090267008 learning.py:507] global step 424: loss = 0.4826 (2.915 sec/step)\n",
            "I0619 18:24:52.422891 139649090267008 learning.py:507] global step 424: loss = 0.3418 (3.020 sec/step)\n",
            "I0619 18:24:55.335513 139649090267008 learning.py:507] global step 424: loss = 0.3563 (2.911 sec/step)\n",
            "I0619 18:24:58.324535 139649090267008 learning.py:507] global step 424: loss = 0.3557 (2.987 sec/step)\n",
            "I0619 18:25:01.579297 139649090267008 learning.py:507] global step 424: loss = 0.3647 (3.253 sec/step)\n",
            "I0619 18:25:04.508719 139649090267008 learning.py:507] global step 424: loss = 0.3360 (2.928 sec/step)\n",
            "I0619 18:25:07.476002 139649090267008 learning.py:507] global step 424: loss = 0.3930 (2.966 sec/step)\n",
            "I0619 18:25:10.449334 139649090267008 learning.py:507] global step 424: loss = 0.2817 (2.971 sec/step)\n",
            "I0619 18:25:13.450193 139649090267008 learning.py:507] global step 425: loss = 0.3777 (2.998 sec/step)\n",
            "I0619 18:25:16.492305 139649090267008 learning.py:507] global step 425: loss = 0.3281 (3.040 sec/step)\n",
            "I0619 18:25:19.420013 139649090267008 learning.py:507] global step 425: loss = 0.3926 (2.926 sec/step)\n",
            "I0619 18:25:22.351048 139649090267008 learning.py:507] global step 425: loss = 0.3479 (2.928 sec/step)\n",
            "I0619 18:25:25.311522 139649090267008 learning.py:507] global step 425: loss = 0.3675 (2.958 sec/step)\n",
            "I0619 18:25:28.336628 139649090267008 learning.py:507] global step 425: loss = 0.3277 (3.023 sec/step)\n",
            "I0619 18:25:31.395722 139649090267008 learning.py:507] global step 425: loss = 0.3292 (3.057 sec/step)\n",
            "I0619 18:25:34.445145 139649090267008 learning.py:507] global step 425: loss = 0.3607 (3.048 sec/step)\n",
            "I0619 18:25:37.452602 139649090267008 learning.py:507] global step 426: loss = 0.3866 (3.004 sec/step)\n",
            "I0619 18:25:40.376327 139649090267008 learning.py:507] global step 426: loss = 0.4473 (2.922 sec/step)\n",
            "I0619 18:25:43.335262 139649090267008 learning.py:507] global step 426: loss = 0.3356 (2.957 sec/step)\n",
            "I0619 18:25:46.331995 139649090267008 learning.py:507] global step 426: loss = 0.5339 (2.995 sec/step)\n",
            "I0619 18:25:49.418810 139649090267008 learning.py:507] global step 426: loss = 0.3597 (3.085 sec/step)\n",
            "I0619 18:25:52.411642 139649090267008 learning.py:507] global step 426: loss = 0.3981 (2.991 sec/step)\n",
            "I0619 18:25:55.350183 139649090267008 learning.py:507] global step 426: loss = 0.2881 (2.937 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:25:58.309480 139649090267008 learning.py:507] global step 426: loss = 0.3419 (2.958 sec/step)\n",
            "I0619 18:26:01.250365 139649090267008 learning.py:507] global step 427: loss = 0.4426 (2.939 sec/step)\n",
            "I0619 18:26:04.192388 139649090267008 learning.py:507] global step 427: loss = 0.4209 (2.940 sec/step)\n",
            "I0619 18:26:07.212164 139649090267008 learning.py:507] global step 427: loss = 0.3921 (3.018 sec/step)\n",
            "I0619 18:26:10.231329 139649090267008 learning.py:507] global step 427: loss = 0.4708 (3.017 sec/step)\n",
            "I0619 18:26:13.247323 139649090267008 learning.py:507] global step 427: loss = 0.3381 (3.014 sec/step)\n",
            "I0619 18:26:16.237886 139649090267008 learning.py:507] global step 427: loss = 0.3482 (2.987 sec/step)\n",
            "I0619 18:26:19.231941 139649090267008 learning.py:507] global step 427: loss = 0.4963 (2.992 sec/step)\n",
            "I0619 18:26:24.007663 139649090267008 learning.py:507] global step 427: loss = 0.3548 (4.774 sec/step)\n",
            "I0619 18:26:26.149108 139646017689344 supervisor.py:1050] Recording summary at step 427.\n",
            "I0619 18:26:27.526687 139649090267008 learning.py:507] global step 428: loss = 0.3341 (3.508 sec/step)\n",
            "I0619 18:26:30.591023 139649090267008 learning.py:507] global step 428: loss = 0.3770 (3.063 sec/step)\n",
            "I0619 18:26:33.565136 139649090267008 learning.py:507] global step 428: loss = 0.4213 (2.972 sec/step)\n",
            "I0619 18:26:36.567938 139649090267008 learning.py:507] global step 428: loss = 0.4029 (3.001 sec/step)\n",
            "I0619 18:26:39.542067 139649090267008 learning.py:507] global step 428: loss = 0.3387 (2.972 sec/step)\n",
            "I0619 18:26:42.451608 139649090267008 learning.py:507] global step 428: loss = 0.4050 (2.908 sec/step)\n",
            "I0619 18:26:45.407044 139649090267008 learning.py:507] global step 428: loss = 0.3698 (2.954 sec/step)\n",
            "I0619 18:26:48.508761 139649090267008 learning.py:507] global step 428: loss = 0.3695 (3.100 sec/step)\n",
            "I0619 18:26:52.007939 139649090267008 learning.py:507] global step 429: loss = 0.3206 (3.497 sec/step)\n",
            "I0619 18:26:54.957767 139649090267008 learning.py:507] global step 429: loss = 0.3871 (2.948 sec/step)\n",
            "I0619 18:26:57.991885 139649090267008 learning.py:507] global step 429: loss = 0.3368 (3.032 sec/step)\n",
            "I0619 18:27:00.957587 139649090267008 learning.py:507] global step 429: loss = 0.3431 (2.964 sec/step)\n",
            "I0619 18:27:03.907141 139649090267008 learning.py:507] global step 429: loss = 0.3227 (2.948 sec/step)\n",
            "I0619 18:27:06.866555 139649090267008 learning.py:507] global step 429: loss = 0.3617 (2.957 sec/step)\n",
            "I0619 18:27:10.486886 139649090267008 learning.py:507] global step 429: loss = 0.3093 (3.619 sec/step)\n",
            "I0619 18:27:13.442222 139649090267008 learning.py:507] global step 429: loss = 0.3427 (2.954 sec/step)\n",
            "I0619 18:27:16.428622 139649090267008 learning.py:507] global step 430: loss = 0.4017 (2.984 sec/step)\n",
            "I0619 18:27:19.461208 139649090267008 learning.py:507] global step 430: loss = 0.4132 (3.031 sec/step)\n",
            "I0619 18:27:22.387748 139649090267008 learning.py:507] global step 430: loss = 0.3859 (2.925 sec/step)\n",
            "I0619 18:27:25.323027 139649090267008 learning.py:507] global step 430: loss = 0.3534 (2.933 sec/step)\n",
            "I0619 18:27:28.313273 139649090267008 learning.py:507] global step 430: loss = 0.4326 (2.989 sec/step)\n",
            "I0619 18:27:31.277035 139649090267008 learning.py:507] global step 430: loss = 0.4251 (2.962 sec/step)\n",
            "I0619 18:27:34.440053 139649090267008 learning.py:507] global step 430: loss = 0.3334 (3.161 sec/step)\n",
            "I0619 18:27:37.370081 139649090267008 learning.py:507] global step 430: loss = 0.3887 (2.928 sec/step)\n",
            "I0619 18:27:40.259198 139649090267008 learning.py:507] global step 431: loss = 0.3573 (2.887 sec/step)\n",
            "I0619 18:27:43.201201 139649090267008 learning.py:507] global step 431: loss = 0.3117 (2.940 sec/step)\n",
            "I0619 18:27:46.180717 139649090267008 learning.py:507] global step 431: loss = 0.4004 (2.977 sec/step)\n",
            "I0619 18:27:49.375344 139649090267008 learning.py:507] global step 431: loss = 0.3412 (3.193 sec/step)\n",
            "I0619 18:27:52.483974 139649090267008 learning.py:507] global step 431: loss = 0.3487 (3.107 sec/step)\n",
            "I0619 18:27:55.473052 139649090267008 learning.py:507] global step 431: loss = 0.4623 (2.987 sec/step)\n",
            "I0619 18:27:58.410421 139649090267008 learning.py:507] global step 431: loss = 0.3761 (2.936 sec/step)\n",
            "I0619 18:28:01.427139 139649090267008 learning.py:507] global step 431: loss = 0.3630 (3.015 sec/step)\n",
            "I0619 18:28:04.401373 139649090267008 learning.py:507] global step 432: loss = 0.4156 (2.972 sec/step)\n",
            "I0619 18:28:07.691919 139649090267008 learning.py:507] global step 432: loss = 0.3977 (3.289 sec/step)\n",
            "I0619 18:28:10.695317 139649090267008 learning.py:507] global step 432: loss = 0.3424 (3.002 sec/step)\n",
            "I0619 18:28:13.645264 139649090267008 learning.py:507] global step 432: loss = 0.3306 (2.948 sec/step)\n",
            "I0619 18:28:16.827024 139649090267008 learning.py:507] global step 432: loss = 0.3286 (3.180 sec/step)\n",
            "I0619 18:28:19.851136 139649090267008 learning.py:507] global step 432: loss = 0.3526 (3.022 sec/step)\n",
            "I0619 18:28:24.729832 139649090267008 learning.py:507] global step 432: loss = 0.3269 (4.876 sec/step)\n",
            "I0619 18:28:26.259379 139646017689344 supervisor.py:1050] Recording summary at step 432.\n",
            "I0619 18:28:27.971070 139649090267008 learning.py:507] global step 432: loss = 0.5295 (3.230 sec/step)\n",
            "I0619 18:28:31.010241 139649090267008 learning.py:507] global step 433: loss = 0.4780 (3.037 sec/step)\n",
            "I0619 18:28:34.217911 139649090267008 learning.py:507] global step 433: loss = 0.3680 (3.206 sec/step)\n",
            "I0619 18:28:37.197358 139649090267008 learning.py:507] global step 433: loss = 0.3301 (2.978 sec/step)\n",
            "I0619 18:28:40.130785 139649090267008 learning.py:507] global step 433: loss = 0.3025 (2.932 sec/step)\n",
            "I0619 18:28:43.140505 139649090267008 learning.py:507] global step 433: loss = 0.3288 (3.008 sec/step)\n",
            "I0619 18:28:46.126594 139649090267008 learning.py:507] global step 433: loss = 0.4482 (2.984 sec/step)\n",
            "I0619 18:28:49.085531 139649090267008 learning.py:507] global step 433: loss = 0.4125 (2.957 sec/step)\n",
            "I0619 18:28:52.118074 139649090267008 learning.py:507] global step 433: loss = 0.3481 (3.031 sec/step)\n",
            "I0619 18:28:55.126257 139649090267008 learning.py:507] global step 434: loss = 0.4872 (3.006 sec/step)\n",
            "I0619 18:28:58.116616 139649090267008 learning.py:507] global step 434: loss = 0.4314 (2.989 sec/step)\n",
            "I0619 18:29:01.135920 139649090267008 learning.py:507] global step 434: loss = 0.4015 (3.018 sec/step)\n",
            "I0619 18:29:04.126680 139649090267008 learning.py:507] global step 434: loss = 0.4674 (2.989 sec/step)\n",
            "I0619 18:29:07.083204 139649090267008 learning.py:507] global step 434: loss = 0.4234 (2.955 sec/step)\n",
            "I0619 18:29:10.104312 139649090267008 learning.py:507] global step 434: loss = 0.2925 (3.020 sec/step)\n",
            "I0619 18:29:13.128887 139649090267008 learning.py:507] global step 434: loss = 0.3626 (3.023 sec/step)\n",
            "I0619 18:29:16.126474 139649090267008 learning.py:507] global step 434: loss = 0.3465 (2.996 sec/step)\n",
            "I0619 18:29:19.145393 139649090267008 learning.py:507] global step 435: loss = 0.3628 (3.017 sec/step)\n",
            "I0619 18:29:22.165159 139649090267008 learning.py:507] global step 435: loss = 0.3792 (3.018 sec/step)\n",
            "I0619 18:29:25.122514 139649090267008 learning.py:507] global step 435: loss = 0.3162 (2.956 sec/step)\n",
            "I0619 18:29:28.066073 139649090267008 learning.py:507] global step 435: loss = 0.3633 (2.942 sec/step)\n",
            "I0619 18:29:31.017889 139649090267008 learning.py:507] global step 435: loss = 0.2861 (2.950 sec/step)\n",
            "I0619 18:29:34.071571 139649090267008 learning.py:507] global step 435: loss = 0.3132 (3.052 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:29:37.040284 139649090267008 learning.py:507] global step 435: loss = 0.4126 (2.967 sec/step)\n",
            "I0619 18:29:40.015160 139649090267008 learning.py:507] global step 435: loss = 0.3494 (2.973 sec/step)\n",
            "I0619 18:29:43.018362 139649090267008 learning.py:507] global step 436: loss = 0.3695 (3.000 sec/step)\n",
            "I0619 18:29:45.941838 139649090267008 learning.py:507] global step 436: loss = 0.3097 (2.922 sec/step)\n",
            "I0619 18:29:48.956732 139649090267008 learning.py:507] global step 436: loss = 0.3373 (3.013 sec/step)\n",
            "I0619 18:29:51.914793 139649090267008 learning.py:507] global step 436: loss = 0.4304 (2.956 sec/step)\n",
            "I0619 18:29:54.897668 139649090267008 learning.py:507] global step 436: loss = 0.3639 (2.981 sec/step)\n",
            "I0619 18:29:57.813655 139649090267008 learning.py:507] global step 436: loss = 0.3772 (2.914 sec/step)\n",
            "I0619 18:30:00.838775 139649090267008 learning.py:507] global step 436: loss = 0.3340 (3.023 sec/step)\n",
            "I0619 18:30:03.807059 139649090267008 learning.py:507] global step 436: loss = 0.3480 (2.967 sec/step)\n",
            "I0619 18:30:06.856918 139649090267008 learning.py:507] global step 437: loss = 0.4238 (3.048 sec/step)\n",
            "I0619 18:30:09.780754 139649090267008 learning.py:507] global step 437: loss = 0.4097 (2.922 sec/step)\n",
            "I0619 18:30:12.772177 139649090267008 learning.py:507] global step 437: loss = 0.3559 (2.989 sec/step)\n",
            "I0619 18:30:15.707711 139649090267008 learning.py:507] global step 437: loss = 0.3198 (2.934 sec/step)\n",
            "I0619 18:30:18.750666 139649090267008 learning.py:507] global step 437: loss = 0.3522 (3.041 sec/step)\n",
            "I0619 18:30:20.934196 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:30:22.724443 139649090267008 learning.py:507] global step 437: loss = 0.3118 (3.937 sec/step)\n",
            "I0619 18:30:27.561448 139646017689344 supervisor.py:1050] Recording summary at step 437.\n",
            "I0619 18:30:28.385476 139649090267008 learning.py:507] global step 437: loss = 0.3326 (5.648 sec/step)\n",
            "I0619 18:30:31.330850 139649090267008 learning.py:507] global step 437: loss = 0.3390 (2.944 sec/step)\n",
            "I0619 18:30:34.296912 139649090267008 learning.py:507] global step 438: loss = 0.3848 (2.963 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:30:37.263490 139649090267008 learning.py:507] global step 438: loss = 0.2755 (2.965 sec/step)\n",
            "I0619 18:30:40.284382 139649090267008 learning.py:507] global step 438: loss = 0.4075 (3.019 sec/step)\n",
            "I0619 18:30:43.264855 139649090267008 learning.py:507] global step 438: loss = 0.3832 (2.979 sec/step)\n",
            "I0619 18:30:46.289156 139649090267008 learning.py:507] global step 438: loss = 0.4638 (3.023 sec/step)\n",
            "I0619 18:30:49.200230 139649090267008 learning.py:507] global step 438: loss = 0.3485 (2.910 sec/step)\n",
            "I0619 18:30:52.183994 139649090267008 learning.py:507] global step 438: loss = 0.4252 (2.982 sec/step)\n",
            "I0619 18:30:55.226851 139649090267008 learning.py:507] global step 438: loss = 0.3692 (3.041 sec/step)\n",
            "I0619 18:30:58.272673 139649090267008 learning.py:507] global step 439: loss = 0.4152 (3.044 sec/step)\n",
            "I0619 18:31:01.258785 139649090267008 learning.py:507] global step 439: loss = 0.3835 (2.984 sec/step)\n",
            "I0619 18:31:04.202336 139649090267008 learning.py:507] global step 439: loss = 0.4480 (2.942 sec/step)\n",
            "I0619 18:31:07.265643 139649090267008 learning.py:507] global step 439: loss = 0.2989 (3.062 sec/step)\n",
            "I0619 18:31:10.254448 139649090267008 learning.py:507] global step 439: loss = 0.4688 (2.987 sec/step)\n",
            "I0619 18:31:13.274203 139649090267008 learning.py:507] global step 439: loss = 0.3407 (3.018 sec/step)\n",
            "I0619 18:31:16.348317 139649090267008 learning.py:507] global step 439: loss = 0.3435 (3.073 sec/step)\n",
            "I0619 18:31:19.416639 139649090267008 learning.py:507] global step 439: loss = 0.4589 (3.067 sec/step)\n",
            "I0619 18:31:22.462457 139649090267008 learning.py:507] global step 440: loss = 0.3067 (3.043 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:31:25.437296 139649090267008 learning.py:507] global step 440: loss = 0.3344 (2.973 sec/step)\n",
            "I0619 18:31:28.416004 139649090267008 learning.py:507] global step 440: loss = 0.2888 (2.977 sec/step)\n",
            "I0619 18:31:31.374441 139649090267008 learning.py:507] global step 440: loss = 0.3458 (2.957 sec/step)\n",
            "I0619 18:31:34.307876 139649090267008 learning.py:507] global step 440: loss = 0.3051 (2.932 sec/step)\n",
            "I0619 18:31:37.397537 139649090267008 learning.py:507] global step 440: loss = 0.3867 (3.088 sec/step)\n",
            "I0619 18:31:40.474430 139649090267008 learning.py:507] global step 440: loss = 0.2953 (3.075 sec/step)\n",
            "I0619 18:31:43.478198 139649090267008 learning.py:507] global step 440: loss = 0.3347 (3.002 sec/step)\n",
            "I0619 18:31:46.438783 139649090267008 learning.py:507] global step 441: loss = 0.4305 (2.958 sec/step)\n",
            "I0619 18:31:49.378026 139649090267008 learning.py:507] global step 441: loss = 0.3116 (2.937 sec/step)\n",
            "I0619 18:31:52.319351 139649090267008 learning.py:507] global step 441: loss = 0.3172 (2.940 sec/step)\n",
            "I0619 18:31:55.284039 139649090267008 learning.py:507] global step 441: loss = 0.3529 (2.963 sec/step)\n",
            "I0619 18:31:58.427343 139649090267008 learning.py:507] global step 441: loss = 0.3838 (3.142 sec/step)\n",
            "I0619 18:32:01.366560 139649090267008 learning.py:507] global step 441: loss = 0.4264 (2.938 sec/step)\n",
            "I0619 18:32:04.355735 139649090267008 learning.py:507] global step 441: loss = 0.3012 (2.987 sec/step)\n",
            "I0619 18:32:07.309277 139649090267008 learning.py:507] global step 441: loss = 0.3284 (2.952 sec/step)\n",
            "I0619 18:32:10.257326 139649090267008 learning.py:507] global step 442: loss = 0.3805 (2.946 sec/step)\n",
            "I0619 18:32:13.375105 139649090267008 learning.py:507] global step 442: loss = 0.2790 (3.116 sec/step)\n",
            "I0619 18:32:16.624791 139649090267008 learning.py:507] global step 442: loss = 0.3323 (3.248 sec/step)\n",
            "I0619 18:32:19.651443 139649090267008 learning.py:507] global step 442: loss = 0.2985 (3.025 sec/step)\n",
            "I0619 18:32:24.611999 139649090267008 learning.py:507] global step 442: loss = 0.4530 (4.948 sec/step)\n",
            "I0619 18:32:26.311794 139646017689344 supervisor.py:1050] Recording summary at step 442.\n",
            "I0619 18:32:28.033316 139649090267008 learning.py:507] global step 442: loss = 0.5244 (3.417 sec/step)\n",
            "I0619 18:32:31.271533 139649090267008 learning.py:507] global step 442: loss = 0.3589 (3.237 sec/step)\n",
            "I0619 18:32:34.305615 139649090267008 learning.py:507] global step 442: loss = 0.3289 (3.032 sec/step)\n",
            "I0619 18:32:37.326581 139649090267008 learning.py:507] global step 443: loss = 0.3356 (3.019 sec/step)\n",
            "I0619 18:32:40.362238 139649090267008 learning.py:507] global step 443: loss = 0.3549 (3.034 sec/step)\n",
            "I0619 18:32:43.372597 139649090267008 learning.py:507] global step 443: loss = 0.3255 (3.009 sec/step)\n",
            "I0619 18:32:46.414463 139649090267008 learning.py:507] global step 443: loss = 0.3975 (3.040 sec/step)\n",
            "I0619 18:32:49.447204 139649090267008 learning.py:507] global step 443: loss = 0.3835 (3.031 sec/step)\n",
            "I0619 18:32:52.627023 139649090267008 learning.py:507] global step 443: loss = 0.3402 (3.178 sec/step)\n",
            "I0619 18:32:55.680202 139649090267008 learning.py:507] global step 443: loss = 0.4095 (3.051 sec/step)\n",
            "I0619 18:32:58.676001 139649090267008 learning.py:507] global step 443: loss = 0.3119 (2.994 sec/step)\n",
            "I0619 18:33:01.756205 139649090267008 learning.py:507] global step 444: loss = 0.3902 (3.078 sec/step)\n",
            "I0619 18:33:04.766598 139649090267008 learning.py:507] global step 444: loss = 0.3235 (3.008 sec/step)\n",
            "I0619 18:33:07.794760 139649090267008 learning.py:507] global step 444: loss = 0.5284 (3.026 sec/step)\n",
            "I0619 18:33:11.154387 139649090267008 learning.py:507] global step 444: loss = 0.3756 (3.358 sec/step)\n",
            "I0619 18:33:14.179226 139649090267008 learning.py:507] global step 444: loss = 0.4123 (3.023 sec/step)\n",
            "I0619 18:33:17.270235 139649090267008 learning.py:507] global step 444: loss = 0.3274 (3.089 sec/step)\n",
            "I0619 18:33:20.272774 139649090267008 learning.py:507] global step 444: loss = 0.4431 (3.001 sec/step)\n",
            "I0619 18:33:23.324598 139649090267008 learning.py:507] global step 444: loss = 0.4466 (3.050 sec/step)\n",
            "I0619 18:33:26.390472 139649090267008 learning.py:507] global step 445: loss = 0.3476 (3.064 sec/step)\n",
            "I0619 18:33:29.481487 139649090267008 learning.py:507] global step 445: loss = 0.3413 (3.089 sec/step)\n",
            "I0619 18:33:32.559883 139649090267008 learning.py:507] global step 445: loss = 0.3304 (3.077 sec/step)\n",
            "I0619 18:33:35.637315 139649090267008 learning.py:507] global step 445: loss = 0.3597 (3.076 sec/step)\n",
            "I0619 18:33:38.721901 139649090267008 learning.py:507] global step 445: loss = 0.3058 (3.082 sec/step)\n",
            "I0619 18:33:41.787030 139649090267008 learning.py:507] global step 445: loss = 0.3833 (3.063 sec/step)\n",
            "I0619 18:33:44.826726 139649090267008 learning.py:507] global step 445: loss = 0.3846 (3.038 sec/step)\n",
            "I0619 18:33:47.911662 139649090267008 learning.py:507] global step 445: loss = 0.3487 (3.083 sec/step)\n",
            "I0619 18:33:50.946883 139649090267008 learning.py:507] global step 446: loss = 0.3326 (3.033 sec/step)\n",
            "I0619 18:33:53.996897 139649090267008 learning.py:507] global step 446: loss = 0.4276 (3.048 sec/step)\n",
            "I0619 18:33:57.057993 139649090267008 learning.py:507] global step 446: loss = 0.3484 (3.059 sec/step)\n",
            "I0619 18:34:00.118637 139649090267008 learning.py:507] global step 446: loss = 0.3736 (3.059 sec/step)\n",
            "I0619 18:34:03.290126 139649090267008 learning.py:507] global step 446: loss = 0.4048 (3.170 sec/step)\n",
            "I0619 18:34:06.378036 139649090267008 learning.py:507] global step 446: loss = 0.3146 (3.086 sec/step)\n",
            "I0619 18:34:09.484798 139649090267008 learning.py:507] global step 446: loss = 0.4148 (3.104 sec/step)\n",
            "I0619 18:34:12.554869 139649090267008 learning.py:507] global step 446: loss = 0.3491 (3.068 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:34:15.616667 139649090267008 learning.py:507] global step 447: loss = 0.3549 (3.059 sec/step)\n",
            "I0619 18:34:18.646292 139649090267008 learning.py:507] global step 447: loss = 0.3823 (3.028 sec/step)\n",
            "I0619 18:34:22.672275 139649090267008 learning.py:507] global step 447: loss = 0.2883 (4.015 sec/step)\n",
            "I0619 18:34:26.276000 139646017689344 supervisor.py:1050] Recording summary at step 447.\n",
            "I0619 18:34:27.190136 139649090267008 learning.py:507] global step 447: loss = 0.3200 (4.512 sec/step)\n",
            "I0619 18:34:30.229060 139649090267008 learning.py:507] global step 447: loss = 0.3571 (3.037 sec/step)\n",
            "I0619 18:34:33.277752 139649090267008 learning.py:507] global step 447: loss = 0.3403 (3.047 sec/step)\n",
            "I0619 18:34:36.342698 139649090267008 learning.py:507] global step 447: loss = 0.3729 (3.063 sec/step)\n",
            "I0619 18:34:39.479333 139649090267008 learning.py:507] global step 447: loss = 0.3022 (3.135 sec/step)\n",
            "I0619 18:34:42.693512 139649090267008 learning.py:507] global step 448: loss = 0.3631 (3.212 sec/step)\n",
            "I0619 18:34:45.743596 139649090267008 learning.py:507] global step 448: loss = 0.4130 (3.048 sec/step)\n",
            "I0619 18:34:48.782612 139649090267008 learning.py:507] global step 448: loss = 0.4972 (3.037 sec/step)\n",
            "I0619 18:34:51.856826 139649090267008 learning.py:507] global step 448: loss = 0.3324 (3.072 sec/step)\n",
            "I0619 18:34:54.930128 139649090267008 learning.py:507] global step 448: loss = 0.3380 (3.071 sec/step)\n",
            "I0619 18:34:58.019392 139649090267008 learning.py:507] global step 448: loss = 0.4067 (3.087 sec/step)\n",
            "I0619 18:35:01.371912 139649090267008 learning.py:507] global step 448: loss = 0.3212 (3.351 sec/step)\n",
            "I0619 18:35:04.426164 139649090267008 learning.py:507] global step 448: loss = 0.3420 (3.052 sec/step)\n",
            "I0619 18:35:07.503756 139649090267008 learning.py:507] global step 449: loss = 0.3981 (3.075 sec/step)\n",
            "I0619 18:35:10.516412 139649090267008 learning.py:507] global step 449: loss = 0.4622 (3.011 sec/step)\n",
            "I0619 18:35:13.574946 139649090267008 learning.py:507] global step 449: loss = 0.3521 (3.057 sec/step)\n",
            "I0619 18:35:16.652707 139649090267008 learning.py:507] global step 449: loss = 0.3650 (3.076 sec/step)\n",
            "I0619 18:35:19.686887 139649090267008 learning.py:507] global step 449: loss = 0.3251 (3.032 sec/step)\n",
            "I0619 18:35:22.765062 139649090267008 learning.py:507] global step 449: loss = 0.4135 (3.076 sec/step)\n",
            "I0619 18:35:25.846994 139649090267008 learning.py:507] global step 449: loss = 0.4198 (3.080 sec/step)\n",
            "I0619 18:35:28.897893 139649090267008 learning.py:507] global step 449: loss = 0.3959 (3.049 sec/step)\n",
            "I0619 18:35:32.020153 139649090267008 learning.py:507] global step 450: loss = 0.3877 (3.120 sec/step)\n",
            "I0619 18:35:35.094120 139649090267008 learning.py:507] global step 450: loss = 0.3419 (3.072 sec/step)\n",
            "I0619 18:35:38.087500 139649090267008 learning.py:507] global step 450: loss = 0.4172 (2.992 sec/step)\n",
            "I0619 18:35:41.149487 139649090267008 learning.py:507] global step 450: loss = 0.2900 (3.060 sec/step)\n",
            "I0619 18:35:44.314367 139649090267008 learning.py:507] global step 450: loss = 0.4444 (3.163 sec/step)\n",
            "I0619 18:35:47.422623 139649090267008 learning.py:507] global step 450: loss = 0.3286 (3.106 sec/step)\n",
            "I0619 18:35:50.463861 139649090267008 learning.py:507] global step 450: loss = 0.4528 (3.039 sec/step)\n",
            "I0619 18:35:53.604759 139649090267008 learning.py:507] global step 450: loss = 0.3844 (3.139 sec/step)\n",
            "I0619 18:35:56.687809 139649090267008 learning.py:507] global step 451: loss = 0.3662 (3.081 sec/step)\n",
            "I0619 18:35:59.750891 139649090267008 learning.py:507] global step 451: loss = 0.3307 (3.061 sec/step)\n",
            "I0619 18:36:03.033816 139649090267008 learning.py:507] global step 451: loss = 0.3673 (3.281 sec/step)\n",
            "I0619 18:36:06.105284 139649090267008 learning.py:507] global step 451: loss = 0.4081 (3.069 sec/step)\n",
            "I0619 18:36:09.223250 139649090267008 learning.py:507] global step 451: loss = 0.4992 (3.116 sec/step)\n",
            "I0619 18:36:12.266307 139649090267008 learning.py:507] global step 451: loss = 0.4432 (3.041 sec/step)\n",
            "I0619 18:36:15.324375 139649090267008 learning.py:507] global step 451: loss = 0.3729 (3.056 sec/step)\n",
            "I0619 18:36:18.472457 139649090267008 learning.py:507] global step 451: loss = 0.3710 (3.146 sec/step)\n",
            "I0619 18:36:22.296354 139649090267008 learning.py:507] global step 452: loss = 0.3273 (3.804 sec/step)\n",
            "I0619 18:36:26.536221 139646017689344 supervisor.py:1050] Recording summary at step 452.\n",
            "I0619 18:36:27.230421 139649090267008 learning.py:507] global step 452: loss = 0.3125 (4.931 sec/step)\n",
            "I0619 18:36:30.282528 139649090267008 learning.py:507] global step 452: loss = 0.3440 (3.050 sec/step)\n",
            "I0619 18:36:33.338177 139649090267008 learning.py:507] global step 452: loss = 0.3703 (3.054 sec/step)\n",
            "I0619 18:36:36.419375 139649090267008 learning.py:507] global step 452: loss = 0.3162 (3.079 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:36:39.561728 139649090267008 learning.py:507] global step 452: loss = 0.3201 (3.140 sec/step)\n",
            "I0619 18:36:42.660522 139649090267008 learning.py:507] global step 452: loss = 0.4357 (3.097 sec/step)\n",
            "I0619 18:36:45.932264 139649090267008 learning.py:507] global step 452: loss = 0.3247 (3.270 sec/step)\n",
            "I0619 18:36:49.065819 139649090267008 learning.py:507] global step 453: loss = 0.3088 (3.131 sec/step)\n",
            "I0619 18:36:52.106740 139649090267008 learning.py:507] global step 453: loss = 0.4450 (3.039 sec/step)\n",
            "I0619 18:36:55.219874 139649090267008 learning.py:507] global step 453: loss = 0.3382 (3.111 sec/step)\n",
            "I0619 18:36:58.288660 139649090267008 learning.py:507] global step 453: loss = 0.3199 (3.067 sec/step)\n",
            "I0619 18:37:01.635650 139649090267008 learning.py:507] global step 453: loss = 0.3186 (3.345 sec/step)\n",
            "I0619 18:37:04.659160 139649090267008 learning.py:507] global step 453: loss = 0.3069 (3.022 sec/step)\n",
            "I0619 18:37:07.703350 139649090267008 learning.py:507] global step 453: loss = 0.3330 (3.042 sec/step)\n",
            "I0619 18:37:10.865868 139649090267008 learning.py:507] global step 453: loss = 0.3518 (3.161 sec/step)\n",
            "I0619 18:37:13.971622 139649090267008 learning.py:507] global step 454: loss = 0.3484 (3.104 sec/step)\n",
            "I0619 18:37:17.042673 139649090267008 learning.py:507] global step 454: loss = 0.4032 (3.069 sec/step)\n",
            "I0619 18:37:20.551502 139649090267008 learning.py:507] global step 454: loss = 0.3476 (3.507 sec/step)\n",
            "I0619 18:37:23.636202 139649090267008 learning.py:507] global step 454: loss = 0.3653 (3.083 sec/step)\n",
            "I0619 18:37:26.739079 139649090267008 learning.py:507] global step 454: loss = 0.3269 (3.101 sec/step)\n",
            "I0619 18:37:29.861005 139649090267008 learning.py:507] global step 454: loss = 0.4621 (3.120 sec/step)\n",
            "I0619 18:37:32.907690 139649090267008 learning.py:507] global step 454: loss = 0.3597 (3.045 sec/step)\n",
            "I0619 18:37:36.017842 139649090267008 learning.py:507] global step 454: loss = 0.3260 (3.108 sec/step)\n",
            "I0619 18:37:39.131727 139649090267008 learning.py:507] global step 455: loss = 0.4206 (3.112 sec/step)\n",
            "I0619 18:37:42.256675 139649090267008 learning.py:507] global step 455: loss = 0.3550 (3.123 sec/step)\n",
            "I0619 18:37:45.341433 139649090267008 learning.py:507] global step 455: loss = 0.3222 (3.083 sec/step)\n",
            "I0619 18:37:48.426119 139649090267008 learning.py:507] global step 455: loss = 0.3543 (3.083 sec/step)\n",
            "I0619 18:37:51.541674 139649090267008 learning.py:507] global step 455: loss = 0.3865 (3.114 sec/step)\n",
            "I0619 18:37:54.638794 139649090267008 learning.py:507] global step 455: loss = 0.3211 (3.095 sec/step)\n",
            "I0619 18:37:57.682655 139649090267008 learning.py:507] global step 455: loss = 0.3362 (3.042 sec/step)\n",
            "I0619 18:38:00.803864 139649090267008 learning.py:507] global step 455: loss = 0.3348 (3.119 sec/step)\n",
            "I0619 18:38:03.873515 139649090267008 learning.py:507] global step 456: loss = 0.3339 (3.068 sec/step)\n",
            "I0619 18:38:06.996312 139649090267008 learning.py:507] global step 456: loss = 0.3978 (3.121 sec/step)\n",
            "I0619 18:38:10.106384 139649090267008 learning.py:507] global step 456: loss = 0.3292 (3.108 sec/step)\n",
            "I0619 18:38:13.350567 139649090267008 learning.py:507] global step 456: loss = 0.3244 (3.242 sec/step)\n",
            "I0619 18:38:16.469349 139649090267008 learning.py:507] global step 456: loss = 0.3769 (3.117 sec/step)\n",
            "I0619 18:38:19.627770 139649090267008 learning.py:507] global step 456: loss = 0.3947 (3.157 sec/step)\n",
            "I0619 18:38:24.752575 139649090267008 learning.py:507] global step 456: loss = 0.3440 (5.123 sec/step)\n",
            "I0619 18:38:26.479030 139646017689344 supervisor.py:1050] Recording summary at step 456.\n",
            "I0619 18:38:28.175807 139649090267008 learning.py:507] global step 456: loss = 0.3914 (3.416 sec/step)\n",
            "I0619 18:38:31.249987 139649090267008 learning.py:507] global step 457: loss = 0.2929 (3.072 sec/step)\n",
            "I0619 18:38:34.315946 139649090267008 learning.py:507] global step 457: loss = 0.3419 (3.064 sec/step)\n",
            "I0619 18:38:37.492010 139649090267008 learning.py:507] global step 457: loss = 0.3637 (3.174 sec/step)\n",
            "I0619 18:38:40.558923 139649090267008 learning.py:507] global step 457: loss = 0.2783 (3.065 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:38:43.738725 139649090267008 learning.py:507] global step 457: loss = 0.3419 (3.178 sec/step)\n",
            "I0619 18:38:46.826014 139649090267008 learning.py:507] global step 457: loss = 0.2929 (3.085 sec/step)\n",
            "I0619 18:38:49.902045 139649090267008 learning.py:507] global step 457: loss = 0.3430 (3.074 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:38:53.114870 139649090267008 learning.py:507] global step 457: loss = 0.3226 (3.211 sec/step)\n",
            "I0619 18:38:56.243708 139649090267008 learning.py:507] global step 458: loss = 0.3733 (3.126 sec/step)\n",
            "I0619 18:38:59.365701 139649090267008 learning.py:507] global step 458: loss = 0.4182 (3.120 sec/step)\n",
            "I0619 18:39:02.506462 139649090267008 learning.py:507] global step 458: loss = 0.3131 (3.139 sec/step)\n",
            "I0619 18:39:05.732510 139649090267008 learning.py:507] global step 458: loss = 0.2997 (3.224 sec/step)\n",
            "I0619 18:39:08.876057 139649090267008 learning.py:507] global step 458: loss = 0.3125 (3.142 sec/step)\n",
            "I0619 18:39:12.129465 139649090267008 learning.py:507] global step 458: loss = 0.3733 (3.252 sec/step)\n",
            "I0619 18:39:15.314716 139649090267008 learning.py:507] global step 458: loss = 0.3217 (3.183 sec/step)\n",
            "I0619 18:39:18.458190 139649090267008 learning.py:507] global step 458: loss = 0.3595 (3.142 sec/step)\n",
            "I0619 18:39:21.609737 139649090267008 learning.py:507] global step 459: loss = 0.3239 (3.150 sec/step)\n",
            "I0619 18:39:24.738064 139649090267008 learning.py:507] global step 459: loss = 0.3438 (3.126 sec/step)\n",
            "I0619 18:39:27.953057 139649090267008 learning.py:507] global step 459: loss = 0.3306 (3.213 sec/step)\n",
            "I0619 18:39:31.063777 139649090267008 learning.py:507] global step 459: loss = 0.3542 (3.109 sec/step)\n",
            "I0619 18:39:34.152771 139649090267008 learning.py:507] global step 459: loss = 0.4305 (3.087 sec/step)\n",
            "I0619 18:39:37.220818 139649090267008 learning.py:507] global step 459: loss = 0.4055 (3.066 sec/step)\n",
            "I0619 18:39:40.339078 139649090267008 learning.py:507] global step 459: loss = 0.3503 (3.116 sec/step)\n",
            "I0619 18:39:43.441505 139649090267008 learning.py:507] global step 459: loss = 0.3559 (3.101 sec/step)\n",
            "I0619 18:39:46.563179 139649090267008 learning.py:507] global step 460: loss = 0.3122 (3.119 sec/step)\n",
            "I0619 18:39:49.620267 139649090267008 learning.py:507] global step 460: loss = 0.3968 (3.055 sec/step)\n",
            "I0619 18:39:52.710268 139649090267008 learning.py:507] global step 460: loss = 0.3235 (3.088 sec/step)\n",
            "I0619 18:39:55.780185 139649090267008 learning.py:507] global step 460: loss = 0.3635 (3.068 sec/step)\n",
            "I0619 18:39:58.952019 139649090267008 learning.py:507] global step 460: loss = 0.2843 (3.170 sec/step)\n",
            "I0619 18:40:02.057557 139649090267008 learning.py:507] global step 460: loss = 0.3748 (3.104 sec/step)\n",
            "I0619 18:40:05.105832 139649090267008 learning.py:507] global step 460: loss = 0.3813 (3.046 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:40:08.151594 139649090267008 learning.py:507] global step 460: loss = 0.3674 (3.044 sec/step)\n",
            "I0619 18:40:11.158339 139649090267008 learning.py:507] global step 461: loss = 0.4288 (3.004 sec/step)\n",
            "I0619 18:40:14.138131 139649090267008 learning.py:507] global step 461: loss = 0.3469 (2.978 sec/step)\n",
            "I0619 18:40:17.612133 139649090267008 learning.py:507] global step 461: loss = 0.4039 (3.472 sec/step)\n",
            "I0619 18:40:20.697501 139649090267008 learning.py:507] global step 461: loss = 0.4374 (3.083 sec/step)\n",
            "I0619 18:40:20.931304 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:40:27.666258 139649090267008 learning.py:507] global step 461: loss = 0.3984 (6.967 sec/step)\n",
            "I0619 18:40:28.374714 139646017689344 supervisor.py:1050] Recording summary at step 461.\n",
            "I0619 18:40:30.781973 139649090267008 learning.py:507] global step 461: loss = 0.3413 (3.112 sec/step)\n",
            "I0619 18:40:33.811780 139649090267008 learning.py:507] global step 461: loss = 0.4241 (3.028 sec/step)\n",
            "I0619 18:40:37.149436 139649090267008 learning.py:507] global step 461: loss = 0.3899 (3.336 sec/step)\n",
            "I0619 18:40:40.233762 139649090267008 learning.py:507] global step 462: loss = 0.3636 (3.082 sec/step)\n",
            "I0619 18:40:43.272404 139649090267008 learning.py:507] global step 462: loss = 0.3664 (3.037 sec/step)\n",
            "I0619 18:40:46.285343 139649090267008 learning.py:507] global step 462: loss = 0.3480 (3.011 sec/step)\n",
            "I0619 18:40:49.348577 139649090267008 learning.py:507] global step 462: loss = 0.3411 (3.061 sec/step)\n",
            "I0619 18:40:52.380681 139649090267008 learning.py:507] global step 462: loss = 0.4113 (3.030 sec/step)\n",
            "I0619 18:40:55.436833 139649090267008 learning.py:507] global step 462: loss = 0.3734 (3.054 sec/step)\n",
            "I0619 18:40:58.478585 139649090267008 learning.py:507] global step 462: loss = 0.3497 (3.040 sec/step)\n",
            "I0619 18:41:01.545426 139649090267008 learning.py:507] global step 462: loss = 0.2908 (3.065 sec/step)\n",
            "I0619 18:41:04.602004 139649090267008 learning.py:507] global step 463: loss = 0.5219 (3.055 sec/step)\n",
            "I0619 18:41:07.691282 139649090267008 learning.py:507] global step 463: loss = 0.3711 (3.087 sec/step)\n",
            "I0619 18:41:10.765033 139649090267008 learning.py:507] global step 463: loss = 0.3687 (3.072 sec/step)\n",
            "I0619 18:41:13.785777 139649090267008 learning.py:507] global step 463: loss = 0.3813 (3.017 sec/step)\n",
            "I0619 18:41:16.812697 139649090267008 learning.py:507] global step 463: loss = 0.3697 (3.025 sec/step)\n",
            "I0619 18:41:19.896239 139649090267008 learning.py:507] global step 463: loss = 0.4006 (3.082 sec/step)\n",
            "I0619 18:41:22.885622 139649090267008 learning.py:507] global step 463: loss = 0.2871 (2.988 sec/step)\n",
            "I0619 18:41:26.060688 139649090267008 learning.py:507] global step 463: loss = 0.3746 (3.173 sec/step)\n",
            "I0619 18:41:29.155375 139649090267008 learning.py:507] global step 464: loss = 0.3080 (3.092 sec/step)\n",
            "I0619 18:41:32.152826 139649090267008 learning.py:507] global step 464: loss = 0.3157 (2.996 sec/step)\n",
            "I0619 18:41:35.119756 139649090267008 learning.py:507] global step 464: loss = 0.3072 (2.965 sec/step)\n",
            "I0619 18:41:38.156587 139649090267008 learning.py:507] global step 464: loss = 0.3870 (3.035 sec/step)\n",
            "I0619 18:41:41.631628 139649090267008 learning.py:507] global step 464: loss = 0.5145 (3.473 sec/step)\n",
            "I0619 18:41:44.631378 139649090267008 learning.py:507] global step 464: loss = 0.4597 (2.998 sec/step)\n",
            "I0619 18:41:47.701735 139649090267008 learning.py:507] global step 464: loss = 0.3611 (3.069 sec/step)\n",
            "I0619 18:41:50.800093 139649090267008 learning.py:507] global step 464: loss = 0.3150 (3.097 sec/step)\n",
            "I0619 18:41:53.854023 139649090267008 learning.py:507] global step 465: loss = 0.3801 (3.052 sec/step)\n",
            "I0619 18:41:56.853465 139649090267008 learning.py:507] global step 465: loss = 0.3714 (2.998 sec/step)\n",
            "I0619 18:42:00.463134 139649090267008 learning.py:507] global step 465: loss = 0.3398 (3.608 sec/step)\n",
            "I0619 18:42:03.498117 139649090267008 learning.py:507] global step 465: loss = 0.3870 (3.033 sec/step)\n",
            "I0619 18:42:06.514182 139649090267008 learning.py:507] global step 465: loss = 0.3356 (3.014 sec/step)\n",
            "I0619 18:42:09.543844 139649090267008 learning.py:507] global step 465: loss = 0.3325 (3.028 sec/step)\n",
            "I0619 18:42:12.653226 139649090267008 learning.py:507] global step 465: loss = 0.4789 (3.108 sec/step)\n",
            "I0619 18:42:15.682319 139649090267008 learning.py:507] global step 465: loss = 0.3392 (3.027 sec/step)\n",
            "I0619 18:42:18.709511 139649090267008 learning.py:507] global step 466: loss = 0.3023 (3.025 sec/step)\n",
            "I0619 18:42:23.022435 139649090267008 learning.py:507] global step 466: loss = 0.4860 (4.299 sec/step)\n",
            "I0619 18:42:26.377907 139646017689344 supervisor.py:1050] Recording summary at step 466.\n",
            "I0619 18:42:27.305528 139649090267008 learning.py:507] global step 466: loss = 0.3483 (4.268 sec/step)\n",
            "I0619 18:42:30.338025 139649090267008 learning.py:507] global step 466: loss = 0.3124 (3.031 sec/step)\n",
            "I0619 18:42:33.370277 139649090267008 learning.py:507] global step 466: loss = 0.4601 (3.031 sec/step)\n",
            "I0619 18:42:36.410658 139649090267008 learning.py:507] global step 466: loss = 0.3628 (3.039 sec/step)\n",
            "I0619 18:42:39.424820 139649090267008 learning.py:507] global step 466: loss = 0.3658 (3.013 sec/step)\n",
            "I0619 18:42:42.678786 139649090267008 learning.py:507] global step 466: loss = 0.3330 (3.252 sec/step)\n",
            "I0619 18:42:45.699793 139649090267008 learning.py:507] global step 467: loss = 0.3742 (3.019 sec/step)\n",
            "I0619 18:42:48.741454 139649090267008 learning.py:507] global step 467: loss = 0.4313 (3.040 sec/step)\n",
            "I0619 18:42:51.772660 139649090267008 learning.py:507] global step 467: loss = 0.2993 (3.029 sec/step)\n",
            "I0619 18:42:54.732587 139649090267008 learning.py:507] global step 467: loss = 0.3951 (2.958 sec/step)\n",
            "I0619 18:42:57.789881 139649090267008 learning.py:507] global step 467: loss = 0.3263 (3.056 sec/step)\n",
            "I0619 18:43:00.803307 139649090267008 learning.py:507] global step 467: loss = 0.3253 (3.012 sec/step)\n",
            "I0619 18:43:03.816654 139649090267008 learning.py:507] global step 467: loss = 0.3668 (3.012 sec/step)\n",
            "I0619 18:43:06.876685 139649090267008 learning.py:507] global step 467: loss = 0.3390 (3.058 sec/step)\n",
            "I0619 18:43:09.921442 139649090267008 learning.py:507] global step 468: loss = 0.3757 (3.042 sec/step)\n",
            "I0619 18:43:12.947093 139649090267008 learning.py:507] global step 468: loss = 0.3033 (3.024 sec/step)\n",
            "I0619 18:43:15.946127 139649090267008 learning.py:507] global step 468: loss = 0.3182 (2.996 sec/step)\n",
            "I0619 18:43:18.988286 139649090267008 learning.py:507] global step 468: loss = 0.3618 (3.040 sec/step)\n",
            "I0619 18:43:22.038938 139649090267008 learning.py:507] global step 468: loss = 0.4823 (3.049 sec/step)\n",
            "I0619 18:43:25.086598 139649090267008 learning.py:507] global step 468: loss = 0.3732 (3.046 sec/step)\n",
            "I0619 18:43:28.164222 139649090267008 learning.py:507] global step 468: loss = 0.3301 (3.076 sec/step)\n",
            "I0619 18:43:31.167531 139649090267008 learning.py:507] global step 468: loss = 0.3040 (3.001 sec/step)\n",
            "I0619 18:43:34.242225 139649090267008 learning.py:507] global step 469: loss = 0.4336 (3.072 sec/step)\n",
            "I0619 18:43:37.262642 139649090267008 learning.py:507] global step 469: loss = 0.3409 (3.019 sec/step)\n",
            "I0619 18:43:40.306500 139649090267008 learning.py:507] global step 469: loss = 0.3325 (3.042 sec/step)\n",
            "I0619 18:43:43.324710 139649090267008 learning.py:507] global step 469: loss = 0.2923 (3.016 sec/step)\n",
            "I0619 18:43:46.367236 139649090267008 learning.py:507] global step 469: loss = 0.3688 (3.041 sec/step)\n",
            "I0619 18:43:49.413385 139649090267008 learning.py:507] global step 469: loss = 0.3282 (3.044 sec/step)\n",
            "I0619 18:43:52.510609 139649090267008 learning.py:507] global step 469: loss = 0.3479 (3.095 sec/step)\n",
            "I0619 18:43:55.606824 139649090267008 learning.py:507] global step 469: loss = 0.4239 (3.094 sec/step)\n",
            "I0619 18:43:58.653551 139649090267008 learning.py:507] global step 470: loss = 0.3916 (3.043 sec/step)\n",
            "I0619 18:44:01.738804 139649090267008 learning.py:507] global step 470: loss = 0.2869 (3.083 sec/step)\n",
            "I0619 18:44:04.819449 139649090267008 learning.py:507] global step 470: loss = 0.3300 (3.079 sec/step)\n",
            "I0619 18:44:07.844060 139649090267008 learning.py:507] global step 470: loss = 0.3445 (3.023 sec/step)\n",
            "I0619 18:44:10.900536 139649090267008 learning.py:507] global step 470: loss = 0.3886 (3.055 sec/step)\n",
            "I0619 18:44:13.981680 139649090267008 learning.py:507] global step 470: loss = 0.3043 (3.079 sec/step)\n",
            "I0619 18:44:17.034354 139649090267008 learning.py:507] global step 470: loss = 0.3723 (3.051 sec/step)\n",
            "I0619 18:44:20.102141 139649090267008 learning.py:507] global step 470: loss = 0.3416 (3.066 sec/step)\n",
            "I0619 18:44:25.345006 139649090267008 learning.py:507] global step 471: loss = 0.3493 (5.237 sec/step)\n",
            "I0619 18:44:26.913755 139646017689344 supervisor.py:1050] Recording summary at step 471.\n",
            "I0619 18:44:28.853407 139649090267008 learning.py:507] global step 471: loss = 0.4074 (3.506 sec/step)\n",
            "I0619 18:44:32.045725 139649090267008 learning.py:507] global step 471: loss = 0.4213 (3.191 sec/step)\n",
            "I0619 18:44:35.203231 139649090267008 learning.py:507] global step 471: loss = 0.4512 (3.156 sec/step)\n",
            "I0619 18:44:38.297685 139649090267008 learning.py:507] global step 471: loss = 0.2866 (3.093 sec/step)\n",
            "I0619 18:44:41.416789 139649090267008 learning.py:507] global step 471: loss = 0.3441 (3.117 sec/step)\n",
            "I0619 18:44:44.534822 139649090267008 learning.py:507] global step 471: loss = 0.3078 (3.116 sec/step)\n",
            "I0619 18:44:47.662769 139649090267008 learning.py:507] global step 471: loss = 0.3583 (3.126 sec/step)\n",
            "I0619 18:44:50.923138 139649090267008 learning.py:507] global step 472: loss = 0.3353 (3.258 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:44:54.023979 139649090267008 learning.py:507] global step 472: loss = 0.4109 (3.099 sec/step)\n",
            "I0619 18:44:57.128753 139649090267008 learning.py:507] global step 472: loss = 0.4294 (3.103 sec/step)\n",
            "I0619 18:45:00.161711 139649090267008 learning.py:507] global step 472: loss = 0.3243 (3.031 sec/step)\n",
            "I0619 18:45:03.332991 139649090267008 learning.py:507] global step 472: loss = 0.3676 (3.169 sec/step)\n",
            "I0619 18:45:06.421596 139649090267008 learning.py:507] global step 472: loss = 0.2744 (3.087 sec/step)\n",
            "I0619 18:45:09.498053 139649090267008 learning.py:507] global step 472: loss = 0.3151 (3.075 sec/step)\n",
            "I0619 18:45:12.604385 139649090267008 learning.py:507] global step 472: loss = 0.3723 (3.104 sec/step)\n",
            "I0619 18:45:15.671798 139649090267008 learning.py:507] global step 473: loss = 0.3058 (3.066 sec/step)\n",
            "I0619 18:45:18.865674 139649090267008 learning.py:507] global step 473: loss = 0.3459 (3.192 sec/step)\n",
            "I0619 18:45:21.969954 139649090267008 learning.py:507] global step 473: loss = 0.3323 (3.102 sec/step)\n",
            "I0619 18:45:25.049896 139649090267008 learning.py:507] global step 473: loss = 0.4306 (3.078 sec/step)\n",
            "I0619 18:45:28.158279 139649090267008 learning.py:507] global step 473: loss = 0.3810 (3.106 sec/step)\n",
            "I0619 18:45:31.256313 139649090267008 learning.py:507] global step 473: loss = 0.3551 (3.096 sec/step)\n",
            "I0619 18:45:34.341195 139649090267008 learning.py:507] global step 473: loss = 0.3169 (3.083 sec/step)\n",
            "I0619 18:45:37.561713 139649090267008 learning.py:507] global step 473: loss = 0.3874 (3.219 sec/step)\n",
            "I0619 18:45:40.658281 139649090267008 learning.py:507] global step 474: loss = 0.3065 (3.094 sec/step)\n",
            "I0619 18:45:43.782952 139649090267008 learning.py:507] global step 474: loss = 0.6326 (3.123 sec/step)\n",
            "I0619 18:45:46.957977 139649090267008 learning.py:507] global step 474: loss = 0.4117 (3.173 sec/step)\n",
            "I0619 18:45:50.057890 139649090267008 learning.py:507] global step 474: loss = 0.3281 (3.098 sec/step)\n",
            "I0619 18:45:53.195807 139649090267008 learning.py:507] global step 474: loss = 0.2527 (3.136 sec/step)\n",
            "I0619 18:45:56.229912 139649090267008 learning.py:507] global step 474: loss = 0.3849 (3.032 sec/step)\n",
            "I0619 18:45:59.313213 139649090267008 learning.py:507] global step 474: loss = 0.3721 (3.082 sec/step)\n",
            "I0619 18:46:02.439348 139649090267008 learning.py:507] global step 474: loss = 0.4936 (3.124 sec/step)\n",
            "I0619 18:46:05.551330 139649090267008 learning.py:507] global step 475: loss = 0.3924 (3.110 sec/step)\n",
            "I0619 18:46:08.619361 139649090267008 learning.py:507] global step 475: loss = 0.2889 (3.066 sec/step)\n",
            "I0619 18:46:11.703327 139649090267008 learning.py:507] global step 475: loss = 0.3094 (3.081 sec/step)\n",
            "I0619 18:46:14.787044 139649090267008 learning.py:507] global step 475: loss = 0.3329 (3.082 sec/step)\n",
            "I0619 18:46:17.847865 139649090267008 learning.py:507] global step 475: loss = 0.3501 (3.059 sec/step)\n",
            "I0619 18:46:21.040384 139649090267008 learning.py:507] global step 475: loss = 0.3826 (3.112 sec/step)\n",
            "I0619 18:46:26.331680 139649090267008 learning.py:507] global step 475: loss = 0.3639 (5.274 sec/step)\n",
            "I0619 18:46:26.968267 139646017689344 supervisor.py:1050] Recording summary at step 475.\n",
            "I0619 18:46:29.475309 139649090267008 learning.py:507] global step 475: loss = 0.4394 (3.141 sec/step)\n",
            "I0619 18:46:32.640143 139649090267008 learning.py:507] global step 476: loss = 0.3898 (3.163 sec/step)\n",
            "I0619 18:46:35.760254 139649090267008 learning.py:507] global step 476: loss = 0.3366 (3.118 sec/step)\n",
            "I0619 18:46:38.829271 139649090267008 learning.py:507] global step 476: loss = 0.3898 (3.067 sec/step)\n",
            "I0619 18:46:41.984451 139649090267008 learning.py:507] global step 476: loss = 0.3415 (3.153 sec/step)\n",
            "I0619 18:46:45.120231 139649090267008 learning.py:507] global step 476: loss = 0.3563 (3.134 sec/step)\n",
            "I0619 18:46:48.239227 139649090267008 learning.py:507] global step 476: loss = 0.3360 (3.117 sec/step)\n",
            "I0619 18:46:51.397454 139649090267008 learning.py:507] global step 476: loss = 0.3247 (3.156 sec/step)\n",
            "I0619 18:46:54.447564 139649090267008 learning.py:507] global step 476: loss = 0.3151 (3.048 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:46:57.552537 139649090267008 learning.py:507] global step 477: loss = 0.3421 (3.103 sec/step)\n",
            "I0619 18:47:00.630828 139649090267008 learning.py:507] global step 477: loss = 0.2805 (3.076 sec/step)\n",
            "I0619 18:47:03.704130 139649090267008 learning.py:507] global step 477: loss = 0.3056 (3.071 sec/step)\n",
            "I0619 18:47:06.853346 139649090267008 learning.py:507] global step 477: loss = 0.2998 (3.147 sec/step)\n",
            "I0619 18:47:09.947645 139649090267008 learning.py:507] global step 477: loss = 0.3075 (3.092 sec/step)\n",
            "I0619 18:47:13.093738 139649090267008 learning.py:507] global step 477: loss = 0.3836 (3.144 sec/step)\n",
            "I0619 18:47:16.246189 139649090267008 learning.py:507] global step 477: loss = 0.3532 (3.151 sec/step)\n",
            "I0619 18:47:19.345015 139649090267008 learning.py:507] global step 477: loss = 0.3530 (3.097 sec/step)\n",
            "I0619 18:47:22.430026 139649090267008 learning.py:507] global step 478: loss = 0.2853 (3.083 sec/step)\n",
            "I0619 18:47:25.523990 139649090267008 learning.py:507] global step 478: loss = 0.3612 (3.092 sec/step)\n",
            "I0619 18:47:28.611716 139649090267008 learning.py:507] global step 478: loss = 0.3904 (3.086 sec/step)\n",
            "I0619 18:47:31.704935 139649090267008 learning.py:507] global step 478: loss = 0.3188 (3.091 sec/step)\n",
            "I0619 18:47:34.773465 139649090267008 learning.py:507] global step 478: loss = 0.3419 (3.067 sec/step)\n",
            "I0619 18:47:37.866201 139649090267008 learning.py:507] global step 478: loss = 0.3574 (3.091 sec/step)\n",
            "I0619 18:47:40.923348 139649090267008 learning.py:507] global step 478: loss = 0.3353 (3.055 sec/step)\n",
            "I0619 18:47:43.978752 139649090267008 learning.py:507] global step 478: loss = 0.3629 (3.054 sec/step)\n",
            "I0619 18:47:47.086507 139649090267008 learning.py:507] global step 479: loss = 0.5214 (3.105 sec/step)\n",
            "I0619 18:47:50.090720 139649090267008 learning.py:507] global step 479: loss = 0.3252 (3.002 sec/step)\n",
            "I0619 18:47:53.446228 139649090267008 learning.py:507] global step 479: loss = 0.3430 (3.354 sec/step)\n",
            "I0619 18:47:56.522991 139649090267008 learning.py:507] global step 479: loss = 0.4233 (3.075 sec/step)\n",
            "I0619 18:47:59.624322 139649090267008 learning.py:507] global step 479: loss = 0.3062 (3.100 sec/step)\n",
            "I0619 18:48:02.684577 139649090267008 learning.py:507] global step 479: loss = 0.4891 (3.059 sec/step)\n",
            "I0619 18:48:05.766686 139649090267008 learning.py:507] global step 479: loss = 0.3197 (3.080 sec/step)\n",
            "I0619 18:48:08.814029 139649090267008 learning.py:507] global step 479: loss = 0.4488 (3.046 sec/step)\n",
            "I0619 18:48:12.141376 139649090267008 learning.py:507] global step 480: loss = 0.5086 (3.325 sec/step)\n",
            "I0619 18:48:15.289185 139649090267008 learning.py:507] global step 480: loss = 0.3325 (3.146 sec/step)\n",
            "I0619 18:48:18.394889 139649090267008 learning.py:507] global step 480: loss = 0.4529 (3.104 sec/step)\n",
            "I0619 18:48:22.063768 139649090267008 learning.py:507] global step 480: loss = 0.3505 (3.648 sec/step)\n",
            "I0619 18:48:26.196074 139646017689344 supervisor.py:1050] Recording summary at step 480.\n",
            "I0619 18:48:26.925718 139649090267008 learning.py:507] global step 480: loss = 0.2662 (4.860 sec/step)\n",
            "I0619 18:48:30.096105 139649090267008 learning.py:507] global step 480: loss = 0.3160 (3.169 sec/step)\n",
            "I0619 18:48:33.227681 139649090267008 learning.py:507] global step 480: loss = 0.3033 (3.130 sec/step)\n",
            "I0619 18:48:36.333209 139649090267008 learning.py:507] global step 480: loss = 0.3632 (3.104 sec/step)\n",
            "I0619 18:48:39.376000 139649090267008 learning.py:507] global step 481: loss = 0.4628 (3.040 sec/step)\n",
            "I0619 18:48:42.473460 139649090267008 learning.py:507] global step 481: loss = 0.3347 (3.096 sec/step)\n",
            "I0619 18:48:45.551798 139649090267008 learning.py:507] global step 481: loss = 0.3419 (3.076 sec/step)\n",
            "I0619 18:48:48.656814 139649090267008 learning.py:507] global step 481: loss = 0.4030 (3.103 sec/step)\n",
            "I0619 18:48:51.839374 139649090267008 learning.py:507] global step 481: loss = 0.3627 (3.181 sec/step)\n",
            "I0619 18:48:54.903884 139649090267008 learning.py:507] global step 481: loss = 0.3380 (3.063 sec/step)\n",
            "I0619 18:48:58.050103 139649090267008 learning.py:507] global step 481: loss = 0.4595 (3.144 sec/step)\n",
            "I0619 18:49:01.160084 139649090267008 learning.py:507] global step 481: loss = 0.3297 (3.108 sec/step)\n",
            "I0619 18:49:04.252186 139649090267008 learning.py:507] global step 482: loss = 0.2846 (3.090 sec/step)\n",
            "I0619 18:49:07.375988 139649090267008 learning.py:507] global step 482: loss = 0.3840 (3.122 sec/step)\n",
            "I0619 18:49:10.537691 139649090267008 learning.py:507] global step 482: loss = 0.3996 (3.160 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:49:13.631397 139649090267008 learning.py:507] global step 482: loss = 0.3603 (3.092 sec/step)\n",
            "I0619 18:49:16.827329 139649090267008 learning.py:507] global step 482: loss = 0.3005 (3.193 sec/step)\n",
            "I0619 18:49:19.851370 139649090267008 learning.py:507] global step 482: loss = 0.3787 (3.022 sec/step)\n",
            "I0619 18:49:23.114825 139649090267008 learning.py:507] global step 482: loss = 0.3791 (3.262 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:49:26.169395 139649090267008 learning.py:507] global step 482: loss = 0.3245 (3.053 sec/step)\n",
            "I0619 18:49:29.320133 139649090267008 learning.py:507] global step 483: loss = 0.4437 (3.148 sec/step)\n",
            "I0619 18:49:32.345292 139649090267008 learning.py:507] global step 483: loss = 0.3606 (3.023 sec/step)\n",
            "I0619 18:49:35.545297 139649090267008 learning.py:507] global step 483: loss = 0.3090 (3.198 sec/step)\n",
            "I0619 18:49:38.709494 139649090267008 learning.py:507] global step 483: loss = 0.4518 (3.161 sec/step)\n",
            "I0619 18:49:42.019337 139649090267008 learning.py:507] global step 483: loss = 0.3462 (3.308 sec/step)\n",
            "I0619 18:49:45.141378 139649090267008 learning.py:507] global step 483: loss = 0.3631 (3.120 sec/step)\n",
            "I0619 18:49:48.244817 139649090267008 learning.py:507] global step 483: loss = 0.3947 (3.102 sec/step)\n",
            "I0619 18:49:51.345210 139649090267008 learning.py:507] global step 483: loss = 0.3745 (3.099 sec/step)\n",
            "I0619 18:49:54.431678 139649090267008 learning.py:507] global step 484: loss = 0.3349 (3.083 sec/step)\n",
            "I0619 18:49:57.501597 139649090267008 learning.py:507] global step 484: loss = 0.4055 (3.068 sec/step)\n",
            "I0619 18:50:00.613166 139649090267008 learning.py:507] global step 484: loss = 0.3238 (3.110 sec/step)\n",
            "I0619 18:50:03.689270 139649090267008 learning.py:507] global step 484: loss = 0.2955 (3.074 sec/step)\n",
            "I0619 18:50:06.756534 139649090267008 learning.py:507] global step 484: loss = 0.3448 (3.065 sec/step)\n",
            "I0619 18:50:09.848631 139649090267008 learning.py:507] global step 484: loss = 0.3068 (3.090 sec/step)\n",
            "I0619 18:50:12.874010 139649090267008 learning.py:507] global step 484: loss = 0.3141 (3.024 sec/step)\n",
            "I0619 18:50:15.923770 139649090267008 learning.py:507] global step 484: loss = 0.3164 (3.048 sec/step)\n",
            "I0619 18:50:19.062645 139649090267008 learning.py:507] global step 485: loss = 0.3245 (3.137 sec/step)\n",
            "I0619 18:50:20.932512 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 18:50:25.360307 139649090267008 learning.py:507] global step 485: loss = 0.4016 (6.291 sec/step)\n",
            "I0619 18:50:27.096809 139646017689344 supervisor.py:1050] Recording summary at step 485.\n",
            "I0619 18:50:29.133468 139649090267008 learning.py:507] global step 485: loss = 0.2596 (3.771 sec/step)\n",
            "I0619 18:50:32.229980 139649090267008 learning.py:507] global step 485: loss = 0.3184 (3.094 sec/step)\n",
            "I0619 18:50:35.243217 139649090267008 learning.py:507] global step 485: loss = 0.4034 (3.011 sec/step)\n",
            "I0619 18:50:38.268418 139649090267008 learning.py:507] global step 485: loss = 0.3330 (3.023 sec/step)\n",
            "I0619 18:50:41.364634 139649090267008 learning.py:507] global step 485: loss = 0.3158 (3.094 sec/step)\n",
            "I0619 18:50:44.435228 139649090267008 learning.py:507] global step 485: loss = 0.3244 (3.069 sec/step)\n",
            "I0619 18:50:47.549737 139649090267008 learning.py:507] global step 486: loss = 0.3629 (3.112 sec/step)\n",
            "I0619 18:50:50.624989 139649090267008 learning.py:507] global step 486: loss = 0.3518 (3.073 sec/step)\n",
            "I0619 18:50:53.642904 139649090267008 learning.py:507] global step 486: loss = 0.3235 (3.016 sec/step)\n",
            "I0619 18:50:56.679156 139649090267008 learning.py:507] global step 486: loss = 0.3163 (3.034 sec/step)\n",
            "I0619 18:50:59.765434 139649090267008 learning.py:507] global step 486: loss = 0.2971 (3.085 sec/step)\n",
            "I0619 18:51:03.087222 139649090267008 learning.py:507] global step 486: loss = 0.3269 (3.320 sec/step)\n",
            "I0619 18:51:06.164595 139649090267008 learning.py:507] global step 486: loss = 0.4129 (3.076 sec/step)\n",
            "I0619 18:51:09.246188 139649090267008 learning.py:507] global step 486: loss = 0.3336 (3.080 sec/step)\n",
            "I0619 18:51:12.319260 139649090267008 learning.py:507] global step 487: loss = 0.3088 (3.070 sec/step)\n",
            "I0619 18:51:15.376242 139649090267008 learning.py:507] global step 487: loss = 0.4249 (3.055 sec/step)\n",
            "I0619 18:51:18.478994 139649090267008 learning.py:507] global step 487: loss = 0.2850 (3.101 sec/step)\n",
            "I0619 18:51:21.709076 139649090267008 learning.py:507] global step 487: loss = 0.2684 (3.228 sec/step)\n",
            "I0619 18:51:24.771996 139649090267008 learning.py:507] global step 487: loss = 0.3486 (3.061 sec/step)\n",
            "I0619 18:51:27.847568 139649090267008 learning.py:507] global step 487: loss = 0.3053 (3.074 sec/step)\n",
            "I0619 18:51:31.146524 139649090267008 learning.py:507] global step 487: loss = 0.3051 (3.297 sec/step)\n",
            "I0619 18:51:34.240477 139649090267008 learning.py:507] global step 487: loss = 0.3456 (3.092 sec/step)\n",
            "I0619 18:51:37.278956 139649090267008 learning.py:507] global step 488: loss = 0.3107 (3.036 sec/step)\n",
            "I0619 18:51:40.427700 139649090267008 learning.py:507] global step 488: loss = 0.3563 (3.146 sec/step)\n",
            "I0619 18:51:43.498346 139649090267008 learning.py:507] global step 488: loss = 0.3272 (3.069 sec/step)\n",
            "I0619 18:51:46.486371 139649090267008 learning.py:507] global step 488: loss = 0.3450 (2.986 sec/step)\n",
            "I0619 18:51:50.079539 139649090267008 learning.py:507] global step 488: loss = 0.3073 (3.591 sec/step)\n",
            "I0619 18:51:53.130188 139649090267008 learning.py:507] global step 488: loss = 0.3199 (3.049 sec/step)\n",
            "I0619 18:51:56.335729 139649090267008 learning.py:507] global step 488: loss = 0.3574 (3.204 sec/step)\n",
            "I0619 18:51:59.506325 139649090267008 learning.py:507] global step 488: loss = 0.4034 (3.169 sec/step)\n",
            "I0619 18:52:02.662655 139649090267008 learning.py:507] global step 489: loss = 0.3166 (3.154 sec/step)\n",
            "I0619 18:52:05.793242 139649090267008 learning.py:507] global step 489: loss = 0.4219 (3.129 sec/step)\n",
            "I0619 18:52:09.184762 139649090267008 learning.py:507] global step 489: loss = 0.3480 (3.389 sec/step)\n",
            "I0619 18:52:12.254280 139649090267008 learning.py:507] global step 489: loss = 0.3220 (3.067 sec/step)\n",
            "I0619 18:52:15.503254 139649090267008 learning.py:507] global step 489: loss = 0.4201 (3.247 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:52:18.727254 139649090267008 learning.py:507] global step 489: loss = 0.2863 (3.222 sec/step)\n",
            "I0619 18:52:22.969242 139649090267008 learning.py:507] global step 489: loss = 0.2852 (4.228 sec/step)\n",
            "I0619 18:52:26.348075 139646017689344 supervisor.py:1050] Recording summary at step 489.\n",
            "I0619 18:52:27.464301 139649090267008 learning.py:507] global step 489: loss = 0.3881 (4.479 sec/step)\n",
            "I0619 18:52:30.543389 139649090267008 learning.py:507] global step 490: loss = 0.2842 (3.077 sec/step)\n",
            "I0619 18:52:33.586891 139649090267008 learning.py:507] global step 490: loss = 0.3237 (3.042 sec/step)\n",
            "I0619 18:52:36.653877 139649090267008 learning.py:507] global step 490: loss = 0.2880 (3.065 sec/step)\n",
            "I0619 18:52:39.891429 139649090267008 learning.py:507] global step 490: loss = 0.3210 (3.236 sec/step)\n",
            "I0619 18:52:42.913984 139649090267008 learning.py:507] global step 490: loss = 0.3111 (3.021 sec/step)\n",
            "I0619 18:52:45.942164 139649090267008 learning.py:507] global step 490: loss = 0.3378 (3.026 sec/step)\n",
            "I0619 18:52:48.975791 139649090267008 learning.py:507] global step 490: loss = 0.5066 (3.032 sec/step)\n",
            "I0619 18:52:52.037431 139649090267008 learning.py:507] global step 490: loss = 0.3775 (3.060 sec/step)\n",
            "I0619 18:52:55.134576 139649090267008 learning.py:507] global step 491: loss = 0.4396 (3.094 sec/step)\n",
            "I0619 18:52:58.372343 139649090267008 learning.py:507] global step 491: loss = 0.3334 (3.236 sec/step)\n",
            "I0619 18:53:01.417159 139649090267008 learning.py:507] global step 491: loss = 0.3046 (3.043 sec/step)\n",
            "I0619 18:53:04.477313 139649090267008 learning.py:507] global step 491: loss = 0.3005 (3.058 sec/step)\n",
            "I0619 18:53:07.581352 139649090267008 learning.py:507] global step 491: loss = 0.3273 (3.102 sec/step)\n",
            "I0619 18:53:10.667918 139649090267008 learning.py:507] global step 491: loss = 0.3532 (3.085 sec/step)\n",
            "I0619 18:53:13.902150 139649090267008 learning.py:507] global step 491: loss = 0.4030 (3.232 sec/step)\n",
            "I0619 18:53:17.042577 139649090267008 learning.py:507] global step 491: loss = 0.3628 (3.139 sec/step)\n",
            "I0619 18:53:20.142758 139649090267008 learning.py:507] global step 492: loss = 0.4031 (3.098 sec/step)\n",
            "I0619 18:53:23.167183 139649090267008 learning.py:507] global step 492: loss = 0.3798 (3.022 sec/step)\n",
            "I0619 18:53:26.249020 139649090267008 learning.py:507] global step 492: loss = 0.3144 (3.080 sec/step)\n",
            "I0619 18:53:29.302046 139649090267008 learning.py:507] global step 492: loss = 0.3653 (3.051 sec/step)\n",
            "I0619 18:53:32.470808 139649090267008 learning.py:507] global step 492: loss = 0.2898 (3.167 sec/step)\n",
            "I0619 18:53:35.467764 139649090267008 learning.py:507] global step 492: loss = 0.2910 (2.995 sec/step)\n",
            "I0619 18:53:38.583672 139649090267008 learning.py:507] global step 492: loss = 0.3376 (3.114 sec/step)\n",
            "I0619 18:53:41.673815 139649090267008 learning.py:507] global step 492: loss = 0.4979 (3.088 sec/step)\n",
            "I0619 18:53:44.773515 139649090267008 learning.py:507] global step 493: loss = 0.5054 (3.097 sec/step)\n",
            "I0619 18:53:47.943897 139649090267008 learning.py:507] global step 493: loss = 0.2977 (3.169 sec/step)\n",
            "I0619 18:53:51.222474 139649090267008 learning.py:507] global step 493: loss = 0.3361 (3.277 sec/step)\n",
            "I0619 18:53:54.266280 139649090267008 learning.py:507] global step 493: loss = 0.4721 (3.042 sec/step)\n",
            "I0619 18:53:57.261538 139649090267008 learning.py:507] global step 493: loss = 0.3408 (2.993 sec/step)\n",
            "I0619 18:54:00.345057 139649090267008 learning.py:507] global step 493: loss = 0.3055 (3.082 sec/step)\n",
            "I0619 18:54:03.404912 139649090267008 learning.py:507] global step 493: loss = 0.3665 (3.058 sec/step)\n",
            "I0619 18:54:06.495090 139649090267008 learning.py:507] global step 493: loss = 0.3031 (3.088 sec/step)\n",
            "I0619 18:54:09.547494 139649090267008 learning.py:507] global step 494: loss = 0.3123 (3.050 sec/step)\n",
            "I0619 18:54:12.613025 139649090267008 learning.py:507] global step 494: loss = 0.3279 (3.064 sec/step)\n",
            "I0619 18:54:15.664563 139649090267008 learning.py:507] global step 494: loss = 0.4106 (3.050 sec/step)\n",
            "I0619 18:54:18.761222 139649090267008 learning.py:507] global step 494: loss = 0.3530 (3.095 sec/step)\n",
            "I0619 18:54:22.912854 139649090267008 learning.py:507] global step 494: loss = 0.2760 (4.140 sec/step)\n",
            "I0619 18:54:26.245439 139646017689344 supervisor.py:1050] Recording summary at step 494.\n",
            "I0619 18:54:27.285717 139649090267008 learning.py:507] global step 494: loss = 0.4111 (4.360 sec/step)\n",
            "I0619 18:54:30.459577 139649090267008 learning.py:507] global step 494: loss = 0.3172 (3.172 sec/step)\n",
            "I0619 18:54:33.558737 139649090267008 learning.py:507] global step 494: loss = 0.3478 (3.097 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:54:36.621410 139649090267008 learning.py:507] global step 495: loss = 0.3364 (3.060 sec/step)\n",
            "I0619 18:54:39.667983 139649090267008 learning.py:507] global step 495: loss = 0.3261 (3.045 sec/step)\n",
            "I0619 18:54:42.756236 139649090267008 learning.py:507] global step 495: loss = 0.4097 (3.084 sec/step)\n",
            "I0619 18:54:45.894019 139649090267008 learning.py:507] global step 495: loss = 0.3645 (3.136 sec/step)\n",
            "I0619 18:54:49.015427 139649090267008 learning.py:507] global step 495: loss = 0.3695 (3.120 sec/step)\n",
            "I0619 18:54:52.155020 139649090267008 learning.py:507] global step 495: loss = 0.3117 (3.136 sec/step)\n",
            "I0619 18:54:55.313107 139649090267008 learning.py:507] global step 495: loss = 0.3757 (3.156 sec/step)\n",
            "I0619 18:54:58.406142 139649090267008 learning.py:507] global step 495: loss = 0.4273 (3.091 sec/step)\n",
            "I0619 18:55:01.458289 139649090267008 learning.py:507] global step 496: loss = 0.3280 (3.050 sec/step)\n",
            "I0619 18:55:04.545704 139649090267008 learning.py:507] global step 496: loss = 0.2961 (3.086 sec/step)\n",
            "I0619 18:55:07.679747 139649090267008 learning.py:507] global step 496: loss = 0.3429 (3.132 sec/step)\n",
            "I0619 18:55:10.768465 139649090267008 learning.py:507] global step 496: loss = 0.3658 (3.087 sec/step)\n",
            "I0619 18:55:13.953625 139649090267008 learning.py:507] global step 496: loss = 0.3332 (3.183 sec/step)\n",
            "I0619 18:55:17.038491 139649090267008 learning.py:507] global step 496: loss = 0.4483 (3.083 sec/step)\n",
            "I0619 18:55:20.088008 139649090267008 learning.py:507] global step 496: loss = 0.3827 (3.048 sec/step)\n",
            "I0619 18:55:23.254434 139649090267008 learning.py:507] global step 496: loss = 0.2739 (3.165 sec/step)\n",
            "I0619 18:55:26.377413 139649090267008 learning.py:507] global step 497: loss = 0.3481 (3.121 sec/step)\n",
            "I0619 18:55:29.472853 139649090267008 learning.py:507] global step 497: loss = 0.3657 (3.094 sec/step)\n",
            "I0619 18:55:32.602741 139649090267008 learning.py:507] global step 497: loss = 0.4545 (3.128 sec/step)\n",
            "I0619 18:55:35.651600 139649090267008 learning.py:507] global step 497: loss = 0.3611 (3.047 sec/step)\n",
            "I0619 18:55:38.790527 139649090267008 learning.py:507] global step 497: loss = 0.2830 (3.137 sec/step)\n",
            "I0619 18:55:41.921183 139649090267008 learning.py:507] global step 497: loss = 0.3204 (3.129 sec/step)\n",
            "I0619 18:55:45.037802 139649090267008 learning.py:507] global step 497: loss = 0.3296 (3.115 sec/step)\n",
            "I0619 18:55:48.147875 139649090267008 learning.py:507] global step 497: loss = 0.3065 (3.108 sec/step)\n",
            "I0619 18:55:51.250329 139649090267008 learning.py:507] global step 498: loss = 0.3080 (3.100 sec/step)\n",
            "I0619 18:55:54.430123 139649090267008 learning.py:507] global step 498: loss = 0.3049 (3.178 sec/step)\n",
            "I0619 18:55:57.560149 139649090267008 learning.py:507] global step 498: loss = 0.2963 (3.128 sec/step)\n",
            "I0619 18:56:00.887322 139649090267008 learning.py:507] global step 498: loss = 0.3496 (3.325 sec/step)\n",
            "I0619 18:56:04.115162 139649090267008 learning.py:507] global step 498: loss = 0.4649 (3.226 sec/step)\n",
            "I0619 18:56:07.289230 139649090267008 learning.py:507] global step 498: loss = 0.3172 (3.172 sec/step)\n",
            "I0619 18:56:10.400995 139649090267008 learning.py:507] global step 498: loss = 0.3451 (3.110 sec/step)\n",
            "I0619 18:56:13.550212 139649090267008 learning.py:507] global step 498: loss = 0.3083 (3.148 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 18:56:16.891821 139649090267008 learning.py:507] global step 499: loss = 0.2931 (3.339 sec/step)\n",
            "I0619 18:56:20.292385 139649090267008 learning.py:507] global step 499: loss = 0.4098 (3.399 sec/step)\n",
            "I0619 18:56:25.648818 139649090267008 learning.py:507] global step 499: loss = 0.3553 (5.350 sec/step)\n",
            "I0619 18:56:26.657184 139646017689344 supervisor.py:1050] Recording summary at step 499.\n",
            "I0619 18:56:29.015121 139649090267008 learning.py:507] global step 499: loss = 0.3952 (3.363 sec/step)\n",
            "I0619 18:56:32.274730 139649090267008 learning.py:507] global step 499: loss = 0.3279 (3.258 sec/step)\n",
            "I0619 18:56:35.538765 139649090267008 learning.py:507] global step 499: loss = 0.2939 (3.262 sec/step)\n",
            "I0619 18:56:38.772213 139649090267008 learning.py:507] global step 499: loss = 0.2987 (3.232 sec/step)\n",
            "I0619 18:56:42.090809 139649090267008 learning.py:507] global step 499: loss = 0.3135 (3.317 sec/step)\n",
            "I0619 18:56:45.489697 139649090267008 learning.py:507] global step 500: loss = 0.3390 (3.395 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 18:56:48.776488 139649090267008 learning.py:507] global step 500: loss = 0.2967 (3.285 sec/step)\n",
            "I0619 18:56:52.023839 139649090267008 learning.py:507] global step 500: loss = 0.3305 (3.246 sec/step)\n",
            "I0619 18:56:55.363515 139649090267008 learning.py:507] global step 500: loss = 0.3098 (3.338 sec/step)\n",
            "I0619 18:56:58.708895 139649090267008 learning.py:507] global step 500: loss = 0.3791 (3.343 sec/step)\n",
            "I0619 18:57:01.942157 139649090267008 learning.py:507] global step 500: loss = 0.3727 (3.231 sec/step)\n",
            "I0619 18:57:05.193866 139649090267008 learning.py:507] global step 500: loss = 0.2614 (3.249 sec/step)\n",
            "I0619 18:57:08.351704 139649090267008 learning.py:507] global step 500: loss = 0.3202 (3.156 sec/step)\n",
            "I0619 18:57:11.627920 139649090267008 learning.py:507] global step 501: loss = 0.3656 (3.274 sec/step)\n",
            "I0619 18:57:14.776586 139649090267008 learning.py:507] global step 501: loss = 0.3589 (3.147 sec/step)\n",
            "I0619 18:57:17.941661 139649090267008 learning.py:507] global step 501: loss = 0.3172 (3.163 sec/step)\n",
            "I0619 18:57:21.051293 139649090267008 learning.py:507] global step 501: loss = 0.4269 (3.108 sec/step)\n",
            "I0619 18:57:24.310980 139649090267008 learning.py:507] global step 501: loss = 0.4623 (3.258 sec/step)\n",
            "I0619 18:57:27.547095 139649090267008 learning.py:507] global step 501: loss = 0.3317 (3.234 sec/step)\n",
            "I0619 18:57:30.756875 139649090267008 learning.py:507] global step 501: loss = 0.4139 (3.208 sec/step)\n",
            "I0619 18:57:33.918592 139649090267008 learning.py:507] global step 501: loss = 0.3307 (3.160 sec/step)\n",
            "I0619 18:57:37.173809 139649090267008 learning.py:507] global step 502: loss = 0.3641 (3.253 sec/step)\n",
            "I0619 18:57:40.428736 139649090267008 learning.py:507] global step 502: loss = 0.3555 (3.253 sec/step)\n",
            "I0619 18:57:43.805900 139649090267008 learning.py:507] global step 502: loss = 0.3076 (3.375 sec/step)\n",
            "I0619 18:57:47.149389 139649090267008 learning.py:507] global step 502: loss = 0.3706 (3.342 sec/step)\n",
            "I0619 18:57:50.498766 139649090267008 learning.py:507] global step 502: loss = 0.2802 (3.347 sec/step)\n",
            "I0619 18:57:53.814547 139649090267008 learning.py:507] global step 502: loss = 0.2825 (3.314 sec/step)\n",
            "I0619 18:57:57.087259 139649090267008 learning.py:507] global step 502: loss = 0.3323 (3.270 sec/step)\n",
            "I0619 18:58:00.245431 139649090267008 learning.py:507] global step 502: loss = 0.2937 (3.156 sec/step)\n",
            "I0619 18:58:03.329792 139649090267008 learning.py:507] global step 503: loss = 0.3380 (3.080 sec/step)\n",
            "I0619 18:58:06.532433 139649090267008 learning.py:507] global step 503: loss = 0.4084 (3.199 sec/step)\n",
            "I0619 18:58:09.892988 139649090267008 learning.py:507] global step 503: loss = 0.3153 (3.359 sec/step)\n",
            "I0619 18:58:13.287558 139649090267008 learning.py:507] global step 503: loss = 0.4077 (3.393 sec/step)\n",
            "I0619 18:58:16.442500 139649090267008 learning.py:507] global step 503: loss = 0.3074 (3.153 sec/step)\n",
            "I0619 18:58:19.591582 139649090267008 learning.py:507] global step 503: loss = 0.4101 (3.147 sec/step)\n",
            "I0619 18:58:24.494127 139649090267008 learning.py:507] global step 503: loss = 0.3254 (4.898 sec/step)\n",
            "I0619 18:58:27.267259 139646017689344 supervisor.py:1050] Recording summary at step 503.\n",
            "I0619 18:58:28.740097 139649090267008 learning.py:507] global step 503: loss = 0.3096 (4.244 sec/step)\n",
            "I0619 18:58:31.967536 139649090267008 learning.py:507] global step 504: loss = 0.3422 (3.225 sec/step)\n",
            "I0619 18:58:35.046705 139649090267008 learning.py:507] global step 504: loss = 0.3767 (3.077 sec/step)\n",
            "I0619 18:58:38.210475 139649090267008 learning.py:507] global step 504: loss = 0.3318 (3.162 sec/step)\n",
            "I0619 18:58:41.308268 139649090267008 learning.py:507] global step 504: loss = 0.3194 (3.096 sec/step)\n",
            "I0619 18:58:44.383590 139649090267008 learning.py:507] global step 504: loss = 0.3467 (3.074 sec/step)\n",
            "I0619 18:58:47.677815 139649090267008 learning.py:507] global step 504: loss = 0.3765 (3.292 sec/step)\n",
            "I0619 18:58:50.758537 139649090267008 learning.py:507] global step 504: loss = 0.2939 (3.079 sec/step)\n",
            "I0619 18:58:53.894003 139649090267008 learning.py:507] global step 504: loss = 0.3438 (3.134 sec/step)\n",
            "I0619 18:58:57.024218 139649090267008 learning.py:507] global step 505: loss = 0.2711 (3.128 sec/step)\n",
            "I0619 18:59:00.178494 139649090267008 learning.py:507] global step 505: loss = 0.3597 (3.152 sec/step)\n",
            "I0619 18:59:03.324326 139649090267008 learning.py:507] global step 505: loss = 0.2937 (3.144 sec/step)\n",
            "I0619 18:59:06.479349 139649090267008 learning.py:507] global step 505: loss = 0.3037 (3.153 sec/step)\n",
            "I0619 18:59:09.664618 139649090267008 learning.py:507] global step 505: loss = 0.3024 (3.182 sec/step)\n",
            "I0619 18:59:12.867076 139649090267008 learning.py:507] global step 505: loss = 0.3646 (3.196 sec/step)\n",
            "I0619 18:59:16.058530 139649090267008 learning.py:507] global step 505: loss = 0.3134 (3.189 sec/step)\n",
            "I0619 18:59:19.225016 139649090267008 learning.py:507] global step 505: loss = 0.3408 (3.164 sec/step)\n",
            "I0619 18:59:22.301197 139649090267008 learning.py:507] global step 506: loss = 0.2981 (3.074 sec/step)\n",
            "I0619 18:59:25.422883 139649090267008 learning.py:507] global step 506: loss = 0.4703 (3.120 sec/step)\n",
            "I0619 18:59:28.534851 139649090267008 learning.py:507] global step 506: loss = 0.4263 (3.110 sec/step)\n",
            "I0619 18:59:31.626506 139649090267008 learning.py:507] global step 506: loss = 0.3571 (3.090 sec/step)\n",
            "I0619 18:59:34.748234 139649090267008 learning.py:507] global step 506: loss = 0.2795 (3.120 sec/step)\n",
            "I0619 18:59:37.846190 139649090267008 learning.py:507] global step 506: loss = 0.3827 (3.096 sec/step)\n",
            "I0619 18:59:41.002736 139649090267008 learning.py:507] global step 506: loss = 0.3166 (3.155 sec/step)\n",
            "I0619 18:59:44.135009 139649090267008 learning.py:507] global step 506: loss = 0.3285 (3.130 sec/step)\n",
            "I0619 18:59:47.258008 139649090267008 learning.py:507] global step 507: loss = 0.3163 (3.120 sec/step)\n",
            "I0619 18:59:50.361330 139649090267008 learning.py:507] global step 507: loss = 0.4012 (3.101 sec/step)\n",
            "I0619 18:59:53.474402 139649090267008 learning.py:507] global step 507: loss = 0.3410 (3.111 sec/step)\n",
            "I0619 18:59:56.695820 139649090267008 learning.py:507] global step 507: loss = 0.3286 (3.220 sec/step)\n",
            "I0619 18:59:59.923798 139649090267008 learning.py:507] global step 507: loss = 0.3531 (3.226 sec/step)\n",
            "I0619 19:00:03.101655 139649090267008 learning.py:507] global step 507: loss = 0.3727 (3.176 sec/step)\n",
            "I0619 19:00:06.279881 139649090267008 learning.py:507] global step 507: loss = 0.2684 (3.176 sec/step)\n",
            "I0619 19:00:09.581121 139649090267008 learning.py:507] global step 507: loss = 0.3725 (3.299 sec/step)\n",
            "I0619 19:00:13.218335 139649090267008 learning.py:507] global step 508: loss = 0.3111 (3.635 sec/step)\n",
            "I0619 19:00:16.395110 139649090267008 learning.py:507] global step 508: loss = 0.3051 (3.175 sec/step)\n",
            "I0619 19:00:19.495014 139649090267008 learning.py:507] global step 508: loss = 0.3619 (3.098 sec/step)\n",
            "I0619 19:00:20.931313 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:00:25.860815 139649090267008 learning.py:507] global step 508: loss = 0.2576 (6.364 sec/step)\n",
            "I0619 19:00:28.146385 139646017689344 supervisor.py:1050] Recording summary at step 508.\n",
            "I0619 19:00:29.926419 139649090267008 learning.py:507] global step 508: loss = 0.3655 (4.063 sec/step)\n",
            "I0619 19:00:33.453738 139649090267008 learning.py:507] global step 508: loss = 0.4291 (3.525 sec/step)\n",
            "I0619 19:00:36.628098 139649090267008 learning.py:507] global step 508: loss = 0.4085 (3.172 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:00:39.802231 139649090267008 learning.py:507] global step 508: loss = 0.3068 (3.172 sec/step)\n",
            "I0619 19:00:43.440294 139649090267008 learning.py:507] global step 509: loss = 0.4434 (3.636 sec/step)\n",
            "I0619 19:00:46.512585 139649090267008 learning.py:507] global step 509: loss = 0.3524 (3.071 sec/step)\n",
            "I0619 19:00:50.345730 139649090267008 learning.py:507] global step 509: loss = 0.2974 (3.831 sec/step)\n",
            "I0619 19:00:53.564388 139649090267008 learning.py:507] global step 509: loss = 0.2694 (3.217 sec/step)\n",
            "I0619 19:00:56.768246 139649090267008 learning.py:507] global step 509: loss = 0.3146 (3.202 sec/step)\n",
            "I0619 19:00:59.994208 139649090267008 learning.py:507] global step 509: loss = 0.3048 (3.224 sec/step)\n",
            "I0619 19:01:03.579547 139649090267008 learning.py:507] global step 509: loss = 0.3632 (3.583 sec/step)\n",
            "I0619 19:01:06.732213 139649090267008 learning.py:507] global step 509: loss = 0.3164 (3.151 sec/step)\n",
            "I0619 19:01:10.205596 139649090267008 learning.py:507] global step 510: loss = 0.3462 (3.469 sec/step)\n",
            "I0619 19:01:13.348881 139649090267008 learning.py:507] global step 510: loss = 0.4018 (3.142 sec/step)\n",
            "I0619 19:01:16.507998 139649090267008 learning.py:507] global step 510: loss = 0.3461 (3.157 sec/step)\n",
            "I0619 19:01:19.729699 139649090267008 learning.py:507] global step 510: loss = 0.3190 (3.219 sec/step)\n",
            "I0619 19:01:22.901481 139649090267008 learning.py:507] global step 510: loss = 0.4196 (3.170 sec/step)\n",
            "I0619 19:01:26.062052 139649090267008 learning.py:507] global step 510: loss = 0.3682 (3.159 sec/step)\n",
            "I0619 19:01:29.188749 139649090267008 learning.py:507] global step 510: loss = 0.3514 (3.125 sec/step)\n",
            "I0619 19:01:32.344118 139649090267008 learning.py:507] global step 510: loss = 0.3473 (3.153 sec/step)\n",
            "I0619 19:01:35.512227 139649090267008 learning.py:507] global step 511: loss = 0.3418 (3.166 sec/step)\n",
            "I0619 19:01:38.665543 139649090267008 learning.py:507] global step 511: loss = 0.3093 (3.151 sec/step)\n",
            "I0619 19:01:41.752205 139649090267008 learning.py:507] global step 511: loss = 0.3096 (3.085 sec/step)\n",
            "I0619 19:01:44.911142 139649090267008 learning.py:507] global step 511: loss = 0.2871 (3.157 sec/step)\n",
            "I0619 19:01:48.151703 139649090267008 learning.py:507] global step 511: loss = 0.2844 (3.238 sec/step)\n",
            "I0619 19:01:51.340938 139649090267008 learning.py:507] global step 511: loss = 0.3530 (3.187 sec/step)\n",
            "I0619 19:01:54.477730 139649090267008 learning.py:507] global step 511: loss = 0.3595 (3.135 sec/step)\n",
            "I0619 19:01:57.693953 139649090267008 learning.py:507] global step 511: loss = 0.3259 (3.214 sec/step)\n",
            "I0619 19:02:00.812891 139649090267008 learning.py:507] global step 512: loss = 0.3617 (3.116 sec/step)\n",
            "I0619 19:02:04.011532 139649090267008 learning.py:507] global step 512: loss = 0.3316 (3.197 sec/step)\n",
            "I0619 19:02:07.146690 139649090267008 learning.py:507] global step 512: loss = 0.3378 (3.133 sec/step)\n",
            "I0619 19:02:10.316739 139649090267008 learning.py:507] global step 512: loss = 0.3793 (3.168 sec/step)\n",
            "I0619 19:02:13.450328 139649090267008 learning.py:507] global step 512: loss = 0.3686 (3.132 sec/step)\n",
            "I0619 19:02:16.683296 139649090267008 learning.py:507] global step 512: loss = 0.3017 (3.231 sec/step)\n",
            "I0619 19:02:19.863789 139649090267008 learning.py:507] global step 512: loss = 0.3431 (3.178 sec/step)\n",
            "I0619 19:02:25.479607 139649090267008 learning.py:507] global step 512: loss = 0.3581 (5.606 sec/step)\n",
            "I0619 19:02:26.865907 139646017689344 supervisor.py:1050] Recording summary at step 512.\n",
            "I0619 19:02:28.905409 139649090267008 learning.py:507] global step 513: loss = 0.3455 (3.422 sec/step)\n",
            "I0619 19:02:32.189742 139649090267008 learning.py:507] global step 513: loss = 0.3218 (3.282 sec/step)\n",
            "I0619 19:02:35.309400 139649090267008 learning.py:507] global step 513: loss = 0.4249 (3.118 sec/step)\n",
            "I0619 19:02:38.449075 139649090267008 learning.py:507] global step 513: loss = 0.3241 (3.138 sec/step)\n",
            "I0619 19:02:41.612107 139649090267008 learning.py:507] global step 513: loss = 0.3991 (3.161 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:02:45.055011 139649090267008 learning.py:507] global step 513: loss = 0.3152 (3.441 sec/step)\n",
            "I0619 19:02:48.156651 139649090267008 learning.py:507] global step 513: loss = 0.3139 (3.100 sec/step)\n",
            "I0619 19:02:51.473066 139649090267008 learning.py:507] global step 513: loss = 0.3138 (3.315 sec/step)\n",
            "I0619 19:02:54.813918 139649090267008 learning.py:507] global step 514: loss = 0.3025 (3.339 sec/step)\n",
            "I0619 19:02:58.044980 139649090267008 learning.py:507] global step 514: loss = 0.3094 (3.229 sec/step)\n",
            "I0619 19:03:01.184544 139649090267008 learning.py:507] global step 514: loss = 0.3716 (3.138 sec/step)\n",
            "I0619 19:03:04.274239 139649090267008 learning.py:507] global step 514: loss = 0.3329 (3.088 sec/step)\n",
            "I0619 19:03:07.350809 139649090267008 learning.py:507] global step 514: loss = 0.3618 (3.075 sec/step)\n",
            "I0619 19:03:10.529323 139649090267008 learning.py:507] global step 514: loss = 0.2937 (3.176 sec/step)\n",
            "I0619 19:03:14.121007 139649090267008 learning.py:507] global step 514: loss = 0.3262 (3.590 sec/step)\n",
            "I0619 19:03:17.341588 139649090267008 learning.py:507] global step 514: loss = 0.3681 (3.219 sec/step)\n",
            "I0619 19:03:20.504025 139649090267008 learning.py:507] global step 515: loss = 0.3047 (3.159 sec/step)\n",
            "I0619 19:03:23.643797 139649090267008 learning.py:507] global step 515: loss = 0.3393 (3.138 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:03:26.820502 139649090267008 learning.py:507] global step 515: loss = 0.3338 (3.175 sec/step)\n",
            "I0619 19:03:30.001932 139649090267008 learning.py:507] global step 515: loss = 0.3416 (3.180 sec/step)\n",
            "I0619 19:03:33.199487 139649090267008 learning.py:507] global step 515: loss = 0.2691 (3.196 sec/step)\n",
            "I0619 19:03:36.493337 139649090267008 learning.py:507] global step 515: loss = 0.3451 (3.292 sec/step)\n",
            "I0619 19:03:39.719754 139649090267008 learning.py:507] global step 515: loss = 0.2856 (3.225 sec/step)\n",
            "I0619 19:03:42.879550 139649090267008 learning.py:507] global step 515: loss = 0.3520 (3.158 sec/step)\n",
            "I0619 19:03:46.000498 139649090267008 learning.py:507] global step 516: loss = 0.2843 (3.119 sec/step)\n",
            "I0619 19:03:49.135263 139649090267008 learning.py:507] global step 516: loss = 0.3962 (3.133 sec/step)\n",
            "I0619 19:03:52.377803 139649090267008 learning.py:507] global step 516: loss = 0.3808 (3.241 sec/step)\n",
            "I0619 19:03:55.487529 139649090267008 learning.py:507] global step 516: loss = 0.2829 (3.108 sec/step)\n",
            "I0619 19:03:58.582696 139649090267008 learning.py:507] global step 516: loss = 0.3876 (3.093 sec/step)\n",
            "I0619 19:04:01.772084 139649090267008 learning.py:507] global step 516: loss = 0.3084 (3.188 sec/step)\n",
            "I0619 19:04:04.922797 139649090267008 learning.py:507] global step 516: loss = 0.3616 (3.149 sec/step)\n",
            "I0619 19:04:08.087330 139649090267008 learning.py:507] global step 516: loss = 0.3415 (3.163 sec/step)\n",
            "I0619 19:04:11.343988 139649090267008 learning.py:507] global step 517: loss = 0.2679 (3.254 sec/step)\n",
            "I0619 19:04:14.414365 139649090267008 learning.py:507] global step 517: loss = 0.3141 (3.069 sec/step)\n",
            "I0619 19:04:17.536501 139649090267008 learning.py:507] global step 517: loss = 0.3347 (3.120 sec/step)\n",
            "I0619 19:04:20.669019 139649090267008 learning.py:507] global step 517: loss = 0.2919 (3.131 sec/step)\n",
            "I0619 19:04:25.931041 139649090267008 learning.py:507] global step 517: loss = 0.2960 (5.257 sec/step)\n",
            "I0619 19:04:27.117270 139646017689344 supervisor.py:1050] Recording summary at step 517.\n",
            "I0619 19:04:29.260903 139649090267008 learning.py:507] global step 517: loss = 0.3086 (3.327 sec/step)\n",
            "I0619 19:04:32.448915 139649090267008 learning.py:507] global step 517: loss = 0.3718 (3.186 sec/step)\n",
            "I0619 19:04:35.585236 139649090267008 learning.py:507] global step 517: loss = 0.3434 (3.135 sec/step)\n",
            "I0619 19:04:38.747639 139649090267008 learning.py:507] global step 518: loss = 0.3161 (3.160 sec/step)\n",
            "I0619 19:04:41.887428 139649090267008 learning.py:507] global step 518: loss = 0.3313 (3.138 sec/step)\n",
            "I0619 19:04:44.960197 139649090267008 learning.py:507] global step 518: loss = 0.3501 (3.071 sec/step)\n",
            "I0619 19:04:48.171851 139649090267008 learning.py:507] global step 518: loss = 0.3336 (3.210 sec/step)\n",
            "I0619 19:04:51.344290 139649090267008 learning.py:507] global step 518: loss = 0.3574 (3.170 sec/step)\n",
            "I0619 19:04:54.482395 139649090267008 learning.py:507] global step 518: loss = 0.2623 (3.136 sec/step)\n",
            "I0619 19:04:57.856749 139649090267008 learning.py:507] global step 518: loss = 0.5159 (3.372 sec/step)\n",
            "I0619 19:05:01.050125 139649090267008 learning.py:507] global step 518: loss = 0.3001 (3.191 sec/step)\n",
            "I0619 19:05:04.290565 139649090267008 learning.py:507] global step 519: loss = 0.3805 (3.238 sec/step)\n",
            "I0619 19:05:07.498295 139649090267008 learning.py:507] global step 519: loss = 0.3516 (3.206 sec/step)\n",
            "I0619 19:05:10.677195 139649090267008 learning.py:507] global step 519: loss = 0.3930 (3.177 sec/step)\n",
            "I0619 19:05:13.976320 139649090267008 learning.py:507] global step 519: loss = 0.3324 (3.297 sec/step)\n",
            "I0619 19:05:17.089855 139649090267008 learning.py:507] global step 519: loss = 0.3082 (3.112 sec/step)\n",
            "I0619 19:05:20.183844 139649090267008 learning.py:507] global step 519: loss = 0.2456 (3.092 sec/step)\n",
            "I0619 19:05:23.336022 139649090267008 learning.py:507] global step 519: loss = 0.3550 (3.150 sec/step)\n",
            "I0619 19:05:26.436040 139649090267008 learning.py:507] global step 519: loss = 0.3290 (3.098 sec/step)\n",
            "I0619 19:05:29.508051 139649090267008 learning.py:507] global step 520: loss = 0.3252 (3.070 sec/step)\n",
            "I0619 19:05:32.633202 139649090267008 learning.py:507] global step 520: loss = 0.2942 (3.123 sec/step)\n",
            "I0619 19:05:35.780711 139649090267008 learning.py:507] global step 520: loss = 0.2760 (3.145 sec/step)\n",
            "I0619 19:05:38.928046 139649090267008 learning.py:507] global step 520: loss = 0.3276 (3.145 sec/step)\n",
            "I0619 19:05:42.040999 139649090267008 learning.py:507] global step 520: loss = 0.3686 (3.111 sec/step)\n",
            "I0619 19:05:45.213981 139649090267008 learning.py:507] global step 520: loss = 0.2942 (3.171 sec/step)\n",
            "I0619 19:05:48.397829 139649090267008 learning.py:507] global step 520: loss = 0.3996 (3.182 sec/step)\n",
            "I0619 19:05:51.540268 139649090267008 learning.py:507] global step 520: loss = 0.2792 (3.141 sec/step)\n",
            "I0619 19:05:54.842350 139649090267008 learning.py:507] global step 521: loss = 0.3305 (3.300 sec/step)\n",
            "I0619 19:05:58.041283 139649090267008 learning.py:507] global step 521: loss = 0.3099 (3.197 sec/step)\n",
            "I0619 19:06:01.231512 139649090267008 learning.py:507] global step 521: loss = 0.2971 (3.188 sec/step)\n",
            "I0619 19:06:04.311225 139649090267008 learning.py:507] global step 521: loss = 0.2792 (3.078 sec/step)\n",
            "I0619 19:06:07.413670 139649090267008 learning.py:507] global step 521: loss = 0.3946 (3.101 sec/step)\n",
            "I0619 19:06:10.532724 139649090267008 learning.py:507] global step 521: loss = 0.4198 (3.117 sec/step)\n",
            "I0619 19:06:13.738026 139649090267008 learning.py:507] global step 521: loss = 0.4988 (3.203 sec/step)\n",
            "I0619 19:06:16.872293 139649090267008 learning.py:507] global step 521: loss = 0.3784 (3.132 sec/step)\n",
            "I0619 19:06:20.114794 139649090267008 learning.py:507] global step 522: loss = 0.3930 (3.240 sec/step)\n",
            "I0619 19:06:25.511332 139649090267008 learning.py:507] global step 522: loss = 0.2802 (5.388 sec/step)\n",
            "I0619 19:06:26.665338 139646017689344 supervisor.py:1050] Recording summary at step 522.\n",
            "I0619 19:06:28.792468 139649090267008 learning.py:507] global step 522: loss = 0.2599 (3.278 sec/step)\n",
            "I0619 19:06:31.968233 139649090267008 learning.py:507] global step 522: loss = 0.2838 (3.174 sec/step)\n",
            "I0619 19:06:35.102659 139649090267008 learning.py:507] global step 522: loss = 0.3432 (3.133 sec/step)\n",
            "I0619 19:06:38.183908 139649090267008 learning.py:507] global step 522: loss = 0.3083 (3.079 sec/step)\n",
            "I0619 19:06:41.248304 139649090267008 learning.py:507] global step 522: loss = 0.2801 (3.062 sec/step)\n",
            "I0619 19:06:44.416160 139649090267008 learning.py:507] global step 522: loss = 0.3921 (3.166 sec/step)\n",
            "I0619 19:06:47.535743 139649090267008 learning.py:507] global step 523: loss = 0.3572 (3.116 sec/step)\n",
            "I0619 19:06:50.661845 139649090267008 learning.py:507] global step 523: loss = 0.2904 (3.124 sec/step)\n",
            "I0619 19:06:53.810815 139649090267008 learning.py:507] global step 523: loss = 0.3505 (3.147 sec/step)\n",
            "I0619 19:06:56.975712 139649090267008 learning.py:507] global step 523: loss = 0.3389 (3.163 sec/step)\n",
            "I0619 19:07:00.102821 139649090267008 learning.py:507] global step 523: loss = 0.3519 (3.125 sec/step)\n",
            "I0619 19:07:03.366596 139649090267008 learning.py:507] global step 523: loss = 0.3570 (3.262 sec/step)\n",
            "I0619 19:07:06.480684 139649090267008 learning.py:507] global step 523: loss = 0.4177 (3.112 sec/step)\n",
            "I0619 19:07:09.595121 139649090267008 learning.py:507] global step 523: loss = 0.3404 (3.113 sec/step)\n",
            "I0619 19:07:12.778625 139649090267008 learning.py:507] global step 524: loss = 0.3967 (3.181 sec/step)\n",
            "I0619 19:07:15.928513 139649090267008 learning.py:507] global step 524: loss = 0.2718 (3.148 sec/step)\n",
            "I0619 19:07:19.116031 139649090267008 learning.py:507] global step 524: loss = 0.3160 (3.185 sec/step)\n",
            "I0619 19:07:22.214010 139649090267008 learning.py:507] global step 524: loss = 0.2855 (3.096 sec/step)\n",
            "I0619 19:07:25.341867 139649090267008 learning.py:507] global step 524: loss = 0.2812 (3.126 sec/step)\n",
            "I0619 19:07:28.550988 139649090267008 learning.py:507] global step 524: loss = 0.2985 (3.207 sec/step)\n",
            "I0619 19:07:31.603177 139649090267008 learning.py:507] global step 524: loss = 0.2859 (3.050 sec/step)\n",
            "I0619 19:07:34.713829 139649090267008 learning.py:507] global step 524: loss = 0.3536 (3.109 sec/step)\n",
            "I0619 19:07:37.849005 139649090267008 learning.py:507] global step 525: loss = 0.2893 (3.133 sec/step)\n",
            "I0619 19:07:40.940636 139649090267008 learning.py:507] global step 525: loss = 0.2971 (3.090 sec/step)\n",
            "I0619 19:07:44.049169 139649090267008 learning.py:507] global step 525: loss = 0.2931 (3.107 sec/step)\n",
            "I0619 19:07:47.108376 139649090267008 learning.py:507] global step 525: loss = 0.2675 (3.057 sec/step)\n",
            "I0619 19:07:50.222408 139649090267008 learning.py:507] global step 525: loss = 0.2780 (3.112 sec/step)\n",
            "I0619 19:07:53.309789 139649090267008 learning.py:507] global step 525: loss = 0.3021 (3.085 sec/step)\n",
            "I0619 19:07:56.401686 139649090267008 learning.py:507] global step 525: loss = 0.2679 (3.090 sec/step)\n",
            "I0619 19:07:59.576986 139649090267008 learning.py:507] global step 525: loss = 0.4040 (3.174 sec/step)\n",
            "I0619 19:08:02.939425 139649090267008 learning.py:507] global step 526: loss = 0.4040 (3.359 sec/step)\n",
            "I0619 19:08:06.049803 139649090267008 learning.py:507] global step 526: loss = 0.3368 (3.109 sec/step)\n",
            "I0619 19:08:09.183089 139649090267008 learning.py:507] global step 526: loss = 0.3755 (3.132 sec/step)\n",
            "I0619 19:08:12.262568 139649090267008 learning.py:507] global step 526: loss = 0.3486 (3.077 sec/step)\n",
            "I0619 19:08:15.292717 139649090267008 learning.py:507] global step 526: loss = 0.3557 (3.028 sec/step)\n",
            "I0619 19:08:18.343784 139649090267008 learning.py:507] global step 526: loss = 0.3215 (3.049 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:08:22.612556 139649090267008 learning.py:507] global step 526: loss = 0.3850 (4.228 sec/step)\n",
            "I0619 19:08:26.378792 139646017689344 supervisor.py:1050] Recording summary at step 526.\n",
            "I0619 19:08:27.348452 139649090267008 learning.py:507] global step 526: loss = 0.4589 (4.734 sec/step)\n",
            "I0619 19:08:30.392118 139649090267008 learning.py:507] global step 527: loss = 0.3308 (3.041 sec/step)\n",
            "I0619 19:08:33.455415 139649090267008 learning.py:507] global step 527: loss = 0.3574 (3.062 sec/step)\n",
            "I0619 19:08:36.583798 139649090267008 learning.py:507] global step 527: loss = 0.4347 (3.126 sec/step)\n",
            "I0619 19:08:39.746371 139649090267008 learning.py:507] global step 527: loss = 0.3784 (3.161 sec/step)\n",
            "I0619 19:08:42.834572 139649090267008 learning.py:507] global step 527: loss = 0.3181 (3.086 sec/step)\n",
            "I0619 19:08:46.089167 139649090267008 learning.py:507] global step 527: loss = 0.3197 (3.253 sec/step)\n",
            "I0619 19:08:49.181437 139649090267008 learning.py:507] global step 527: loss = 0.2877 (3.091 sec/step)\n",
            "I0619 19:08:52.459432 139649090267008 learning.py:507] global step 527: loss = 0.3147 (3.276 sec/step)\n",
            "I0619 19:08:55.553833 139649090267008 learning.py:507] global step 528: loss = 0.2889 (3.092 sec/step)\n",
            "I0619 19:08:58.751768 139649090267008 learning.py:507] global step 528: loss = 0.3343 (3.196 sec/step)\n",
            "I0619 19:09:01.912561 139649090267008 learning.py:507] global step 528: loss = 0.3185 (3.159 sec/step)\n",
            "I0619 19:09:05.001430 139649090267008 learning.py:507] global step 528: loss = 0.3425 (3.087 sec/step)\n",
            "I0619 19:09:08.126111 139649090267008 learning.py:507] global step 528: loss = 0.4283 (3.123 sec/step)\n",
            "I0619 19:09:11.607795 139649090267008 learning.py:507] global step 528: loss = 0.2363 (3.480 sec/step)\n",
            "I0619 19:09:14.854795 139649090267008 learning.py:507] global step 528: loss = 0.3548 (3.245 sec/step)\n",
            "I0619 19:09:17.979495 139649090267008 learning.py:507] global step 528: loss = 0.3125 (3.123 sec/step)\n",
            "I0619 19:09:21.145615 139649090267008 learning.py:507] global step 529: loss = 0.2979 (3.164 sec/step)\n",
            "I0619 19:09:24.341309 139649090267008 learning.py:507] global step 529: loss = 0.3172 (3.194 sec/step)\n",
            "I0619 19:09:27.534112 139649090267008 learning.py:507] global step 529: loss = 0.2628 (3.191 sec/step)\n",
            "I0619 19:09:30.858109 139649090267008 learning.py:507] global step 529: loss = 0.3021 (3.322 sec/step)\n",
            "I0619 19:09:34.293744 139649090267008 learning.py:507] global step 529: loss = 0.2555 (3.434 sec/step)\n",
            "I0619 19:09:37.556151 139649090267008 learning.py:507] global step 529: loss = 0.3326 (3.261 sec/step)\n",
            "I0619 19:09:40.854886 139649090267008 learning.py:507] global step 529: loss = 0.5106 (3.297 sec/step)\n",
            "I0619 19:09:44.227412 139649090267008 learning.py:507] global step 529: loss = 0.4262 (3.371 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:09:47.457241 139649090267008 learning.py:507] global step 530: loss = 0.3290 (3.228 sec/step)\n",
            "I0619 19:09:50.706540 139649090267008 learning.py:507] global step 530: loss = 0.3614 (3.247 sec/step)\n",
            "I0619 19:09:54.100671 139649090267008 learning.py:507] global step 530: loss = 0.3078 (3.392 sec/step)\n",
            "I0619 19:09:57.301614 139649090267008 learning.py:507] global step 530: loss = 0.3370 (3.199 sec/step)\n",
            "I0619 19:10:00.591121 139649090267008 learning.py:507] global step 530: loss = 0.3419 (3.287 sec/step)\n",
            "I0619 19:10:03.797085 139649090267008 learning.py:507] global step 530: loss = 0.3954 (3.204 sec/step)\n",
            "I0619 19:10:06.997685 139649090267008 learning.py:507] global step 530: loss = 0.3436 (3.199 sec/step)\n",
            "I0619 19:10:10.177607 139649090267008 learning.py:507] global step 530: loss = 0.3869 (3.178 sec/step)\n",
            "I0619 19:10:13.522017 139649090267008 learning.py:507] global step 531: loss = 0.2919 (3.342 sec/step)\n",
            "I0619 19:10:16.770079 139649090267008 learning.py:507] global step 531: loss = 0.4354 (3.246 sec/step)\n",
            "I0619 19:10:20.140201 139649090267008 learning.py:507] global step 531: loss = 0.4748 (3.368 sec/step)\n",
            "I0619 19:10:20.931447 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:10:27.176098 139649090267008 learning.py:507] global step 531: loss = 0.3475 (7.030 sec/step)\n",
            "I0619 19:10:28.256711 139646017689344 supervisor.py:1050] Recording summary at step 531.\n",
            "I0619 19:10:30.754484 139649090267008 learning.py:507] global step 531: loss = 0.3388 (3.576 sec/step)\n",
            "I0619 19:10:33.992949 139649090267008 learning.py:507] global step 531: loss = 0.3168 (3.237 sec/step)\n",
            "I0619 19:10:37.134370 139649090267008 learning.py:507] global step 531: loss = 0.3043 (3.139 sec/step)\n",
            "I0619 19:10:40.198258 139649090267008 learning.py:507] global step 531: loss = 0.3153 (3.062 sec/step)\n",
            "I0619 19:10:43.268904 139649090267008 learning.py:507] global step 532: loss = 0.3302 (3.068 sec/step)\n",
            "I0619 19:10:46.341216 139649090267008 learning.py:507] global step 532: loss = 0.3813 (3.070 sec/step)\n",
            "I0619 19:10:49.444378 139649090267008 learning.py:507] global step 532: loss = 0.3889 (3.101 sec/step)\n",
            "I0619 19:10:52.504824 139649090267008 learning.py:507] global step 532: loss = 0.3452 (3.059 sec/step)\n",
            "I0619 19:10:55.653291 139649090267008 learning.py:507] global step 532: loss = 0.2894 (3.147 sec/step)\n",
            "I0619 19:10:58.704127 139649090267008 learning.py:507] global step 532: loss = 0.2750 (3.049 sec/step)\n",
            "I0619 19:11:01.841560 139649090267008 learning.py:507] global step 532: loss = 0.3465 (3.135 sec/step)\n",
            "I0619 19:11:04.865483 139649090267008 learning.py:507] global step 532: loss = 0.3564 (3.022 sec/step)\n",
            "I0619 19:11:07.977787 139649090267008 learning.py:507] global step 533: loss = 0.2845 (3.110 sec/step)\n",
            "I0619 19:11:11.057132 139649090267008 learning.py:507] global step 533: loss = 0.3832 (3.077 sec/step)\n",
            "I0619 19:11:14.106870 139649090267008 learning.py:507] global step 533: loss = 0.3059 (3.048 sec/step)\n",
            "I0619 19:11:17.294782 139649090267008 learning.py:507] global step 533: loss = 0.3248 (3.186 sec/step)\n",
            "I0619 19:11:20.389038 139649090267008 learning.py:507] global step 533: loss = 0.3473 (3.093 sec/step)\n",
            "I0619 19:11:23.475985 139649090267008 learning.py:507] global step 533: loss = 0.2683 (3.085 sec/step)\n",
            "I0619 19:11:26.532233 139649090267008 learning.py:507] global step 533: loss = 0.3623 (3.054 sec/step)\n",
            "I0619 19:11:29.585453 139649090267008 learning.py:507] global step 533: loss = 0.3868 (3.051 sec/step)\n",
            "I0619 19:11:32.691537 139649090267008 learning.py:507] global step 534: loss = 0.4793 (3.104 sec/step)\n",
            "I0619 19:11:35.743955 139649090267008 learning.py:507] global step 534: loss = 0.3006 (3.051 sec/step)\n",
            "I0619 19:11:38.946409 139649090267008 learning.py:507] global step 534: loss = 0.3149 (3.200 sec/step)\n",
            "I0619 19:11:42.029361 139649090267008 learning.py:507] global step 534: loss = 0.3715 (3.081 sec/step)\n",
            "I0619 19:11:45.134009 139649090267008 learning.py:507] global step 534: loss = 0.3720 (3.103 sec/step)\n",
            "I0619 19:11:48.167722 139649090267008 learning.py:507] global step 534: loss = 0.3879 (3.032 sec/step)\n",
            "I0619 19:11:51.205680 139649090267008 learning.py:507] global step 534: loss = 0.3542 (3.036 sec/step)\n",
            "I0619 19:11:54.173893 139649090267008 learning.py:507] global step 534: loss = 0.3254 (2.966 sec/step)\n",
            "I0619 19:11:57.501534 139649090267008 learning.py:507] global step 535: loss = 0.3447 (3.325 sec/step)\n",
            "I0619 19:12:00.758313 139649090267008 learning.py:507] global step 535: loss = 0.3645 (3.255 sec/step)\n",
            "I0619 19:12:03.879174 139649090267008 learning.py:507] global step 535: loss = 0.3019 (3.119 sec/step)\n",
            "I0619 19:12:06.942826 139649090267008 learning.py:507] global step 535: loss = 0.4620 (3.062 sec/step)\n",
            "I0619 19:12:10.054194 139649090267008 learning.py:507] global step 535: loss = 0.2466 (3.110 sec/step)\n",
            "I0619 19:12:13.120404 139649090267008 learning.py:507] global step 535: loss = 0.2975 (3.064 sec/step)\n",
            "I0619 19:12:16.175107 139649090267008 learning.py:507] global step 535: loss = 0.2966 (3.053 sec/step)\n",
            "I0619 19:12:19.397305 139649090267008 learning.py:507] global step 535: loss = 0.2506 (3.220 sec/step)\n",
            "I0619 19:12:24.483655 139649090267008 learning.py:507] global step 536: loss = 0.2654 (5.081 sec/step)\n",
            "I0619 19:12:25.902433 139646017689344 supervisor.py:1050] Recording summary at step 536.\n",
            "I0619 19:12:27.953391 139649090267008 learning.py:507] global step 536: loss = 0.3136 (3.463 sec/step)\n",
            "I0619 19:12:31.053622 139649090267008 learning.py:507] global step 536: loss = 0.3109 (3.098 sec/step)\n",
            "I0619 19:12:34.119619 139649090267008 learning.py:507] global step 536: loss = 0.2799 (3.064 sec/step)\n",
            "I0619 19:12:37.246139 139649090267008 learning.py:507] global step 536: loss = 0.3711 (3.125 sec/step)\n",
            "I0619 19:12:40.381943 139649090267008 learning.py:507] global step 536: loss = 0.3334 (3.134 sec/step)\n",
            "I0619 19:12:43.719236 139649090267008 learning.py:507] global step 536: loss = 0.5173 (3.336 sec/step)\n",
            "I0619 19:12:46.812077 139649090267008 learning.py:507] global step 536: loss = 0.4609 (3.091 sec/step)\n",
            "I0619 19:12:49.892514 139649090267008 learning.py:507] global step 537: loss = 0.3834 (3.078 sec/step)\n",
            "I0619 19:12:52.936283 139649090267008 learning.py:507] global step 537: loss = 0.3746 (3.042 sec/step)\n",
            "I0619 19:12:56.049957 139649090267008 learning.py:507] global step 537: loss = 0.3497 (3.112 sec/step)\n",
            "I0619 19:12:59.194563 139649090267008 learning.py:507] global step 537: loss = 0.2744 (3.142 sec/step)\n",
            "I0619 19:13:02.707448 139649090267008 learning.py:507] global step 537: loss = 0.3355 (3.511 sec/step)\n",
            "I0619 19:13:05.740321 139649090267008 learning.py:507] global step 537: loss = 0.2826 (3.031 sec/step)\n",
            "I0619 19:13:08.846008 139649090267008 learning.py:507] global step 537: loss = 0.3912 (3.104 sec/step)\n",
            "I0619 19:13:12.482291 139649090267008 learning.py:507] global step 537: loss = 0.2679 (3.634 sec/step)\n",
            "I0619 19:13:15.571002 139649090267008 learning.py:507] global step 538: loss = 0.3061 (3.087 sec/step)\n",
            "I0619 19:13:18.597419 139649090267008 learning.py:507] global step 538: loss = 0.2569 (3.024 sec/step)\n",
            "I0619 19:13:21.709519 139649090267008 learning.py:507] global step 538: loss = 0.4181 (3.109 sec/step)\n",
            "I0619 19:13:24.808499 139649090267008 learning.py:507] global step 538: loss = 0.3167 (3.097 sec/step)\n",
            "I0619 19:13:27.874723 139649090267008 learning.py:507] global step 538: loss = 0.3238 (3.065 sec/step)\n",
            "I0619 19:13:31.569028 139649090267008 learning.py:507] global step 538: loss = 0.3542 (3.693 sec/step)\n",
            "I0619 19:13:34.674816 139649090267008 learning.py:507] global step 538: loss = 0.2838 (3.104 sec/step)\n",
            "I0619 19:13:37.830657 139649090267008 learning.py:507] global step 538: loss = 0.3151 (3.154 sec/step)\n",
            "I0619 19:13:40.878100 139649090267008 learning.py:507] global step 539: loss = 0.3318 (3.046 sec/step)\n",
            "I0619 19:13:43.998734 139649090267008 learning.py:507] global step 539: loss = 0.3425 (3.119 sec/step)\n",
            "I0619 19:13:47.119143 139649090267008 learning.py:507] global step 539: loss = 0.2705 (3.118 sec/step)\n",
            "I0619 19:13:50.383840 139649090267008 learning.py:507] global step 539: loss = 0.3242 (3.263 sec/step)\n",
            "I0619 19:13:53.423042 139649090267008 learning.py:507] global step 539: loss = 0.3104 (3.037 sec/step)\n",
            "I0619 19:13:56.488869 139649090267008 learning.py:507] global step 539: loss = 0.4244 (3.064 sec/step)\n",
            "I0619 19:13:59.632900 139649090267008 learning.py:507] global step 539: loss = 0.3018 (3.142 sec/step)\n",
            "I0619 19:14:02.728347 139649090267008 learning.py:507] global step 539: loss = 0.2941 (3.094 sec/step)\n",
            "I0619 19:14:05.839328 139649090267008 learning.py:507] global step 540: loss = 0.2989 (3.108 sec/step)\n",
            "I0619 19:14:09.074162 139649090267008 learning.py:507] global step 540: loss = 0.3312 (3.233 sec/step)\n",
            "I0619 19:14:12.106032 139649090267008 learning.py:507] global step 540: loss = 0.3108 (3.030 sec/step)\n",
            "I0619 19:14:15.185335 139649090267008 learning.py:507] global step 540: loss = 0.3007 (3.078 sec/step)\n",
            "I0619 19:14:18.303449 139649090267008 learning.py:507] global step 540: loss = 0.3589 (3.116 sec/step)\n",
            "I0619 19:14:21.753024 139649090267008 learning.py:507] global step 540: loss = 0.2926 (3.172 sec/step)\n",
            "I0619 19:14:26.068942 139646017689344 supervisor.py:1050] Recording summary at step 540.\n",
            "I0619 19:14:26.856534 139649090267008 learning.py:507] global step 540: loss = 0.3649 (4.899 sec/step)\n",
            "I0619 19:14:29.938038 139649090267008 learning.py:507] global step 540: loss = 0.3783 (3.080 sec/step)\n",
            "I0619 19:14:33.029676 139649090267008 learning.py:507] global step 541: loss = 0.3314 (3.089 sec/step)\n",
            "I0619 19:14:36.105459 139649090267008 learning.py:507] global step 541: loss = 0.2998 (3.074 sec/step)\n",
            "I0619 19:14:39.202119 139649090267008 learning.py:507] global step 541: loss = 0.2671 (3.095 sec/step)\n",
            "I0619 19:14:42.282769 139649090267008 learning.py:507] global step 541: loss = 0.3293 (3.079 sec/step)\n",
            "I0619 19:14:45.465589 139649090267008 learning.py:507] global step 541: loss = 0.3720 (3.181 sec/step)\n",
            "I0619 19:14:48.573984 139649090267008 learning.py:507] global step 541: loss = 0.2841 (3.106 sec/step)\n",
            "I0619 19:14:51.645983 139649090267008 learning.py:507] global step 541: loss = 0.3074 (3.070 sec/step)\n",
            "I0619 19:14:54.687569 139649090267008 learning.py:507] global step 541: loss = 0.2955 (3.040 sec/step)\n",
            "I0619 19:14:57.732660 139649090267008 learning.py:507] global step 542: loss = 0.2639 (3.042 sec/step)\n",
            "I0619 19:15:00.815137 139649090267008 learning.py:507] global step 542: loss = 0.2952 (3.081 sec/step)\n",
            "I0619 19:15:03.959100 139649090267008 learning.py:507] global step 542: loss = 0.3199 (3.142 sec/step)\n",
            "I0619 19:15:07.017698 139649090267008 learning.py:507] global step 542: loss = 0.2875 (3.057 sec/step)\n",
            "I0619 19:15:10.061527 139649090267008 learning.py:507] global step 542: loss = 0.3056 (3.042 sec/step)\n",
            "I0619 19:15:13.066006 139649090267008 learning.py:507] global step 542: loss = 0.3523 (3.002 sec/step)\n",
            "I0619 19:15:16.193303 139649090267008 learning.py:507] global step 542: loss = 0.3306 (3.125 sec/step)\n",
            "I0619 19:15:19.291599 139649090267008 learning.py:507] global step 542: loss = 0.2861 (3.096 sec/step)\n",
            "I0619 19:15:22.507212 139649090267008 learning.py:507] global step 543: loss = 0.3677 (3.213 sec/step)\n",
            "I0619 19:15:25.606507 139649090267008 learning.py:507] global step 543: loss = 0.3105 (3.097 sec/step)\n",
            "I0619 19:15:28.754923 139649090267008 learning.py:507] global step 543: loss = 0.2980 (3.147 sec/step)\n",
            "I0619 19:15:31.905401 139649090267008 learning.py:507] global step 543: loss = 0.3147 (3.149 sec/step)\n",
            "I0619 19:15:34.997690 139649090267008 learning.py:507] global step 543: loss = 0.3362 (3.090 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:15:38.407236 139649090267008 learning.py:507] global step 543: loss = 0.3381 (3.407 sec/step)\n",
            "I0619 19:15:41.443765 139649090267008 learning.py:507] global step 543: loss = 0.2838 (3.035 sec/step)\n",
            "I0619 19:15:44.473504 139649090267008 learning.py:507] global step 543: loss = 0.3436 (3.028 sec/step)\n",
            "I0619 19:15:47.581955 139649090267008 learning.py:507] global step 544: loss = 0.2696 (3.106 sec/step)\n",
            "I0619 19:15:50.637150 139649090267008 learning.py:507] global step 544: loss = 0.2827 (3.053 sec/step)\n",
            "I0619 19:15:53.729373 139649090267008 learning.py:507] global step 544: loss = 0.2504 (3.090 sec/step)\n",
            "I0619 19:15:56.920754 139649090267008 learning.py:507] global step 544: loss = 0.3368 (3.190 sec/step)\n",
            "I0619 19:15:59.930326 139649090267008 learning.py:507] global step 544: loss = 0.3070 (3.008 sec/step)\n",
            "I0619 19:16:02.946595 139649090267008 learning.py:507] global step 544: loss = 0.3749 (3.015 sec/step)\n",
            "I0619 19:16:06.057171 139649090267008 learning.py:507] global step 544: loss = 0.3747 (3.109 sec/step)\n",
            "I0619 19:16:09.107841 139649090267008 learning.py:507] global step 544: loss = 0.2585 (3.049 sec/step)\n",
            "I0619 19:16:12.275573 139649090267008 learning.py:507] global step 545: loss = 0.3972 (3.166 sec/step)\n",
            "I0619 19:16:15.327725 139649090267008 learning.py:507] global step 545: loss = 0.3245 (3.050 sec/step)\n",
            "I0619 19:16:18.412467 139649090267008 learning.py:507] global step 545: loss = 0.3283 (3.083 sec/step)\n",
            "I0619 19:16:21.594710 139649090267008 learning.py:507] global step 545: loss = 0.2909 (3.139 sec/step)\n",
            "I0619 19:16:26.208239 139646017689344 supervisor.py:1050] Recording summary at step 545.\n",
            "I0619 19:16:26.807717 139649090267008 learning.py:507] global step 545: loss = 0.3241 (5.085 sec/step)\n",
            "I0619 19:16:29.909424 139649090267008 learning.py:507] global step 545: loss = 0.3080 (3.100 sec/step)\n",
            "I0619 19:16:32.952375 139649090267008 learning.py:507] global step 545: loss = 0.3855 (3.041 sec/step)\n",
            "I0619 19:16:36.009605 139649090267008 learning.py:507] global step 545: loss = 0.3968 (3.055 sec/step)\n",
            "I0619 19:16:39.154882 139649090267008 learning.py:507] global step 546: loss = 0.2860 (3.143 sec/step)\n",
            "I0619 19:16:42.222285 139649090267008 learning.py:507] global step 546: loss = 0.2739 (3.065 sec/step)\n",
            "I0619 19:16:45.293993 139649090267008 learning.py:507] global step 546: loss = 0.2458 (3.070 sec/step)\n",
            "I0619 19:16:48.472663 139649090267008 learning.py:507] global step 546: loss = 0.3572 (3.177 sec/step)\n",
            "I0619 19:16:51.739801 139649090267008 learning.py:507] global step 546: loss = 0.3387 (3.265 sec/step)\n",
            "I0619 19:16:54.934689 139649090267008 learning.py:507] global step 546: loss = 0.3037 (3.193 sec/step)\n",
            "I0619 19:16:58.087389 139649090267008 learning.py:507] global step 546: loss = 0.3306 (3.151 sec/step)\n",
            "I0619 19:17:01.190520 139649090267008 learning.py:507] global step 546: loss = 0.3307 (3.101 sec/step)\n",
            "I0619 19:17:04.223989 139649090267008 learning.py:507] global step 547: loss = 0.2893 (3.031 sec/step)\n",
            "I0619 19:17:07.354290 139649090267008 learning.py:507] global step 547: loss = 0.2695 (3.128 sec/step)\n",
            "I0619 19:17:10.463144 139649090267008 learning.py:507] global step 547: loss = 0.3168 (3.107 sec/step)\n",
            "I0619 19:17:13.539001 139649090267008 learning.py:507] global step 547: loss = 0.4431 (3.074 sec/step)\n",
            "I0619 19:17:16.612073 139649090267008 learning.py:507] global step 547: loss = 0.2809 (3.071 sec/step)\n",
            "I0619 19:17:19.746953 139649090267008 learning.py:507] global step 547: loss = 0.3404 (3.133 sec/step)\n",
            "I0619 19:17:22.825394 139649090267008 learning.py:507] global step 547: loss = 0.2925 (3.077 sec/step)\n",
            "I0619 19:17:25.898331 139649090267008 learning.py:507] global step 547: loss = 0.2938 (3.071 sec/step)\n",
            "I0619 19:17:29.051506 139649090267008 learning.py:507] global step 548: loss = 0.3237 (3.150 sec/step)\n",
            "I0619 19:17:32.200172 139649090267008 learning.py:507] global step 548: loss = 0.2718 (3.146 sec/step)\n",
            "I0619 19:17:35.226737 139649090267008 learning.py:507] global step 548: loss = 0.3181 (3.025 sec/step)\n",
            "I0619 19:17:38.316627 139649090267008 learning.py:507] global step 548: loss = 0.3068 (3.087 sec/step)\n",
            "I0619 19:17:41.394792 139649090267008 learning.py:507] global step 548: loss = 0.2770 (3.076 sec/step)\n",
            "I0619 19:17:44.463030 139649090267008 learning.py:507] global step 548: loss = 0.3370 (3.066 sec/step)\n",
            "I0619 19:17:47.694880 139649090267008 learning.py:507] global step 548: loss = 0.3000 (3.230 sec/step)\n",
            "I0619 19:17:50.750121 139649090267008 learning.py:507] global step 548: loss = 0.3761 (3.053 sec/step)\n",
            "I0619 19:17:53.819750 139649090267008 learning.py:507] global step 549: loss = 0.2994 (3.068 sec/step)\n",
            "I0619 19:17:56.896926 139649090267008 learning.py:507] global step 549: loss = 0.4540 (3.075 sec/step)\n",
            "I0619 19:17:59.997541 139649090267008 learning.py:507] global step 549: loss = 0.3098 (3.099 sec/step)\n",
            "I0619 19:18:03.124648 139649090267008 learning.py:507] global step 549: loss = 0.3283 (3.125 sec/step)\n",
            "I0619 19:18:06.231977 139649090267008 learning.py:507] global step 549: loss = 0.3950 (3.106 sec/step)\n",
            "I0619 19:18:09.298475 139649090267008 learning.py:507] global step 549: loss = 0.6170 (3.065 sec/step)\n",
            "I0619 19:18:12.390301 139649090267008 learning.py:507] global step 549: loss = 0.2591 (3.090 sec/step)\n",
            "I0619 19:18:15.486718 139649090267008 learning.py:507] global step 549: loss = 0.4109 (3.094 sec/step)\n",
            "I0619 19:18:18.574284 139649090267008 learning.py:507] global step 550: loss = 0.3744 (3.085 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:18:22.473779 139649090267008 learning.py:507] global step 550: loss = 0.2692 (3.887 sec/step)\n",
            "I0619 19:18:26.249456 139646017689344 supervisor.py:1050] Recording summary at step 550.\n",
            "I0619 19:18:27.160305 139649090267008 learning.py:507] global step 550: loss = 0.3293 (4.641 sec/step)\n",
            "I0619 19:18:30.375227 139649090267008 learning.py:507] global step 550: loss = 0.3034 (3.213 sec/step)\n",
            "I0619 19:18:33.475999 139649090267008 learning.py:507] global step 550: loss = 0.3616 (3.099 sec/step)\n",
            "I0619 19:18:36.599923 139649090267008 learning.py:507] global step 550: loss = 0.3394 (3.122 sec/step)\n",
            "I0619 19:18:39.835755 139649090267008 learning.py:507] global step 550: loss = 0.2716 (3.234 sec/step)\n",
            "I0619 19:18:42.996666 139649090267008 learning.py:507] global step 550: loss = 0.3015 (3.159 sec/step)\n",
            "I0619 19:18:46.061987 139649090267008 learning.py:507] global step 551: loss = 0.2906 (3.063 sec/step)\n",
            "I0619 19:18:49.289362 139649090267008 learning.py:507] global step 551: loss = 0.3139 (3.225 sec/step)\n",
            "I0619 19:18:52.433020 139649090267008 learning.py:507] global step 551: loss = 0.4040 (3.142 sec/step)\n",
            "I0619 19:18:55.537540 139649090267008 learning.py:507] global step 551: loss = 0.3266 (3.103 sec/step)\n",
            "I0619 19:18:58.685266 139649090267008 learning.py:507] global step 551: loss = 0.2615 (3.146 sec/step)\n",
            "I0619 19:19:01.767587 139649090267008 learning.py:507] global step 551: loss = 0.3604 (3.081 sec/step)\n",
            "I0619 19:19:04.959061 139649090267008 learning.py:507] global step 551: loss = 0.3178 (3.190 sec/step)\n",
            "I0619 19:19:08.082766 139649090267008 learning.py:507] global step 551: loss = 0.2581 (3.122 sec/step)\n",
            "I0619 19:19:11.172537 139649090267008 learning.py:507] global step 552: loss = 0.3451 (3.087 sec/step)\n",
            "I0619 19:19:14.273374 139649090267008 learning.py:507] global step 552: loss = 0.2472 (3.099 sec/step)\n",
            "I0619 19:19:17.448245 139649090267008 learning.py:507] global step 552: loss = 0.3847 (3.173 sec/step)\n",
            "I0619 19:19:20.578047 139649090267008 learning.py:507] global step 552: loss = 0.3415 (3.128 sec/step)\n",
            "I0619 19:19:23.752855 139649090267008 learning.py:507] global step 552: loss = 0.3262 (3.173 sec/step)\n",
            "I0619 19:19:26.844593 139649090267008 learning.py:507] global step 552: loss = 0.2782 (3.090 sec/step)\n",
            "I0619 19:19:29.892644 139649090267008 learning.py:507] global step 552: loss = 0.2604 (3.046 sec/step)\n",
            "I0619 19:19:32.947088 139649090267008 learning.py:507] global step 552: loss = 0.3242 (3.053 sec/step)\n",
            "I0619 19:19:36.109270 139649090267008 learning.py:507] global step 553: loss = 0.3993 (3.160 sec/step)\n",
            "I0619 19:19:39.159490 139649090267008 learning.py:507] global step 553: loss = 0.3474 (3.048 sec/step)\n",
            "I0619 19:19:42.231806 139649090267008 learning.py:507] global step 553: loss = 0.2999 (3.070 sec/step)\n",
            "I0619 19:19:45.292639 139649090267008 learning.py:507] global step 553: loss = 0.3842 (3.059 sec/step)\n",
            "I0619 19:19:48.376440 139649090267008 learning.py:507] global step 553: loss = 0.3015 (3.082 sec/step)\n",
            "I0619 19:19:51.421177 139649090267008 learning.py:507] global step 553: loss = 0.3563 (3.043 sec/step)\n",
            "I0619 19:19:54.498936 139649090267008 learning.py:507] global step 553: loss = 0.4059 (3.076 sec/step)\n",
            "I0619 19:19:57.582612 139649090267008 learning.py:507] global step 553: loss = 0.2910 (3.082 sec/step)\n",
            "I0619 19:20:00.628432 139649090267008 learning.py:507] global step 554: loss = 0.2912 (3.043 sec/step)\n",
            "I0619 19:20:03.699593 139649090267008 learning.py:507] global step 554: loss = 0.3900 (3.069 sec/step)\n",
            "I0619 19:20:06.799412 139649090267008 learning.py:507] global step 554: loss = 0.3273 (3.098 sec/step)\n",
            "I0619 19:20:09.824869 139649090267008 learning.py:507] global step 554: loss = 0.3302 (3.024 sec/step)\n",
            "I0619 19:20:12.929838 139649090267008 learning.py:507] global step 554: loss = 0.3443 (3.103 sec/step)\n",
            "I0619 19:20:15.995668 139649090267008 learning.py:507] global step 554: loss = 0.2845 (3.064 sec/step)\n",
            "I0619 19:20:19.068307 139649090267008 learning.py:507] global step 554: loss = 0.3316 (3.071 sec/step)\n",
            "I0619 19:20:20.931320 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:20:24.822266 139649090267008 learning.py:507] global step 554: loss = 0.3725 (5.744 sec/step)\n",
            "I0619 19:20:28.335189 139646017689344 supervisor.py:1050] Recording summary at step 554.\n",
            "I0619 19:20:29.616173 139649090267008 learning.py:507] global step 555: loss = 0.3774 (4.786 sec/step)\n",
            "I0619 19:20:32.722165 139649090267008 learning.py:507] global step 555: loss = 0.2899 (3.104 sec/step)\n",
            "I0619 19:20:35.738515 139649090267008 learning.py:507] global step 555: loss = 0.3558 (3.014 sec/step)\n",
            "I0619 19:20:39.252134 139649090267008 learning.py:507] global step 555: loss = 0.3077 (3.511 sec/step)\n",
            "I0619 19:20:42.335983 139649090267008 learning.py:507] global step 555: loss = 0.3525 (3.082 sec/step)\n",
            "I0619 19:20:45.448211 139649090267008 learning.py:507] global step 555: loss = 0.3091 (3.110 sec/step)\n",
            "I0619 19:20:48.575528 139649090267008 learning.py:507] global step 555: loss = 0.3271 (3.125 sec/step)\n",
            "I0619 19:20:51.597873 139649090267008 learning.py:507] global step 555: loss = 0.2998 (3.020 sec/step)\n",
            "I0619 19:20:54.661144 139649090267008 learning.py:507] global step 556: loss = 0.3203 (3.061 sec/step)\n",
            "I0619 19:20:58.229974 139649090267008 learning.py:507] global step 556: loss = 0.3114 (3.567 sec/step)\n",
            "I0619 19:21:01.324929 139649090267008 learning.py:507] global step 556: loss = 0.2891 (3.093 sec/step)\n",
            "I0619 19:21:04.371765 139649090267008 learning.py:507] global step 556: loss = 0.2759 (3.045 sec/step)\n",
            "I0619 19:21:07.427400 139649090267008 learning.py:507] global step 556: loss = 0.3649 (3.054 sec/step)\n",
            "I0619 19:21:10.538577 139649090267008 learning.py:507] global step 556: loss = 0.2937 (3.109 sec/step)\n",
            "I0619 19:21:13.619244 139649090267008 learning.py:507] global step 556: loss = 0.3572 (3.079 sec/step)\n",
            "I0619 19:21:16.773282 139649090267008 learning.py:507] global step 556: loss = 0.4009 (3.152 sec/step)\n",
            "I0619 19:21:19.840865 139649090267008 learning.py:507] global step 557: loss = 0.3607 (3.065 sec/step)\n",
            "I0619 19:21:22.907973 139649090267008 learning.py:507] global step 557: loss = 0.3198 (3.065 sec/step)\n",
            "I0619 19:21:25.935934 139649090267008 learning.py:507] global step 557: loss = 0.2897 (3.026 sec/step)\n",
            "I0619 19:21:28.990843 139649090267008 learning.py:507] global step 557: loss = 0.3524 (3.053 sec/step)\n",
            "I0619 19:21:32.262433 139649090267008 learning.py:507] global step 557: loss = 0.3201 (3.270 sec/step)\n",
            "I0619 19:21:35.274607 139649090267008 learning.py:507] global step 557: loss = 0.3290 (3.010 sec/step)\n",
            "I0619 19:21:38.299950 139649090267008 learning.py:507] global step 557: loss = 0.2884 (3.024 sec/step)\n",
            "I0619 19:21:41.354819 139649090267008 learning.py:507] global step 557: loss = 0.4538 (3.053 sec/step)\n",
            "I0619 19:21:44.465068 139649090267008 learning.py:507] global step 558: loss = 0.3641 (3.108 sec/step)\n",
            "I0619 19:21:47.497031 139649090267008 learning.py:507] global step 558: loss = 0.3704 (3.030 sec/step)\n",
            "I0619 19:21:50.915327 139649090267008 learning.py:507] global step 558: loss = 0.2969 (3.416 sec/step)\n",
            "I0619 19:21:54.002609 139649090267008 learning.py:507] global step 558: loss = 0.3469 (3.086 sec/step)\n",
            "I0619 19:21:57.069285 139649090267008 learning.py:507] global step 558: loss = 0.3098 (3.065 sec/step)\n",
            "I0619 19:22:00.190733 139649090267008 learning.py:507] global step 558: loss = 0.3794 (3.119 sec/step)\n",
            "I0619 19:22:03.244298 139649090267008 learning.py:507] global step 558: loss = 0.3202 (3.052 sec/step)\n",
            "I0619 19:22:06.352475 139649090267008 learning.py:507] global step 558: loss = 0.3340 (3.106 sec/step)\n",
            "I0619 19:22:09.736831 139649090267008 learning.py:507] global step 559: loss = 0.3826 (3.382 sec/step)\n",
            "I0619 19:22:12.817103 139649090267008 learning.py:507] global step 559: loss = 0.2951 (3.078 sec/step)\n",
            "I0619 19:22:15.934154 139649090267008 learning.py:507] global step 559: loss = 0.3139 (3.115 sec/step)\n",
            "I0619 19:22:18.976482 139649090267008 learning.py:507] global step 559: loss = 0.3729 (3.041 sec/step)\n",
            "I0619 19:22:23.795738 139649090267008 learning.py:507] global step 559: loss = 0.3530 (4.814 sec/step)\n",
            "I0619 19:22:26.641754 139646017689344 supervisor.py:1050] Recording summary at step 559.\n",
            "I0619 19:22:27.953660 139649090267008 learning.py:507] global step 559: loss = 0.2966 (4.155 sec/step)\n",
            "I0619 19:22:31.230039 139649090267008 learning.py:507] global step 559: loss = 0.2874 (3.275 sec/step)\n",
            "I0619 19:22:34.242594 139649090267008 learning.py:507] global step 559: loss = 0.3154 (3.011 sec/step)\n",
            "I0619 19:22:37.271305 139649090267008 learning.py:507] global step 560: loss = 0.3667 (3.026 sec/step)\n",
            "I0619 19:22:40.368600 139649090267008 learning.py:507] global step 560: loss = 0.3969 (3.096 sec/step)\n",
            "I0619 19:22:43.385909 139649090267008 learning.py:507] global step 560: loss = 0.3484 (3.015 sec/step)\n",
            "I0619 19:22:46.398574 139649090267008 learning.py:507] global step 560: loss = 0.2706 (3.011 sec/step)\n",
            "I0619 19:22:49.711318 139649090267008 learning.py:507] global step 560: loss = 0.2868 (3.311 sec/step)\n",
            "I0619 19:22:52.828578 139649090267008 learning.py:507] global step 560: loss = 0.2978 (3.116 sec/step)\n",
            "I0619 19:22:55.866472 139649090267008 learning.py:507] global step 560: loss = 0.3349 (3.036 sec/step)\n",
            "I0619 19:22:58.904407 139649090267008 learning.py:507] global step 560: loss = 0.3254 (3.036 sec/step)\n",
            "I0619 19:23:01.898775 139649090267008 learning.py:507] global step 561: loss = 0.3021 (2.992 sec/step)\n",
            "I0619 19:23:04.957213 139649090267008 learning.py:507] global step 561: loss = 0.2716 (3.056 sec/step)\n",
            "I0619 19:23:07.960529 139649090267008 learning.py:507] global step 561: loss = 0.2653 (3.001 sec/step)\n",
            "I0619 19:23:10.955447 139649090267008 learning.py:507] global step 561: loss = 0.2740 (2.993 sec/step)\n",
            "I0619 19:23:13.951221 139649090267008 learning.py:507] global step 561: loss = 0.3157 (2.994 sec/step)\n",
            "I0619 19:23:17.030047 139649090267008 learning.py:507] global step 561: loss = 0.3076 (3.077 sec/step)\n",
            "I0619 19:23:20.030466 139649090267008 learning.py:507] global step 561: loss = 0.4096 (2.999 sec/step)\n",
            "I0619 19:23:23.063929 139649090267008 learning.py:507] global step 561: loss = 0.2980 (3.031 sec/step)\n",
            "I0619 19:23:26.121596 139649090267008 learning.py:507] global step 562: loss = 0.2853 (3.055 sec/step)\n",
            "I0619 19:23:29.133949 139649090267008 learning.py:507] global step 562: loss = 0.3420 (3.011 sec/step)\n",
            "I0619 19:23:32.118041 139649090267008 learning.py:507] global step 562: loss = 0.3218 (2.982 sec/step)\n",
            "I0619 19:23:35.184689 139649090267008 learning.py:507] global step 562: loss = 0.3208 (3.065 sec/step)\n",
            "I0619 19:23:38.256107 139649090267008 learning.py:507] global step 562: loss = 0.3275 (3.069 sec/step)\n",
            "I0619 19:23:41.278461 139649090267008 learning.py:507] global step 562: loss = 0.3122 (3.021 sec/step)\n",
            "I0619 19:23:44.378167 139649090267008 learning.py:507] global step 562: loss = 0.2832 (3.098 sec/step)\n",
            "I0619 19:23:47.389062 139649090267008 learning.py:507] global step 562: loss = 0.2896 (3.009 sec/step)\n",
            "I0619 19:23:50.419662 139649090267008 learning.py:507] global step 563: loss = 0.3879 (3.028 sec/step)\n",
            "I0619 19:23:53.483359 139649090267008 learning.py:507] global step 563: loss = 0.3117 (3.062 sec/step)\n",
            "I0619 19:23:56.517649 139649090267008 learning.py:507] global step 563: loss = 0.2744 (3.033 sec/step)\n",
            "I0619 19:23:59.528266 139649090267008 learning.py:507] global step 563: loss = 0.3916 (3.009 sec/step)\n",
            "I0619 19:24:02.538531 139649090267008 learning.py:507] global step 563: loss = 0.3088 (3.008 sec/step)\n",
            "I0619 19:24:05.550053 139649090267008 learning.py:507] global step 563: loss = 0.3067 (3.010 sec/step)\n",
            "I0619 19:24:08.514393 139649090267008 learning.py:507] global step 563: loss = 0.2982 (2.960 sec/step)\n",
            "I0619 19:24:11.512195 139649090267008 learning.py:507] global step 563: loss = 0.3563 (2.996 sec/step)\n",
            "I0619 19:24:14.525887 139649090267008 learning.py:507] global step 564: loss = 0.2606 (3.010 sec/step)\n",
            "I0619 19:24:17.515854 139649090267008 learning.py:507] global step 564: loss = 0.4289 (2.988 sec/step)\n",
            "I0619 19:24:20.562737 139649090267008 learning.py:507] global step 564: loss = 0.3222 (3.045 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:24:25.801688 139649090267008 learning.py:507] global step 564: loss = 0.3203 (5.235 sec/step)\n",
            "I0619 19:24:26.909121 139646017689344 supervisor.py:1050] Recording summary at step 564.\n",
            "I0619 19:24:28.968750 139649090267008 learning.py:507] global step 564: loss = 0.3865 (3.165 sec/step)\n",
            "I0619 19:24:32.093829 139649090267008 learning.py:507] global step 564: loss = 0.2880 (3.123 sec/step)\n",
            "I0619 19:24:35.236643 139649090267008 learning.py:507] global step 564: loss = 0.3656 (3.141 sec/step)\n",
            "I0619 19:24:38.249820 139649090267008 learning.py:507] global step 564: loss = 0.2788 (3.011 sec/step)\n",
            "I0619 19:24:41.317434 139649090267008 learning.py:507] global step 565: loss = 0.2990 (3.065 sec/step)\n",
            "I0619 19:24:44.344335 139649090267008 learning.py:507] global step 565: loss = 0.2629 (3.025 sec/step)\n",
            "I0619 19:24:47.526587 139649090267008 learning.py:507] global step 565: loss = 0.2941 (3.180 sec/step)\n",
            "I0619 19:24:50.525283 139649090267008 learning.py:507] global step 565: loss = 0.2747 (2.997 sec/step)\n",
            "I0619 19:24:53.657992 139649090267008 learning.py:507] global step 565: loss = 0.3756 (3.131 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:24:56.746227 139649090267008 learning.py:507] global step 565: loss = 0.2812 (3.086 sec/step)\n",
            "I0619 19:24:59.785930 139649090267008 learning.py:507] global step 565: loss = 0.3018 (3.038 sec/step)\n",
            "I0619 19:25:03.668215 139649090267008 learning.py:507] global step 565: loss = 0.3965 (3.880 sec/step)\n",
            "I0619 19:25:06.769279 139649090267008 learning.py:507] global step 566: loss = 0.3579 (3.099 sec/step)\n",
            "I0619 19:25:09.810878 139649090267008 learning.py:507] global step 566: loss = 0.2581 (3.040 sec/step)\n",
            "I0619 19:25:12.872068 139649090267008 learning.py:507] global step 566: loss = 0.3725 (3.059 sec/step)\n",
            "I0619 19:25:15.836388 139649090267008 learning.py:507] global step 566: loss = 0.3587 (2.963 sec/step)\n",
            "I0619 19:25:18.878360 139649090267008 learning.py:507] global step 566: loss = 0.4063 (3.040 sec/step)\n",
            "I0619 19:25:22.650243 139649090267008 learning.py:507] global step 566: loss = 0.2849 (3.770 sec/step)\n",
            "I0619 19:25:25.685783 139649090267008 learning.py:507] global step 566: loss = 0.3038 (3.034 sec/step)\n",
            "I0619 19:25:28.739220 139649090267008 learning.py:507] global step 566: loss = 0.3589 (3.052 sec/step)\n",
            "I0619 19:25:31.781165 139649090267008 learning.py:507] global step 567: loss = 0.3208 (3.039 sec/step)\n",
            "I0619 19:25:34.921545 139649090267008 learning.py:507] global step 567: loss = 0.3632 (3.139 sec/step)\n",
            "I0619 19:25:38.143363 139649090267008 learning.py:507] global step 567: loss = 0.3024 (3.220 sec/step)\n",
            "I0619 19:25:41.185508 139649090267008 learning.py:507] global step 567: loss = 0.2933 (3.040 sec/step)\n",
            "I0619 19:25:44.248519 139649090267008 learning.py:507] global step 567: loss = 0.3358 (3.061 sec/step)\n",
            "I0619 19:25:47.246913 139649090267008 learning.py:507] global step 567: loss = 0.3347 (2.997 sec/step)\n",
            "I0619 19:25:50.434363 139649090267008 learning.py:507] global step 567: loss = 0.3610 (3.185 sec/step)\n",
            "I0619 19:25:53.471584 139649090267008 learning.py:507] global step 567: loss = 0.3022 (3.035 sec/step)\n",
            "I0619 19:25:56.551088 139649090267008 learning.py:507] global step 568: loss = 0.3305 (3.077 sec/step)\n",
            "I0619 19:25:59.691242 139649090267008 learning.py:507] global step 568: loss = 0.2790 (3.138 sec/step)\n",
            "I0619 19:26:02.776156 139649090267008 learning.py:507] global step 568: loss = 0.2988 (3.083 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:26:05.817690 139649090267008 learning.py:507] global step 568: loss = 0.3548 (3.040 sec/step)\n",
            "I0619 19:26:08.769423 139649090267008 learning.py:507] global step 568: loss = 0.3390 (2.950 sec/step)\n",
            "I0619 19:26:11.802619 139649090267008 learning.py:507] global step 568: loss = 0.2912 (3.031 sec/step)\n",
            "I0619 19:26:14.866824 139649090267008 learning.py:507] global step 568: loss = 0.3273 (3.063 sec/step)\n",
            "I0619 19:26:18.009377 139649090267008 learning.py:507] global step 568: loss = 0.2626 (3.141 sec/step)\n",
            "I0619 19:26:21.040167 139649090267008 learning.py:507] global step 569: loss = 0.3170 (3.029 sec/step)\n",
            "I0619 19:26:26.377689 139649090267008 learning.py:507] global step 569: loss = 0.3790 (5.335 sec/step)\n",
            "I0619 19:26:26.904481 139646017689344 supervisor.py:1050] Recording summary at step 569.\n",
            "I0619 19:26:29.406699 139649090267008 learning.py:507] global step 569: loss = 0.2807 (3.027 sec/step)\n",
            "I0619 19:26:32.514468 139649090267008 learning.py:507] global step 569: loss = 0.3488 (3.106 sec/step)\n",
            "I0619 19:26:35.708183 139649090267008 learning.py:507] global step 569: loss = 0.4321 (3.192 sec/step)\n",
            "I0619 19:26:38.741627 139649090267008 learning.py:507] global step 569: loss = 0.4231 (3.032 sec/step)\n",
            "I0619 19:26:41.816897 139649090267008 learning.py:507] global step 569: loss = 0.3153 (3.074 sec/step)\n",
            "I0619 19:26:44.866178 139649090267008 learning.py:507] global step 569: loss = 0.2482 (3.048 sec/step)\n",
            "I0619 19:26:47.866619 139649090267008 learning.py:507] global step 570: loss = 0.3674 (2.998 sec/step)\n",
            "I0619 19:26:50.914436 139649090267008 learning.py:507] global step 570: loss = 0.3905 (3.046 sec/step)\n",
            "I0619 19:26:53.944537 139649090267008 learning.py:507] global step 570: loss = 0.3722 (3.028 sec/step)\n",
            "I0619 19:26:56.953352 139649090267008 learning.py:507] global step 570: loss = 0.3217 (3.007 sec/step)\n",
            "I0619 19:27:00.017686 139649090267008 learning.py:507] global step 570: loss = 0.2862 (3.063 sec/step)\n",
            "I0619 19:27:03.073259 139649090267008 learning.py:507] global step 570: loss = 0.4615 (3.054 sec/step)\n",
            "I0619 19:27:06.114249 139649090267008 learning.py:507] global step 570: loss = 0.2794 (3.039 sec/step)\n",
            "I0619 19:27:09.102001 139649090267008 learning.py:507] global step 570: loss = 0.2905 (2.986 sec/step)\n",
            "I0619 19:27:12.194786 139649090267008 learning.py:507] global step 571: loss = 0.2864 (3.091 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:27:15.205666 139649090267008 learning.py:507] global step 571: loss = 0.2974 (3.009 sec/step)\n",
            "I0619 19:27:18.241017 139649090267008 learning.py:507] global step 571: loss = 0.3544 (3.033 sec/step)\n",
            "I0619 19:27:21.310711 139649090267008 learning.py:507] global step 571: loss = 0.2329 (3.068 sec/step)\n",
            "I0619 19:27:24.368865 139649090267008 learning.py:507] global step 571: loss = 0.2878 (3.057 sec/step)\n",
            "I0619 19:27:27.456190 139649090267008 learning.py:507] global step 571: loss = 0.2718 (3.085 sec/step)\n",
            "I0619 19:27:30.515589 139649090267008 learning.py:507] global step 571: loss = 0.3175 (3.058 sec/step)\n",
            "I0619 19:27:33.578293 139649090267008 learning.py:507] global step 571: loss = 0.3655 (3.061 sec/step)\n",
            "I0619 19:27:36.600837 139649090267008 learning.py:507] global step 572: loss = 0.3107 (3.021 sec/step)\n",
            "I0619 19:27:39.582993 139649090267008 learning.py:507] global step 572: loss = 0.2613 (2.980 sec/step)\n",
            "I0619 19:27:42.646784 139649090267008 learning.py:507] global step 572: loss = 0.3551 (3.062 sec/step)\n",
            "I0619 19:27:45.691333 139649090267008 learning.py:507] global step 572: loss = 0.3146 (3.043 sec/step)\n",
            "I0619 19:27:48.677404 139649090267008 learning.py:507] global step 572: loss = 0.3007 (2.984 sec/step)\n",
            "I0619 19:27:51.719782 139649090267008 learning.py:507] global step 572: loss = 0.3948 (3.041 sec/step)\n",
            "I0619 19:27:54.759673 139649090267008 learning.py:507] global step 572: loss = 0.3625 (3.038 sec/step)\n",
            "I0619 19:27:57.731423 139649090267008 learning.py:507] global step 572: loss = 0.5677 (2.970 sec/step)\n",
            "I0619 19:28:00.762927 139649090267008 learning.py:507] global step 573: loss = 0.2893 (3.029 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:28:03.786573 139649090267008 learning.py:507] global step 573: loss = 0.3876 (3.022 sec/step)\n",
            "I0619 19:28:06.779308 139649090267008 learning.py:507] global step 573: loss = 0.3052 (2.990 sec/step)\n",
            "I0619 19:28:09.802857 139649090267008 learning.py:507] global step 573: loss = 0.3412 (3.022 sec/step)\n",
            "I0619 19:28:12.846840 139649090267008 learning.py:507] global step 573: loss = 0.2863 (3.042 sec/step)\n",
            "I0619 19:28:15.885594 139649090267008 learning.py:507] global step 573: loss = 0.2742 (3.037 sec/step)\n",
            "I0619 19:28:18.936922 139649090267008 learning.py:507] global step 573: loss = 0.4606 (3.050 sec/step)\n",
            "I0619 19:28:23.195476 139649090267008 learning.py:507] global step 573: loss = 0.2459 (4.237 sec/step)\n",
            "I0619 19:28:26.242429 139646017689344 supervisor.py:1050] Recording summary at step 573.\n",
            "I0619 19:28:27.374582 139649090267008 learning.py:507] global step 574: loss = 0.2684 (4.177 sec/step)\n",
            "I0619 19:28:30.408076 139649090267008 learning.py:507] global step 574: loss = 0.2696 (3.032 sec/step)\n",
            "I0619 19:28:33.462907 139649090267008 learning.py:507] global step 574: loss = 0.5391 (3.053 sec/step)\n",
            "I0619 19:28:36.581855 139649090267008 learning.py:507] global step 574: loss = 0.3093 (3.117 sec/step)\n",
            "I0619 19:28:39.602233 139649090267008 learning.py:507] global step 574: loss = 0.2982 (3.019 sec/step)\n",
            "I0619 19:28:42.711376 139649090267008 learning.py:507] global step 574: loss = 0.3058 (3.107 sec/step)\n",
            "I0619 19:28:45.727811 139649090267008 learning.py:507] global step 574: loss = 0.3688 (3.015 sec/step)\n",
            "I0619 19:28:48.724061 139649090267008 learning.py:507] global step 574: loss = 0.2991 (2.994 sec/step)\n",
            "I0619 19:28:51.789829 139649090267008 learning.py:507] global step 575: loss = 0.3574 (3.063 sec/step)\n",
            "I0619 19:28:54.927400 139649090267008 learning.py:507] global step 575: loss = 0.3581 (3.136 sec/step)\n",
            "I0619 19:28:57.974470 139649090267008 learning.py:507] global step 575: loss = 0.3308 (3.045 sec/step)\n",
            "I0619 19:29:00.936456 139649090267008 learning.py:507] global step 575: loss = 0.3387 (2.960 sec/step)\n",
            "I0619 19:29:03.956734 139649090267008 learning.py:507] global step 575: loss = 0.3386 (3.018 sec/step)\n",
            "I0619 19:29:07.010626 139649090267008 learning.py:507] global step 575: loss = 0.2607 (3.052 sec/step)\n",
            "I0619 19:29:10.099790 139649090267008 learning.py:507] global step 575: loss = 0.2982 (3.087 sec/step)\n",
            "I0619 19:29:13.093290 139649090267008 learning.py:507] global step 575: loss = 0.2665 (2.991 sec/step)\n",
            "I0619 19:29:16.243382 139649090267008 learning.py:507] global step 576: loss = 0.3612 (3.147 sec/step)\n",
            "I0619 19:29:19.192526 139649090267008 learning.py:507] global step 576: loss = 0.3678 (2.947 sec/step)\n",
            "I0619 19:29:22.133645 139649090267008 learning.py:507] global step 576: loss = 0.3326 (2.939 sec/step)\n",
            "I0619 19:29:25.280270 139649090267008 learning.py:507] global step 576: loss = 0.3865 (3.145 sec/step)\n",
            "I0619 19:29:28.300734 139649090267008 learning.py:507] global step 576: loss = 0.3079 (3.019 sec/step)\n",
            "I0619 19:29:31.401208 139649090267008 learning.py:507] global step 576: loss = 0.2962 (3.098 sec/step)\n",
            "I0619 19:29:34.424705 139649090267008 learning.py:507] global step 576: loss = 0.4111 (3.022 sec/step)\n",
            "I0619 19:29:37.420918 139649090267008 learning.py:507] global step 576: loss = 0.2593 (2.994 sec/step)\n",
            "I0619 19:29:40.362320 139649090267008 learning.py:507] global step 577: loss = 0.3329 (2.939 sec/step)\n",
            "I0619 19:29:43.561628 139649090267008 learning.py:507] global step 577: loss = 0.3598 (3.197 sec/step)\n",
            "I0619 19:29:47.031785 139649090267008 learning.py:507] global step 577: loss = 0.2800 (3.468 sec/step)\n",
            "I0619 19:29:50.037048 139649090267008 learning.py:507] global step 577: loss = 0.4674 (3.004 sec/step)\n",
            "I0619 19:29:52.990061 139649090267008 learning.py:507] global step 577: loss = 0.3080 (2.951 sec/step)\n",
            "I0619 19:29:55.975518 139649090267008 learning.py:507] global step 577: loss = 0.3011 (2.984 sec/step)\n",
            "I0619 19:29:58.957146 139649090267008 learning.py:507] global step 577: loss = 0.2899 (2.980 sec/step)\n",
            "I0619 19:30:01.993247 139649090267008 learning.py:507] global step 577: loss = 0.3192 (3.034 sec/step)\n",
            "I0619 19:30:05.397610 139649090267008 learning.py:507] global step 578: loss = 0.3192 (3.402 sec/step)\n",
            "I0619 19:30:08.462740 139649090267008 learning.py:507] global step 578: loss = 0.3204 (3.063 sec/step)\n",
            "I0619 19:30:11.464799 139649090267008 learning.py:507] global step 578: loss = 0.2784 (3.000 sec/step)\n",
            "I0619 19:30:14.433141 139649090267008 learning.py:507] global step 578: loss = 0.3620 (2.964 sec/step)\n",
            "I0619 19:30:17.429740 139649090267008 learning.py:507] global step 578: loss = 0.3919 (2.994 sec/step)\n",
            "I0619 19:30:20.468082 139649090267008 learning.py:507] global step 578: loss = 0.2963 (3.037 sec/step)\n",
            "I0619 19:30:20.934300 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:30:27.262523 139649090267008 learning.py:507] global step 578: loss = 0.3320 (6.792 sec/step)\n",
            "I0619 19:30:27.981408 139646017689344 supervisor.py:1050] Recording summary at step 578.\n",
            "I0619 19:30:30.344257 139649090267008 learning.py:507] global step 578: loss = 0.3030 (3.080 sec/step)\n",
            "I0619 19:30:33.395427 139649090267008 learning.py:507] global step 579: loss = 0.3552 (3.049 sec/step)\n",
            "I0619 19:30:36.384329 139649090267008 learning.py:507] global step 579: loss = 0.3242 (2.987 sec/step)\n",
            "I0619 19:30:39.428652 139649090267008 learning.py:507] global step 579: loss = 0.4112 (3.042 sec/step)\n",
            "I0619 19:30:42.411636 139649090267008 learning.py:507] global step 579: loss = 0.2517 (2.981 sec/step)\n",
            "I0619 19:30:45.374305 139649090267008 learning.py:507] global step 579: loss = 0.3346 (2.961 sec/step)\n",
            "I0619 19:30:48.455676 139649090267008 learning.py:507] global step 579: loss = 0.3981 (3.080 sec/step)\n",
            "I0619 19:30:51.462339 139649090267008 learning.py:507] global step 579: loss = 0.3082 (3.005 sec/step)\n",
            "I0619 19:30:54.481855 139649090267008 learning.py:507] global step 579: loss = 0.2692 (3.018 sec/step)\n",
            "I0619 19:30:58.151596 139649090267008 learning.py:507] global step 580: loss = 0.3269 (3.668 sec/step)\n",
            "I0619 19:31:01.288630 139649090267008 learning.py:507] global step 580: loss = 0.3225 (3.135 sec/step)\n",
            "I0619 19:31:04.368486 139649090267008 learning.py:507] global step 580: loss = 0.2720 (3.078 sec/step)\n",
            "I0619 19:31:07.627073 139649090267008 learning.py:507] global step 580: loss = 0.3565 (3.257 sec/step)\n",
            "I0619 19:31:10.689945 139649090267008 learning.py:507] global step 580: loss = 0.3041 (3.061 sec/step)\n",
            "I0619 19:31:13.741566 139649090267008 learning.py:507] global step 580: loss = 0.2621 (3.050 sec/step)\n",
            "I0619 19:31:17.338236 139649090267008 learning.py:507] global step 580: loss = 0.3117 (3.595 sec/step)\n",
            "I0619 19:31:20.387947 139649090267008 learning.py:507] global step 580: loss = 0.2923 (3.048 sec/step)\n",
            "I0619 19:31:23.496421 139649090267008 learning.py:507] global step 581: loss = 0.4025 (3.106 sec/step)\n",
            "I0619 19:31:26.487775 139649090267008 learning.py:507] global step 581: loss = 0.3026 (2.990 sec/step)\n",
            "I0619 19:31:29.470774 139649090267008 learning.py:507] global step 581: loss = 0.2648 (2.981 sec/step)\n",
            "I0619 19:31:32.594889 139649090267008 learning.py:507] global step 581: loss = 0.3150 (3.122 sec/step)\n",
            "I0619 19:31:35.589726 139649090267008 learning.py:507] global step 581: loss = 0.3831 (2.993 sec/step)\n",
            "I0619 19:31:38.578125 139649090267008 learning.py:507] global step 581: loss = 0.2624 (2.987 sec/step)\n",
            "I0619 19:31:41.562221 139649090267008 learning.py:507] global step 581: loss = 0.2987 (2.978 sec/step)\n",
            "I0619 19:31:44.516002 139649090267008 learning.py:507] global step 581: loss = 0.3201 (2.951 sec/step)\n",
            "I0619 19:31:47.487352 139649090267008 learning.py:507] global step 582: loss = 0.3427 (2.969 sec/step)\n",
            "I0619 19:31:50.533065 139649090267008 learning.py:507] global step 582: loss = 0.4008 (3.044 sec/step)\n",
            "I0619 19:31:53.653834 139649090267008 learning.py:507] global step 582: loss = 0.2729 (3.119 sec/step)\n",
            "I0619 19:31:56.711503 139649090267008 learning.py:507] global step 582: loss = 0.3230 (3.056 sec/step)\n",
            "I0619 19:31:59.742135 139649090267008 learning.py:507] global step 582: loss = 0.3057 (3.029 sec/step)\n",
            "I0619 19:32:02.836222 139649090267008 learning.py:507] global step 582: loss = 0.3126 (3.092 sec/step)\n",
            "I0619 19:32:05.960934 139649090267008 learning.py:507] global step 582: loss = 0.3264 (3.123 sec/step)\n",
            "I0619 19:32:08.980821 139649090267008 learning.py:507] global step 582: loss = 0.2984 (3.018 sec/step)\n",
            "I0619 19:32:12.071404 139649090267008 learning.py:507] global step 583: loss = 0.2734 (3.088 sec/step)\n",
            "I0619 19:32:15.157329 139649090267008 learning.py:507] global step 583: loss = 0.3324 (3.084 sec/step)\n",
            "I0619 19:32:18.305141 139649090267008 learning.py:507] global step 583: loss = 0.2834 (3.146 sec/step)\n",
            "I0619 19:32:21.404004 139649090267008 learning.py:507] global step 583: loss = 0.3030 (3.097 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:32:26.615252 139646017689344 supervisor.py:1050] Recording summary at step 583.\n",
            "I0619 19:32:27.192238 139649090267008 learning.py:507] global step 583: loss = 0.3013 (5.786 sec/step)\n",
            "I0619 19:32:30.269376 139649090267008 learning.py:507] global step 583: loss = 0.3286 (3.075 sec/step)\n",
            "I0619 19:32:33.360744 139649090267008 learning.py:507] global step 583: loss = 0.3078 (3.089 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:32:36.443177 139649090267008 learning.py:507] global step 583: loss = 0.2917 (3.081 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:32:39.521888 139649090267008 learning.py:507] global step 584: loss = 0.2828 (3.076 sec/step)\n",
            "I0619 19:32:42.634868 139649090267008 learning.py:507] global step 584: loss = 0.3312 (3.111 sec/step)\n",
            "I0619 19:32:46.018591 139649090267008 learning.py:507] global step 584: loss = 0.2789 (3.382 sec/step)\n",
            "I0619 19:32:49.049676 139649090267008 learning.py:507] global step 584: loss = 0.3112 (3.029 sec/step)\n",
            "I0619 19:32:52.069313 139649090267008 learning.py:507] global step 584: loss = 0.2832 (3.018 sec/step)\n",
            "I0619 19:32:55.224207 139649090267008 learning.py:507] global step 584: loss = 0.3521 (3.152 sec/step)\n",
            "I0619 19:32:58.215600 139649090267008 learning.py:507] global step 584: loss = 0.3093 (2.989 sec/step)\n",
            "I0619 19:33:01.272080 139649090267008 learning.py:507] global step 584: loss = 0.3313 (3.055 sec/step)\n",
            "I0619 19:33:04.399399 139649090267008 learning.py:507] global step 585: loss = 0.3031 (3.125 sec/step)\n",
            "I0619 19:33:07.490013 139649090267008 learning.py:507] global step 585: loss = 0.3123 (3.089 sec/step)\n",
            "I0619 19:33:10.502369 139649090267008 learning.py:507] global step 585: loss = 0.3048 (3.010 sec/step)\n",
            "I0619 19:33:13.624703 139649090267008 learning.py:507] global step 585: loss = 0.3029 (3.120 sec/step)\n",
            "I0619 19:33:16.761365 139649090267008 learning.py:507] global step 585: loss = 0.3880 (3.135 sec/step)\n",
            "I0619 19:33:19.820379 139649090267008 learning.py:507] global step 585: loss = 0.3165 (3.057 sec/step)\n",
            "I0619 19:33:22.918028 139649090267008 learning.py:507] global step 585: loss = 0.3150 (3.096 sec/step)\n",
            "I0619 19:33:26.079194 139649090267008 learning.py:507] global step 585: loss = 0.2838 (3.159 sec/step)\n",
            "I0619 19:33:29.148217 139649090267008 learning.py:507] global step 586: loss = 0.3094 (3.067 sec/step)\n",
            "I0619 19:33:32.184216 139649090267008 learning.py:507] global step 586: loss = 0.2576 (3.034 sec/step)\n",
            "I0619 19:33:35.353936 139649090267008 learning.py:507] global step 586: loss = 0.2756 (3.168 sec/step)\n",
            "I0619 19:33:38.486621 139649090267008 learning.py:507] global step 586: loss = 0.3024 (3.131 sec/step)\n",
            "I0619 19:33:41.519751 139649090267008 learning.py:507] global step 586: loss = 0.2404 (3.031 sec/step)\n",
            "I0619 19:33:44.585539 139649090267008 learning.py:507] global step 586: loss = 0.3438 (3.064 sec/step)\n",
            "I0619 19:33:47.678950 139649090267008 learning.py:507] global step 586: loss = 0.2860 (3.092 sec/step)\n",
            "I0619 19:33:50.829864 139649090267008 learning.py:507] global step 586: loss = 0.3610 (3.149 sec/step)\n",
            "I0619 19:33:53.911370 139649090267008 learning.py:507] global step 587: loss = 0.3572 (3.079 sec/step)\n",
            "I0619 19:33:57.030507 139649090267008 learning.py:507] global step 587: loss = 0.2946 (3.117 sec/step)\n",
            "I0619 19:34:00.177167 139649090267008 learning.py:507] global step 587: loss = 0.2908 (3.144 sec/step)\n",
            "I0619 19:34:03.316746 139649090267008 learning.py:507] global step 587: loss = 0.3943 (3.138 sec/step)\n",
            "I0619 19:34:06.451717 139649090267008 learning.py:507] global step 587: loss = 0.3595 (3.133 sec/step)\n",
            "I0619 19:34:09.599793 139649090267008 learning.py:507] global step 587: loss = 0.3122 (3.146 sec/step)\n",
            "I0619 19:34:12.770137 139649090267008 learning.py:507] global step 587: loss = 0.4877 (3.168 sec/step)\n",
            "I0619 19:34:15.990995 139649090267008 learning.py:507] global step 587: loss = 0.3227 (3.219 sec/step)\n",
            "I0619 19:34:19.030354 139649090267008 learning.py:507] global step 588: loss = 0.4031 (3.037 sec/step)\n",
            "I0619 19:34:24.979603 139649090267008 learning.py:507] global step 588: loss = 0.2828 (5.945 sec/step)\n",
            "I0619 19:34:26.836752 139646017689344 supervisor.py:1050] Recording summary at step 588.\n",
            "I0619 19:34:28.490041 139649090267008 learning.py:507] global step 588: loss = 0.3342 (3.508 sec/step)\n",
            "I0619 19:34:31.774371 139649090267008 learning.py:507] global step 588: loss = 0.2890 (3.282 sec/step)\n",
            "I0619 19:34:35.054470 139649090267008 learning.py:507] global step 588: loss = 0.3173 (3.277 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:34:38.186344 139649090267008 learning.py:507] global step 588: loss = 0.3089 (3.130 sec/step)\n",
            "I0619 19:34:42.105364 139649090267008 learning.py:507] global step 588: loss = 0.2850 (3.917 sec/step)\n",
            "I0619 19:34:45.147164 139649090267008 learning.py:507] global step 588: loss = 0.2900 (3.040 sec/step)\n",
            "I0619 19:34:48.197956 139649090267008 learning.py:507] global step 589: loss = 0.3297 (3.049 sec/step)\n",
            "I0619 19:34:51.241895 139649090267008 learning.py:507] global step 589: loss = 0.3335 (3.042 sec/step)\n",
            "I0619 19:34:54.476030 139649090267008 learning.py:507] global step 589: loss = 0.3097 (3.232 sec/step)\n",
            "I0619 19:34:57.556825 139649090267008 learning.py:507] global step 589: loss = 0.3433 (3.079 sec/step)\n",
            "I0619 19:35:00.595008 139649090267008 learning.py:507] global step 589: loss = 0.2656 (3.036 sec/step)\n",
            "I0619 19:35:03.662801 139649090267008 learning.py:507] global step 589: loss = 0.3452 (3.066 sec/step)\n",
            "I0619 19:35:06.722577 139649090267008 learning.py:507] global step 589: loss = 0.3074 (3.058 sec/step)\n",
            "I0619 19:35:09.770951 139649090267008 learning.py:507] global step 589: loss = 0.2675 (3.047 sec/step)\n",
            "I0619 19:35:13.138421 139649090267008 learning.py:507] global step 590: loss = 0.3424 (3.365 sec/step)\n",
            "I0619 19:35:16.136898 139649090267008 learning.py:507] global step 590: loss = 0.2710 (2.997 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:35:19.139663 139649090267008 learning.py:507] global step 590: loss = 0.2872 (3.001 sec/step)\n",
            "I0619 19:35:22.328755 139649090267008 learning.py:507] global step 590: loss = 0.2843 (3.187 sec/step)\n",
            "I0619 19:35:25.415048 139649090267008 learning.py:507] global step 590: loss = 0.2665 (3.084 sec/step)\n",
            "I0619 19:35:28.438886 139649090267008 learning.py:507] global step 590: loss = 0.3218 (3.022 sec/step)\n",
            "I0619 19:35:31.536311 139649090267008 learning.py:507] global step 590: loss = 0.3382 (3.096 sec/step)\n",
            "I0619 19:35:34.552977 139649090267008 learning.py:507] global step 590: loss = 0.2980 (3.015 sec/step)\n",
            "I0619 19:35:37.537033 139649090267008 learning.py:507] global step 591: loss = 0.2710 (2.982 sec/step)\n",
            "I0619 19:35:40.835317 139649090267008 learning.py:507] global step 591: loss = 0.2633 (3.296 sec/step)\n",
            "I0619 19:35:43.817767 139649090267008 learning.py:507] global step 591: loss = 0.4009 (2.980 sec/step)\n",
            "I0619 19:35:46.789800 139649090267008 learning.py:507] global step 591: loss = 0.2787 (2.970 sec/step)\n",
            "I0619 19:35:49.740904 139649090267008 learning.py:507] global step 591: loss = 0.3161 (2.949 sec/step)\n",
            "I0619 19:35:52.773184 139649090267008 learning.py:507] global step 591: loss = 0.3737 (3.031 sec/step)\n",
            "I0619 19:35:55.790010 139649090267008 learning.py:507] global step 591: loss = 0.2799 (3.015 sec/step)\n",
            "I0619 19:35:58.782639 139649090267008 learning.py:507] global step 591: loss = 0.3104 (2.991 sec/step)\n",
            "I0619 19:36:01.845184 139649090267008 learning.py:507] global step 592: loss = 0.3440 (3.060 sec/step)\n",
            "I0619 19:36:04.841881 139649090267008 learning.py:507] global step 592: loss = 0.3736 (2.995 sec/step)\n",
            "I0619 19:36:07.918221 139649090267008 learning.py:507] global step 592: loss = 0.2702 (3.075 sec/step)\n",
            "I0619 19:36:10.941833 139649090267008 learning.py:507] global step 592: loss = 0.4377 (3.022 sec/step)\n",
            "I0619 19:36:14.159262 139649090267008 learning.py:507] global step 592: loss = 0.2783 (3.215 sec/step)\n",
            "I0619 19:36:17.279873 139649090267008 learning.py:507] global step 592: loss = 0.3816 (3.119 sec/step)\n",
            "I0619 19:36:20.335516 139649090267008 learning.py:507] global step 592: loss = 0.3070 (3.053 sec/step)\n",
            "I0619 19:36:25.708852 139649090267008 learning.py:507] global step 592: loss = 0.3007 (5.366 sec/step)\n",
            "I0619 19:36:26.050424 139646017689344 supervisor.py:1050] Recording summary at step 592.\n",
            "I0619 19:36:28.782109 139649090267008 learning.py:507] global step 593: loss = 0.2523 (3.070 sec/step)\n",
            "I0619 19:36:32.002727 139649090267008 learning.py:507] global step 593: loss = 0.4548 (3.219 sec/step)\n",
            "I0619 19:36:34.968687 139649090267008 learning.py:507] global step 593: loss = 0.2688 (2.964 sec/step)\n",
            "I0619 19:36:38.007701 139649090267008 learning.py:507] global step 593: loss = 0.3074 (3.037 sec/step)\n",
            "I0619 19:36:41.176934 139649090267008 learning.py:507] global step 593: loss = 0.2945 (3.167 sec/step)\n",
            "I0619 19:36:44.263702 139649090267008 learning.py:507] global step 593: loss = 0.3537 (3.085 sec/step)\n",
            "I0619 19:36:47.250719 139649090267008 learning.py:507] global step 593: loss = 0.3125 (2.985 sec/step)\n",
            "I0619 19:36:50.292344 139649090267008 learning.py:507] global step 593: loss = 0.3176 (3.040 sec/step)\n",
            "I0619 19:36:53.328287 139649090267008 learning.py:507] global step 594: loss = 0.3052 (3.033 sec/step)\n",
            "I0619 19:36:56.346709 139649090267008 learning.py:507] global step 594: loss = 0.2831 (3.017 sec/step)\n",
            "I0619 19:36:59.392346 139649090267008 learning.py:507] global step 594: loss = 0.2471 (3.044 sec/step)\n",
            "I0619 19:37:02.412341 139649090267008 learning.py:507] global step 594: loss = 0.3793 (3.018 sec/step)\n",
            "I0619 19:37:05.406686 139649090267008 learning.py:507] global step 594: loss = 0.3464 (2.992 sec/step)\n",
            "I0619 19:37:08.528143 139649090267008 learning.py:507] global step 594: loss = 0.3839 (3.118 sec/step)\n",
            "I0619 19:37:11.517576 139649090267008 learning.py:507] global step 594: loss = 0.2698 (2.988 sec/step)\n",
            "I0619 19:37:14.548655 139649090267008 learning.py:507] global step 594: loss = 0.3095 (3.029 sec/step)\n",
            "I0619 19:37:17.574391 139649090267008 learning.py:507] global step 595: loss = 0.3112 (3.023 sec/step)\n",
            "I0619 19:37:20.617034 139649090267008 learning.py:507] global step 595: loss = 0.3959 (3.041 sec/step)\n",
            "I0619 19:37:23.721976 139649090267008 learning.py:507] global step 595: loss = 0.2981 (3.103 sec/step)\n",
            "I0619 19:37:26.789988 139649090267008 learning.py:507] global step 595: loss = 0.2904 (3.066 sec/step)\n",
            "I0619 19:37:29.747081 139649090267008 learning.py:507] global step 595: loss = 0.3071 (2.955 sec/step)\n",
            "I0619 19:37:32.757016 139649090267008 learning.py:507] global step 595: loss = 0.2506 (3.008 sec/step)\n",
            "I0619 19:37:35.874711 139649090267008 learning.py:507] global step 595: loss = 0.3440 (3.116 sec/step)\n",
            "I0619 19:37:38.858119 139649090267008 learning.py:507] global step 595: loss = 0.2831 (2.982 sec/step)\n",
            "I0619 19:37:41.833071 139649090267008 learning.py:507] global step 596: loss = 0.3270 (2.973 sec/step)\n",
            "I0619 19:37:44.804782 139649090267008 learning.py:507] global step 596: loss = 0.3038 (2.970 sec/step)\n",
            "I0619 19:37:47.851201 139649090267008 learning.py:507] global step 596: loss = 0.2815 (3.045 sec/step)\n",
            "I0619 19:37:50.767706 139649090267008 learning.py:507] global step 596: loss = 0.2759 (2.915 sec/step)\n",
            "I0619 19:37:54.012217 139649090267008 learning.py:507] global step 596: loss = 0.2946 (3.243 sec/step)\n",
            "I0619 19:37:57.028885 139649090267008 learning.py:507] global step 596: loss = 0.3061 (3.015 sec/step)\n",
            "I0619 19:38:00.036289 139649090267008 learning.py:507] global step 596: loss = 0.3332 (3.006 sec/step)\n",
            "I0619 19:38:03.306199 139649090267008 learning.py:507] global step 596: loss = 0.2648 (3.268 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:38:06.403331 139649090267008 learning.py:507] global step 597: loss = 0.2594 (3.095 sec/step)\n",
            "I0619 19:38:09.491233 139649090267008 learning.py:507] global step 597: loss = 0.2513 (3.086 sec/step)\n",
            "I0619 19:38:12.509625 139649090267008 learning.py:507] global step 597: loss = 0.2242 (3.017 sec/step)\n",
            "I0619 19:38:15.494003 139649090267008 learning.py:507] global step 597: loss = 0.4243 (2.983 sec/step)\n",
            "I0619 19:38:18.790813 139649090267008 learning.py:507] global step 597: loss = 0.2691 (3.295 sec/step)\n",
            "I0619 19:38:23.470112 139649090267008 learning.py:507] global step 597: loss = 0.3015 (4.671 sec/step)\n",
            "I0619 19:38:26.149250 139646017689344 supervisor.py:1050] Recording summary at step 597.\n",
            "I0619 19:38:27.312676 139649090267008 learning.py:507] global step 597: loss = 0.3048 (3.829 sec/step)\n",
            "I0619 19:38:30.259732 139649090267008 learning.py:507] global step 597: loss = 0.2714 (2.945 sec/step)\n",
            "I0619 19:38:33.285273 139649090267008 learning.py:507] global step 598: loss = 0.2955 (3.023 sec/step)\n",
            "I0619 19:38:36.458979 139649090267008 learning.py:507] global step 598: loss = 0.2848 (3.172 sec/step)\n",
            "I0619 19:38:39.490008 139649090267008 learning.py:507] global step 598: loss = 0.3681 (3.029 sec/step)\n",
            "I0619 19:38:42.478350 139649090267008 learning.py:507] global step 598: loss = 0.2918 (2.987 sec/step)\n",
            "I0619 19:38:45.487665 139649090267008 learning.py:507] global step 598: loss = 0.2495 (3.008 sec/step)\n",
            "I0619 19:38:48.545122 139649090267008 learning.py:507] global step 598: loss = 0.2885 (3.056 sec/step)\n",
            "I0619 19:38:51.541214 139649090267008 learning.py:507] global step 598: loss = 0.3394 (2.994 sec/step)\n",
            "I0619 19:38:54.579127 139649090267008 learning.py:507] global step 598: loss = 0.3460 (3.036 sec/step)\n",
            "I0619 19:38:57.566754 139649090267008 learning.py:507] global step 599: loss = 0.3702 (2.985 sec/step)\n",
            "I0619 19:39:00.528177 139649090267008 learning.py:507] global step 599: loss = 0.2695 (2.960 sec/step)\n",
            "I0619 19:39:03.616899 139649090267008 learning.py:507] global step 599: loss = 0.3117 (3.087 sec/step)\n",
            "I0619 19:39:06.619276 139649090267008 learning.py:507] global step 599: loss = 0.3099 (3.001 sec/step)\n",
            "I0619 19:39:09.700920 139649090267008 learning.py:507] global step 599: loss = 0.2709 (3.080 sec/step)\n",
            "I0619 19:39:12.657433 139649090267008 learning.py:507] global step 599: loss = 0.3935 (2.955 sec/step)\n",
            "I0619 19:39:15.617876 139649090267008 learning.py:507] global step 599: loss = 0.4079 (2.959 sec/step)\n",
            "I0619 19:39:18.608726 139649090267008 learning.py:507] global step 599: loss = 0.3590 (2.989 sec/step)\n",
            "I0619 19:39:21.720603 139649090267008 learning.py:507] global step 600: loss = 0.2909 (3.109 sec/step)\n",
            "I0619 19:39:24.745522 139649090267008 learning.py:507] global step 600: loss = 0.2787 (3.023 sec/step)\n",
            "I0619 19:39:27.734187 139649090267008 learning.py:507] global step 600: loss = 0.3154 (2.982 sec/step)\n",
            "I0619 19:39:30.748141 139649090267008 learning.py:507] global step 600: loss = 0.2887 (3.011 sec/step)\n",
            "I0619 19:39:33.750182 139649090267008 learning.py:507] global step 600: loss = 0.3835 (3.000 sec/step)\n",
            "I0619 19:39:36.772998 139649090267008 learning.py:507] global step 600: loss = 0.3599 (3.021 sec/step)\n",
            "I0619 19:39:39.739111 139649090267008 learning.py:507] global step 600: loss = 0.4333 (2.964 sec/step)\n",
            "I0619 19:39:42.727800 139649090267008 learning.py:507] global step 600: loss = 0.2520 (2.987 sec/step)\n",
            "I0619 19:39:45.710871 139649090267008 learning.py:507] global step 601: loss = 0.3485 (2.980 sec/step)\n",
            "I0619 19:39:48.678233 139649090267008 learning.py:507] global step 601: loss = 0.2847 (2.965 sec/step)\n",
            "I0619 19:39:51.723467 139649090267008 learning.py:507] global step 601: loss = 0.2890 (3.043 sec/step)\n",
            "I0619 19:39:54.686247 139649090267008 learning.py:507] global step 601: loss = 0.3447 (2.961 sec/step)\n",
            "I0619 19:39:57.675640 139649090267008 learning.py:507] global step 601: loss = 0.3985 (2.988 sec/step)\n",
            "I0619 19:40:00.691134 139649090267008 learning.py:507] global step 601: loss = 0.2817 (3.014 sec/step)\n",
            "I0619 19:40:03.708033 139649090267008 learning.py:507] global step 601: loss = 0.3146 (3.015 sec/step)\n",
            "I0619 19:40:06.706476 139649090267008 learning.py:507] global step 601: loss = 0.2886 (2.997 sec/step)\n",
            "I0619 19:40:09.727073 139649090267008 learning.py:507] global step 602: loss = 0.4063 (3.017 sec/step)\n",
            "I0619 19:40:12.719484 139649090267008 learning.py:507] global step 602: loss = 0.3185 (2.990 sec/step)\n",
            "I0619 19:40:15.693058 139649090267008 learning.py:507] global step 602: loss = 0.2537 (2.972 sec/step)\n",
            "I0619 19:40:18.623144 139649090267008 learning.py:507] global step 602: loss = 0.2632 (2.928 sec/step)\n",
            "I0619 19:40:20.931290 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:40:22.503298 139649090267008 learning.py:507] global step 602: loss = 0.2894 (3.854 sec/step)\n",
            "I0619 19:40:27.473807 139646017689344 supervisor.py:1050] Recording summary at step 602.\n",
            "I0619 19:40:28.476225 139649090267008 learning.py:507] global step 602: loss = 0.2700 (5.956 sec/step)\n",
            "I0619 19:40:31.480028 139649090267008 learning.py:507] global step 602: loss = 0.2829 (3.002 sec/step)\n",
            "I0619 19:40:34.518913 139649090267008 learning.py:507] global step 602: loss = 0.3165 (3.037 sec/step)\n",
            "I0619 19:40:37.498132 139649090267008 learning.py:507] global step 603: loss = 0.2450 (2.977 sec/step)\n",
            "I0619 19:40:40.475719 139649090267008 learning.py:507] global step 603: loss = 0.4062 (2.976 sec/step)\n",
            "I0619 19:40:43.478872 139649090267008 learning.py:507] global step 603: loss = 0.3323 (3.001 sec/step)\n",
            "I0619 19:40:46.476255 139649090267008 learning.py:507] global step 603: loss = 0.3751 (2.996 sec/step)\n",
            "I0619 19:40:49.697580 139649090267008 learning.py:507] global step 603: loss = 0.2954 (3.220 sec/step)\n",
            "I0619 19:40:52.764591 139649090267008 learning.py:507] global step 603: loss = 0.3965 (3.065 sec/step)\n",
            "I0619 19:40:55.781365 139649090267008 learning.py:507] global step 603: loss = 0.3650 (3.015 sec/step)\n",
            "I0619 19:40:58.731324 139649090267008 learning.py:507] global step 603: loss = 0.2873 (2.948 sec/step)\n",
            "I0619 19:41:01.750174 139649090267008 learning.py:507] global step 604: loss = 0.2495 (3.017 sec/step)\n",
            "I0619 19:41:04.716106 139649090267008 learning.py:507] global step 604: loss = 0.3164 (2.964 sec/step)\n",
            "I0619 19:41:08.001930 139649090267008 learning.py:507] global step 604: loss = 0.2636 (3.284 sec/step)\n",
            "I0619 19:41:10.984519 139649090267008 learning.py:507] global step 604: loss = 0.2678 (2.981 sec/step)\n",
            "I0619 19:41:13.946100 139649090267008 learning.py:507] global step 604: loss = 0.2561 (2.960 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:41:16.935174 139649090267008 learning.py:507] global step 604: loss = 0.3176 (2.987 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:41:19.985208 139649090267008 learning.py:507] global step 604: loss = 0.3620 (3.048 sec/step)\n",
            "I0619 19:41:23.037086 139649090267008 learning.py:507] global step 604: loss = 0.4335 (3.049 sec/step)\n",
            "I0619 19:41:26.151138 139649090267008 learning.py:507] global step 605: loss = 0.3520 (3.112 sec/step)\n",
            "I0619 19:41:29.147557 139649090267008 learning.py:507] global step 605: loss = 0.3029 (2.994 sec/step)\n",
            "I0619 19:41:32.265264 139649090267008 learning.py:507] global step 605: loss = 0.3673 (3.116 sec/step)\n",
            "I0619 19:41:35.222508 139649090267008 learning.py:507] global step 605: loss = 0.3406 (2.956 sec/step)\n",
            "I0619 19:41:38.175262 139649090267008 learning.py:507] global step 605: loss = 0.2931 (2.951 sec/step)\n",
            "I0619 19:41:41.158249 139649090267008 learning.py:507] global step 605: loss = 0.3766 (2.981 sec/step)\n",
            "I0619 19:41:44.105022 139649090267008 learning.py:507] global step 605: loss = 0.3299 (2.945 sec/step)\n",
            "I0619 19:41:47.861297 139649090267008 learning.py:507] global step 605: loss = 0.2726 (3.755 sec/step)\n",
            "I0619 19:41:50.937022 139649090267008 learning.py:507] global step 606: loss = 0.2867 (3.074 sec/step)\n",
            "I0619 19:41:53.962020 139649090267008 learning.py:507] global step 606: loss = 0.2852 (3.023 sec/step)\n",
            "I0619 19:41:56.884613 139649090267008 learning.py:507] global step 606: loss = 0.3130 (2.921 sec/step)\n",
            "I0619 19:41:59.920855 139649090267008 learning.py:507] global step 606: loss = 0.3224 (3.034 sec/step)\n",
            "I0619 19:42:02.909341 139649090267008 learning.py:507] global step 606: loss = 0.3424 (2.987 sec/step)\n",
            "I0619 19:42:06.541998 139649090267008 learning.py:507] global step 606: loss = 0.3149 (3.631 sec/step)\n",
            "I0619 19:42:09.559735 139649090267008 learning.py:507] global step 606: loss = 0.3571 (3.016 sec/step)\n",
            "I0619 19:42:12.619748 139649090267008 learning.py:507] global step 606: loss = 0.2817 (3.059 sec/step)\n",
            "I0619 19:42:15.637673 139649090267008 learning.py:507] global step 607: loss = 0.3150 (3.015 sec/step)\n",
            "I0619 19:42:18.588568 139649090267008 learning.py:507] global step 607: loss = 0.3689 (2.949 sec/step)\n",
            "I0619 19:42:22.351494 139649090267008 learning.py:507] global step 607: loss = 0.3451 (3.709 sec/step)\n",
            "I0619 19:42:26.026873 139646017689344 supervisor.py:1050] Recording summary at step 607.\n",
            "I0619 19:42:26.750690 139649090267008 learning.py:507] global step 607: loss = 0.3845 (4.397 sec/step)\n",
            "I0619 19:42:29.774268 139649090267008 learning.py:507] global step 607: loss = 0.3739 (3.022 sec/step)\n",
            "I0619 19:42:32.755258 139649090267008 learning.py:507] global step 607: loss = 0.3691 (2.979 sec/step)\n",
            "I0619 19:42:35.704378 139649090267008 learning.py:507] global step 607: loss = 0.3943 (2.947 sec/step)\n",
            "I0619 19:42:38.618059 139649090267008 learning.py:507] global step 607: loss = 0.3555 (2.912 sec/step)\n",
            "I0619 19:42:41.627081 139649090267008 learning.py:507] global step 608: loss = 0.2451 (3.007 sec/step)\n",
            "I0619 19:42:44.608843 139649090267008 learning.py:507] global step 608: loss = 0.3350 (2.979 sec/step)\n",
            "I0619 19:42:47.672761 139649090267008 learning.py:507] global step 608: loss = 0.3105 (3.062 sec/step)\n",
            "I0619 19:42:50.620084 139649090267008 learning.py:507] global step 608: loss = 0.2810 (2.945 sec/step)\n",
            "I0619 19:42:53.609080 139649090267008 learning.py:507] global step 608: loss = 0.3108 (2.987 sec/step)\n",
            "I0619 19:42:56.568893 139649090267008 learning.py:507] global step 608: loss = 0.3709 (2.958 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:42:59.573154 139649090267008 learning.py:507] global step 608: loss = 0.2817 (3.002 sec/step)\n",
            "I0619 19:43:02.560478 139649090267008 learning.py:507] global step 608: loss = 0.2686 (2.985 sec/step)\n",
            "I0619 19:43:05.589394 139649090267008 learning.py:507] global step 609: loss = 0.2849 (3.027 sec/step)\n",
            "I0619 19:43:08.678425 139649090267008 learning.py:507] global step 609: loss = 0.4913 (3.087 sec/step)\n",
            "I0619 19:43:11.661503 139649090267008 learning.py:507] global step 609: loss = 0.3841 (2.981 sec/step)\n",
            "I0619 19:43:14.621303 139649090267008 learning.py:507] global step 609: loss = 0.2410 (2.958 sec/step)\n",
            "I0619 19:43:17.615272 139649090267008 learning.py:507] global step 609: loss = 0.2917 (2.992 sec/step)\n",
            "I0619 19:43:20.645171 139649090267008 learning.py:507] global step 609: loss = 0.3431 (3.028 sec/step)\n",
            "I0619 19:43:24.023597 139649090267008 learning.py:507] global step 609: loss = 0.2754 (3.377 sec/step)\n",
            "I0619 19:43:27.026077 139649090267008 learning.py:507] global step 609: loss = 0.3590 (3.001 sec/step)\n",
            "I0619 19:43:30.123726 139649090267008 learning.py:507] global step 610: loss = 0.4210 (3.095 sec/step)\n",
            "I0619 19:43:33.113456 139649090267008 learning.py:507] global step 610: loss = 0.2227 (2.988 sec/step)\n",
            "I0619 19:43:36.050194 139649090267008 learning.py:507] global step 610: loss = 0.2414 (2.935 sec/step)\n",
            "I0619 19:43:39.010748 139649090267008 learning.py:507] global step 610: loss = 0.2805 (2.958 sec/step)\n",
            "I0619 19:43:42.514132 139649090267008 learning.py:507] global step 610: loss = 0.2567 (3.501 sec/step)\n",
            "I0619 19:43:45.703403 139649090267008 learning.py:507] global step 610: loss = 0.2602 (3.187 sec/step)\n",
            "I0619 19:43:48.714306 139649090267008 learning.py:507] global step 610: loss = 0.2792 (3.009 sec/step)\n",
            "I0619 19:43:51.738235 139649090267008 learning.py:507] global step 610: loss = 0.2958 (3.022 sec/step)\n",
            "I0619 19:43:54.703884 139649090267008 learning.py:507] global step 611: loss = 0.3108 (2.964 sec/step)\n",
            "I0619 19:43:57.741483 139649090267008 learning.py:507] global step 611: loss = 0.3160 (3.036 sec/step)\n",
            "I0619 19:44:00.774433 139649090267008 learning.py:507] global step 611: loss = 0.2751 (3.031 sec/step)\n",
            "I0619 19:44:03.997596 139649090267008 learning.py:507] global step 611: loss = 0.3620 (3.219 sec/step)\n",
            "I0619 19:44:07.064903 139649090267008 learning.py:507] global step 611: loss = 0.2969 (3.066 sec/step)\n",
            "I0619 19:44:10.081074 139649090267008 learning.py:507] global step 611: loss = 0.3088 (3.015 sec/step)\n",
            "I0619 19:44:13.122400 139649090267008 learning.py:507] global step 611: loss = 0.3045 (3.040 sec/step)\n",
            "I0619 19:44:16.286660 139649090267008 learning.py:507] global step 611: loss = 0.3991 (3.162 sec/step)\n",
            "I0619 19:44:19.335163 139649090267008 learning.py:507] global step 612: loss = 0.2778 (3.046 sec/step)\n",
            "I0619 19:44:24.350141 139649090267008 learning.py:507] global step 612: loss = 0.2671 (5.009 sec/step)\n",
            "I0619 19:44:26.347428 139646017689344 supervisor.py:1050] Recording summary at step 612.\n",
            "I0619 19:44:27.969000 139649090267008 learning.py:507] global step 612: loss = 0.2922 (3.613 sec/step)\n",
            "I0619 19:44:30.954554 139649090267008 learning.py:507] global step 612: loss = 0.3022 (2.984 sec/step)\n",
            "I0619 19:44:33.964651 139649090267008 learning.py:507] global step 612: loss = 0.2991 (3.008 sec/step)\n",
            "I0619 19:44:36.941998 139649090267008 learning.py:507] global step 612: loss = 0.2707 (2.976 sec/step)\n",
            "I0619 19:44:40.083138 139649090267008 learning.py:507] global step 612: loss = 0.3258 (3.139 sec/step)\n",
            "I0619 19:44:43.284000 139649090267008 learning.py:507] global step 612: loss = 0.3314 (3.199 sec/step)\n",
            "I0619 19:44:46.318434 139649090267008 learning.py:507] global step 613: loss = 0.2950 (3.032 sec/step)\n",
            "I0619 19:44:49.365719 139649090267008 learning.py:507] global step 613: loss = 0.3800 (3.046 sec/step)\n",
            "I0619 19:44:52.433621 139649090267008 learning.py:507] global step 613: loss = 0.2416 (3.066 sec/step)\n",
            "I0619 19:44:55.374695 139649090267008 learning.py:507] global step 613: loss = 0.2557 (2.939 sec/step)\n",
            "I0619 19:44:58.376634 139649090267008 learning.py:507] global step 613: loss = 0.4112 (3.000 sec/step)\n",
            "I0619 19:45:01.464416 139649090267008 learning.py:507] global step 613: loss = 0.2681 (3.086 sec/step)\n",
            "I0619 19:45:04.455497 139649090267008 learning.py:507] global step 613: loss = 0.2709 (2.989 sec/step)\n",
            "I0619 19:45:07.610291 139649090267008 learning.py:507] global step 613: loss = 0.3302 (3.153 sec/step)\n",
            "I0619 19:45:10.596511 139649090267008 learning.py:507] global step 614: loss = 0.2902 (2.984 sec/step)\n",
            "I0619 19:45:13.768477 139649090267008 learning.py:507] global step 614: loss = 0.2779 (3.170 sec/step)\n",
            "I0619 19:45:16.794809 139649090267008 learning.py:507] global step 614: loss = 0.3278 (3.025 sec/step)\n",
            "I0619 19:45:19.758652 139649090267008 learning.py:507] global step 614: loss = 0.3294 (2.962 sec/step)\n",
            "I0619 19:45:22.770479 139649090267008 learning.py:507] global step 614: loss = 0.2426 (3.010 sec/step)\n",
            "I0619 19:45:25.977396 139649090267008 learning.py:507] global step 614: loss = 0.4359 (3.205 sec/step)\n",
            "I0619 19:45:28.923072 139649090267008 learning.py:507] global step 614: loss = 0.3329 (2.944 sec/step)\n",
            "I0619 19:45:32.165279 139649090267008 learning.py:507] global step 614: loss = 0.2873 (3.240 sec/step)\n",
            "I0619 19:45:35.133857 139649090267008 learning.py:507] global step 615: loss = 0.2839 (2.966 sec/step)\n",
            "I0619 19:45:38.114765 139649090267008 learning.py:507] global step 615: loss = 0.3823 (2.979 sec/step)\n",
            "I0619 19:45:41.046688 139649090267008 learning.py:507] global step 615: loss = 0.2908 (2.930 sec/step)\n",
            "I0619 19:45:43.976574 139649090267008 learning.py:507] global step 615: loss = 0.3485 (2.928 sec/step)\n",
            "I0619 19:45:46.997668 139649090267008 learning.py:507] global step 615: loss = 0.3153 (3.019 sec/step)\n",
            "I0619 19:45:49.955753 139649090267008 learning.py:507] global step 615: loss = 0.2565 (2.956 sec/step)\n",
            "I0619 19:45:52.975795 139649090267008 learning.py:507] global step 615: loss = 0.3711 (3.018 sec/step)\n",
            "I0619 19:45:56.000868 139649090267008 learning.py:507] global step 615: loss = 0.3702 (3.023 sec/step)\n",
            "I0619 19:45:58.998382 139649090267008 learning.py:507] global step 616: loss = 0.3251 (2.995 sec/step)\n",
            "I0619 19:46:02.084619 139649090267008 learning.py:507] global step 616: loss = 0.2807 (3.085 sec/step)\n",
            "I0619 19:46:05.247751 139649090267008 learning.py:507] global step 616: loss = 0.2701 (3.161 sec/step)\n",
            "I0619 19:46:08.227333 139649090267008 learning.py:507] global step 616: loss = 0.3024 (2.978 sec/step)\n",
            "I0619 19:46:11.282377 139649090267008 learning.py:507] global step 616: loss = 0.3320 (3.053 sec/step)\n",
            "I0619 19:46:14.455341 139649090267008 learning.py:507] global step 616: loss = 0.2651 (3.171 sec/step)\n",
            "I0619 19:46:17.439990 139649090267008 learning.py:507] global step 616: loss = 0.3098 (2.983 sec/step)\n",
            "I0619 19:46:20.580942 139649090267008 learning.py:507] global step 616: loss = 0.2993 (3.139 sec/step)\n",
            "I0619 19:46:26.185188 139649090267008 learning.py:507] global step 617: loss = 0.3346 (5.598 sec/step)\n",
            "I0619 19:46:26.800864 139646017689344 supervisor.py:1050] Recording summary at step 617.\n",
            "I0619 19:46:29.180215 139649090267008 learning.py:507] global step 617: loss = 0.3896 (2.992 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:46:32.252009 139649090267008 learning.py:507] global step 617: loss = 0.2539 (3.070 sec/step)\n",
            "I0619 19:46:35.353587 139649090267008 learning.py:507] global step 617: loss = 0.3648 (3.100 sec/step)\n",
            "I0619 19:46:38.362745 139649090267008 learning.py:507] global step 617: loss = 0.3524 (3.007 sec/step)\n",
            "I0619 19:46:41.406261 139649090267008 learning.py:507] global step 617: loss = 0.2765 (3.042 sec/step)\n",
            "I0619 19:46:44.763465 139649090267008 learning.py:507] global step 617: loss = 0.2542 (3.355 sec/step)\n",
            "I0619 19:46:47.790395 139649090267008 learning.py:507] global step 617: loss = 0.2717 (3.025 sec/step)\n",
            "I0619 19:46:50.769831 139649090267008 learning.py:507] global step 618: loss = 0.2928 (2.977 sec/step)\n",
            "I0619 19:46:53.771630 139649090267008 learning.py:507] global step 618: loss = 0.2397 (3.000 sec/step)\n",
            "I0619 19:46:56.790549 139649090267008 learning.py:507] global step 618: loss = 0.3088 (3.017 sec/step)\n",
            "I0619 19:46:59.789551 139649090267008 learning.py:507] global step 618: loss = 0.2729 (2.997 sec/step)\n",
            "I0619 19:47:02.812520 139649090267008 learning.py:507] global step 618: loss = 0.3795 (3.021 sec/step)\n",
            "I0619 19:47:05.813598 139649090267008 learning.py:507] global step 618: loss = 0.3037 (2.999 sec/step)\n",
            "I0619 19:47:08.788284 139649090267008 learning.py:507] global step 618: loss = 0.2745 (2.973 sec/step)\n",
            "I0619 19:47:11.756911 139649090267008 learning.py:507] global step 618: loss = 0.3781 (2.967 sec/step)\n",
            "I0619 19:47:14.744795 139649090267008 learning.py:507] global step 619: loss = 0.3013 (2.986 sec/step)\n",
            "I0619 19:47:17.773594 139649090267008 learning.py:507] global step 619: loss = 0.2875 (3.027 sec/step)\n",
            "I0619 19:47:20.756484 139649090267008 learning.py:507] global step 619: loss = 0.3225 (2.981 sec/step)\n",
            "I0619 19:47:23.721418 139649090267008 learning.py:507] global step 619: loss = 0.3239 (2.963 sec/step)\n",
            "I0619 19:47:26.738166 139649090267008 learning.py:507] global step 619: loss = 0.3033 (3.015 sec/step)\n",
            "I0619 19:47:29.726341 139649090267008 learning.py:507] global step 619: loss = 0.2697 (2.987 sec/step)\n",
            "I0619 19:47:32.673939 139649090267008 learning.py:507] global step 619: loss = 0.3721 (2.946 sec/step)\n",
            "I0619 19:47:35.646070 139649090267008 learning.py:507] global step 619: loss = 0.3432 (2.970 sec/step)\n",
            "I0619 19:47:38.653350 139649090267008 learning.py:507] global step 620: loss = 0.3486 (3.005 sec/step)\n",
            "I0619 19:47:41.621634 139649090267008 learning.py:507] global step 620: loss = 0.3199 (2.967 sec/step)\n",
            "I0619 19:47:44.594505 139649090267008 learning.py:507] global step 620: loss = 0.2688 (2.971 sec/step)\n",
            "I0619 19:47:47.577436 139649090267008 learning.py:507] global step 620: loss = 0.3023 (2.981 sec/step)\n",
            "I0619 19:47:50.464110 139649090267008 learning.py:507] global step 620: loss = 0.3393 (2.885 sec/step)\n",
            "I0619 19:47:53.382267 139649090267008 learning.py:507] global step 620: loss = 0.2352 (2.916 sec/step)\n",
            "I0619 19:47:56.352293 139649090267008 learning.py:507] global step 620: loss = 0.2862 (2.968 sec/step)\n",
            "I0619 19:47:59.337663 139649090267008 learning.py:507] global step 620: loss = 0.2820 (2.984 sec/step)\n",
            "I0619 19:48:02.398323 139649090267008 learning.py:507] global step 621: loss = 0.3479 (3.058 sec/step)\n",
            "I0619 19:48:05.331296 139649090267008 learning.py:507] global step 621: loss = 0.3359 (2.931 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:48:08.335421 139649090267008 learning.py:507] global step 621: loss = 0.2810 (3.003 sec/step)\n",
            "I0619 19:48:11.293609 139649090267008 learning.py:507] global step 621: loss = 0.3065 (2.956 sec/step)\n",
            "I0619 19:48:14.347784 139649090267008 learning.py:507] global step 621: loss = 0.3021 (3.053 sec/step)\n",
            "I0619 19:48:17.271679 139649090267008 learning.py:507] global step 621: loss = 0.3543 (2.922 sec/step)\n",
            "I0619 19:48:20.395086 139649090267008 learning.py:507] global step 621: loss = 0.2440 (3.119 sec/step)\n",
            "I0619 19:48:25.635403 139649090267008 learning.py:507] global step 621: loss = 0.2513 (5.238 sec/step)\n",
            "I0619 19:48:25.812124 139646017689344 supervisor.py:1050] Recording summary at step 621.\n",
            "I0619 19:48:28.853114 139649090267008 learning.py:507] global step 622: loss = 0.2535 (3.214 sec/step)\n",
            "I0619 19:48:31.869848 139649090267008 learning.py:507] global step 622: loss = 0.2986 (3.015 sec/step)\n",
            "I0619 19:48:34.809976 139649090267008 learning.py:507] global step 622: loss = 0.2260 (2.939 sec/step)\n",
            "I0619 19:48:37.759085 139649090267008 learning.py:507] global step 622: loss = 0.2425 (2.947 sec/step)\n",
            "I0619 19:48:40.821554 139649090267008 learning.py:507] global step 622: loss = 0.3382 (3.061 sec/step)\n",
            "I0619 19:48:43.770164 139649090267008 learning.py:507] global step 622: loss = 0.3339 (2.947 sec/step)\n",
            "I0619 19:48:47.038198 139649090267008 learning.py:507] global step 622: loss = 0.3751 (3.266 sec/step)\n",
            "I0619 19:48:49.999334 139649090267008 learning.py:507] global step 622: loss = 0.3818 (2.959 sec/step)\n",
            "I0619 19:48:52.975736 139649090267008 learning.py:507] global step 623: loss = 0.3040 (2.974 sec/step)\n",
            "I0619 19:48:55.952854 139649090267008 learning.py:507] global step 623: loss = 0.3314 (2.975 sec/step)\n",
            "I0619 19:48:59.026624 139649090267008 learning.py:507] global step 623: loss = 0.2939 (3.072 sec/step)\n",
            "I0619 19:49:02.033298 139649090267008 learning.py:507] global step 623: loss = 0.2738 (3.005 sec/step)\n",
            "I0619 19:49:05.242430 139649090267008 learning.py:507] global step 623: loss = 0.2715 (3.207 sec/step)\n",
            "I0619 19:49:08.169631 139649090267008 learning.py:507] global step 623: loss = 0.2624 (2.926 sec/step)\n",
            "I0619 19:49:11.196476 139649090267008 learning.py:507] global step 623: loss = 0.3271 (3.025 sec/step)\n",
            "I0619 19:49:14.150230 139649090267008 learning.py:507] global step 623: loss = 0.2917 (2.952 sec/step)\n",
            "I0619 19:49:17.084834 139649090267008 learning.py:507] global step 624: loss = 0.2549 (2.932 sec/step)\n",
            "I0619 19:49:20.194871 139649090267008 learning.py:507] global step 624: loss = 0.3145 (3.108 sec/step)\n",
            "I0619 19:49:23.164723 139649090267008 learning.py:507] global step 624: loss = 0.2876 (2.968 sec/step)\n",
            "I0619 19:49:26.187575 139649090267008 learning.py:507] global step 624: loss = 0.2557 (3.021 sec/step)\n",
            "I0619 19:49:29.152305 139649090267008 learning.py:507] global step 624: loss = 0.2955 (2.963 sec/step)\n",
            "I0619 19:49:32.134631 139649090267008 learning.py:507] global step 624: loss = 0.3016 (2.981 sec/step)\n",
            "I0619 19:49:35.117573 139649090267008 learning.py:507] global step 624: loss = 0.3058 (2.981 sec/step)\n",
            "I0619 19:49:38.380136 139649090267008 learning.py:507] global step 624: loss = 0.2745 (3.261 sec/step)\n",
            "I0619 19:49:41.358514 139649090267008 learning.py:507] global step 625: loss = 0.2685 (2.976 sec/step)\n",
            "I0619 19:49:44.352346 139649090267008 learning.py:507] global step 625: loss = 0.3303 (2.992 sec/step)\n",
            "I0619 19:49:47.352514 139649090267008 learning.py:507] global step 625: loss = 0.3714 (2.999 sec/step)\n",
            "I0619 19:49:50.346561 139649090267008 learning.py:507] global step 625: loss = 0.3526 (2.992 sec/step)\n",
            "I0619 19:49:53.320832 139649090267008 learning.py:507] global step 625: loss = 0.3060 (2.973 sec/step)\n",
            "I0619 19:49:56.330169 139649090267008 learning.py:507] global step 625: loss = 0.3333 (3.008 sec/step)\n",
            "I0619 19:49:59.271497 139649090267008 learning.py:507] global step 625: loss = 0.2828 (2.940 sec/step)\n",
            "I0619 19:50:02.355173 139649090267008 learning.py:507] global step 625: loss = 0.3011 (3.082 sec/step)\n",
            "I0619 19:50:05.359484 139649090267008 learning.py:507] global step 626: loss = 0.2914 (3.002 sec/step)\n",
            "I0619 19:50:08.423563 139649090267008 learning.py:507] global step 626: loss = 0.4150 (3.063 sec/step)\n",
            "I0619 19:50:11.367708 139649090267008 learning.py:507] global step 626: loss = 0.2626 (2.942 sec/step)\n",
            "I0619 19:50:14.469733 139649090267008 learning.py:507] global step 626: loss = 0.2795 (3.100 sec/step)\n",
            "I0619 19:50:17.421226 139649090267008 learning.py:507] global step 626: loss = 0.3175 (2.950 sec/step)\n",
            "I0619 19:50:20.503762 139649090267008 learning.py:507] global step 626: loss = 0.2833 (3.081 sec/step)\n",
            "I0619 19:50:20.931279 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 19:50:26.888712 139649090267008 learning.py:507] global step 626: loss = 0.3346 (6.383 sec/step)\n",
            "I0619 19:50:28.091483 139646017689344 supervisor.py:1050] Recording summary at step 626.\n",
            "I0619 19:50:30.048831 139649090267008 learning.py:507] global step 626: loss = 0.3570 (3.158 sec/step)\n",
            "I0619 19:50:33.142642 139649090267008 learning.py:507] global step 627: loss = 0.3286 (3.092 sec/step)\n",
            "I0619 19:50:36.145039 139649090267008 learning.py:507] global step 627: loss = 0.2843 (3.000 sec/step)\n",
            "I0619 19:50:39.090072 139649090267008 learning.py:507] global step 627: loss = 0.3825 (2.942 sec/step)\n",
            "I0619 19:50:42.088716 139649090267008 learning.py:507] global step 627: loss = 0.3023 (2.997 sec/step)\n",
            "I0619 19:50:45.119401 139649090267008 learning.py:507] global step 627: loss = 0.2703 (3.029 sec/step)\n",
            "I0619 19:50:48.352441 139649090267008 learning.py:507] global step 627: loss = 0.2940 (3.231 sec/step)\n",
            "I0619 19:50:51.316002 139649090267008 learning.py:507] global step 627: loss = 0.2798 (2.962 sec/step)\n",
            "I0619 19:50:54.262838 139649090267008 learning.py:507] global step 627: loss = 0.3184 (2.945 sec/step)\n",
            "I0619 19:50:57.209175 139649090267008 learning.py:507] global step 628: loss = 0.2745 (2.944 sec/step)\n",
            "I0619 19:51:00.135854 139649090267008 learning.py:507] global step 628: loss = 0.2568 (2.925 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:51:03.052356 139649090267008 learning.py:507] global step 628: loss = 0.3251 (2.915 sec/step)\n",
            "I0619 19:51:06.500273 139649090267008 learning.py:507] global step 628: loss = 0.2924 (3.446 sec/step)\n",
            "I0619 19:51:09.411717 139649090267008 learning.py:507] global step 628: loss = 0.2916 (2.910 sec/step)\n",
            "I0619 19:51:12.365673 139649090267008 learning.py:507] global step 628: loss = 0.3170 (2.952 sec/step)\n",
            "I0619 19:51:15.361462 139649090267008 learning.py:507] global step 628: loss = 0.2653 (2.994 sec/step)\n",
            "I0619 19:51:18.334129 139649090267008 learning.py:507] global step 628: loss = 0.3214 (2.971 sec/step)\n",
            "I0619 19:51:21.270713 139649090267008 learning.py:507] global step 629: loss = 0.2712 (2.934 sec/step)\n",
            "I0619 19:51:24.229740 139649090267008 learning.py:507] global step 629: loss = 0.2771 (2.957 sec/step)\n",
            "I0619 19:51:27.212992 139649090267008 learning.py:507] global step 629: loss = 0.3716 (2.981 sec/step)\n",
            "I0619 19:51:30.218983 139649090267008 learning.py:507] global step 629: loss = 0.2615 (3.004 sec/step)\n",
            "I0619 19:51:33.225428 139649090267008 learning.py:507] global step 629: loss = 0.2638 (3.004 sec/step)\n",
            "I0619 19:51:36.203988 139649090267008 learning.py:507] global step 629: loss = 0.3516 (2.977 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:51:39.134762 139649090267008 learning.py:507] global step 629: loss = 0.3506 (2.929 sec/step)\n",
            "I0619 19:51:42.191983 139649090267008 learning.py:507] global step 629: loss = 0.2885 (3.055 sec/step)\n",
            "I0619 19:51:45.207598 139649090267008 learning.py:507] global step 630: loss = 0.3548 (3.013 sec/step)\n",
            "I0619 19:51:48.224938 139649090267008 learning.py:507] global step 630: loss = 0.4565 (3.015 sec/step)\n",
            "I0619 19:51:51.202826 139649090267008 learning.py:507] global step 630: loss = 0.2680 (2.976 sec/step)\n",
            "I0619 19:51:54.164633 139649090267008 learning.py:507] global step 630: loss = 0.2910 (2.960 sec/step)\n",
            "I0619 19:51:57.126897 139649090267008 learning.py:507] global step 630: loss = 0.2895 (2.961 sec/step)\n",
            "I0619 19:52:00.133742 139649090267008 learning.py:507] global step 630: loss = 0.2670 (3.005 sec/step)\n",
            "I0619 19:52:03.126734 139649090267008 learning.py:507] global step 630: loss = 0.3030 (2.991 sec/step)\n",
            "I0619 19:52:06.093376 139649090267008 learning.py:507] global step 630: loss = 0.2504 (2.965 sec/step)\n",
            "I0619 19:52:09.081803 139649090267008 learning.py:507] global step 631: loss = 0.2576 (2.986 sec/step)\n",
            "I0619 19:52:12.016562 139649090267008 learning.py:507] global step 631: loss = 0.4173 (2.933 sec/step)\n",
            "I0619 19:52:14.943228 139649090267008 learning.py:507] global step 631: loss = 0.4440 (2.925 sec/step)\n",
            "I0619 19:52:17.945760 139649090267008 learning.py:507] global step 631: loss = 0.2609 (3.000 sec/step)\n",
            "I0619 19:52:20.925216 139649090267008 learning.py:507] global step 631: loss = 0.3248 (2.978 sec/step)\n",
            "I0619 19:52:26.191504 139649090267008 learning.py:507] global step 631: loss = 0.2827 (5.264 sec/step)\n",
            "I0619 19:52:26.594430 139646017689344 supervisor.py:1050] Recording summary at step 631.\n",
            "I0619 19:52:29.177906 139649090267008 learning.py:507] global step 631: loss = 0.2775 (2.984 sec/step)\n",
            "I0619 19:52:32.165394 139649090267008 learning.py:507] global step 631: loss = 0.3997 (2.986 sec/step)\n",
            "I0619 19:52:35.086342 139649090267008 learning.py:507] global step 632: loss = 0.2651 (2.919 sec/step)\n",
            "I0619 19:52:38.068708 139649090267008 learning.py:507] global step 632: loss = 0.3775 (2.981 sec/step)\n",
            "I0619 19:52:41.064539 139649090267008 learning.py:507] global step 632: loss = 0.3438 (2.994 sec/step)\n",
            "I0619 19:52:44.109294 139649090267008 learning.py:507] global step 632: loss = 0.3115 (3.043 sec/step)\n",
            "I0619 19:52:47.168059 139649090267008 learning.py:507] global step 632: loss = 0.2640 (3.057 sec/step)\n",
            "I0619 19:52:50.154917 139649090267008 learning.py:507] global step 632: loss = 0.2708 (2.985 sec/step)\n",
            "I0619 19:52:53.175248 139649090267008 learning.py:507] global step 632: loss = 0.3067 (3.018 sec/step)\n",
            "I0619 19:52:56.154245 139649090267008 learning.py:507] global step 632: loss = 0.3254 (2.977 sec/step)\n",
            "I0619 19:52:59.091681 139649090267008 learning.py:507] global step 633: loss = 0.3680 (2.935 sec/step)\n",
            "I0619 19:53:02.049503 139649090267008 learning.py:507] global step 633: loss = 0.2866 (2.956 sec/step)\n",
            "I0619 19:53:05.164637 139649090267008 learning.py:507] global step 633: loss = 0.3565 (3.114 sec/step)\n",
            "I0619 19:53:08.300912 139649090267008 learning.py:507] global step 633: loss = 0.3558 (3.135 sec/step)\n",
            "I0619 19:53:11.278504 139649090267008 learning.py:507] global step 633: loss = 0.2501 (2.976 sec/step)\n",
            "I0619 19:53:14.244276 139649090267008 learning.py:507] global step 633: loss = 0.3701 (2.964 sec/step)\n",
            "I0619 19:53:17.204106 139649090267008 learning.py:507] global step 633: loss = 0.2969 (2.958 sec/step)\n",
            "I0619 19:53:20.149856 139649090267008 learning.py:507] global step 633: loss = 0.2638 (2.944 sec/step)\n",
            "I0619 19:53:23.108979 139649090267008 learning.py:507] global step 634: loss = 0.3720 (2.957 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 19:53:26.144660 139649090267008 learning.py:507] global step 634: loss = 0.3304 (3.034 sec/step)\n",
            "I0619 19:53:29.092912 139649090267008 learning.py:507] global step 634: loss = 0.3168 (2.946 sec/step)\n",
            "I0619 19:53:32.069587 139649090267008 learning.py:507] global step 634: loss = 0.2781 (2.975 sec/step)\n",
            "I0619 19:53:35.027028 139649090267008 learning.py:507] global step 634: loss = 0.2967 (2.956 sec/step)\n",
            "I0619 19:53:37.965425 139649090267008 learning.py:507] global step 634: loss = 0.3253 (2.937 sec/step)\n",
            "I0619 19:53:40.898301 139649090267008 learning.py:507] global step 634: loss = 0.4365 (2.931 sec/step)\n",
            "I0619 19:53:43.862013 139649090267008 learning.py:507] global step 634: loss = 0.2754 (2.962 sec/step)\n",
            "I0619 19:53:46.822231 139649090267008 learning.py:507] global step 635: loss = 0.3548 (2.958 sec/step)\n",
            "I0619 19:53:49.724190 139649090267008 learning.py:507] global step 635: loss = 0.4154 (2.900 sec/step)\n",
            "I0619 19:53:52.742216 139649090267008 learning.py:507] global step 635: loss = 0.3273 (3.016 sec/step)\n",
            "I0619 19:53:55.638005 139649090267008 learning.py:507] global step 635: loss = 0.2916 (2.894 sec/step)\n",
            "I0619 19:53:58.609212 139649090267008 learning.py:507] global step 635: loss = 0.2657 (2.969 sec/step)\n",
            "I0619 19:54:01.640862 139649090267008 learning.py:507] global step 635: loss = 0.3011 (3.030 sec/step)\n",
            "I0619 19:54:04.616755 139649090267008 learning.py:507] global step 635: loss = 0.2726 (2.974 sec/step)\n",
            "I0619 19:54:07.673450 139649090267008 learning.py:507] global step 635: loss = 0.2762 (3.055 sec/step)\n",
            "I0619 19:54:10.680321 139649090267008 learning.py:507] global step 636: loss = 0.4303 (3.005 sec/step)\n",
            "I0619 19:54:13.699330 139649090267008 learning.py:507] global step 636: loss = 0.4018 (3.017 sec/step)\n",
            "I0619 19:54:16.702517 139649090267008 learning.py:507] global step 636: loss = 0.2308 (3.001 sec/step)\n",
            "I0619 19:54:19.600096 139649090267008 learning.py:507] global step 636: loss = 0.3353 (2.896 sec/step)\n",
            "I0619 19:54:24.378114 139649090267008 learning.py:507] global step 636: loss = 0.3722 (4.776 sec/step)\n",
            "I0619 19:54:26.271170 139646017689344 supervisor.py:1050] Recording summary at step 636.\n",
            "I0619 19:54:27.822672 139649090267008 learning.py:507] global step 636: loss = 0.3601 (3.437 sec/step)\n",
            "I0619 19:54:30.779606 139649090267008 learning.py:507] global step 636: loss = 0.2867 (2.955 sec/step)\n",
            "I0619 19:54:33.728625 139649090267008 learning.py:507] global step 636: loss = 0.3015 (2.947 sec/step)\n",
            "I0619 19:54:36.726492 139649090267008 learning.py:507] global step 637: loss = 0.3564 (2.993 sec/step)\n",
            "I0619 19:54:39.704591 139649090267008 learning.py:507] global step 637: loss = 0.2757 (2.976 sec/step)\n",
            "I0619 19:54:42.714257 139649090267008 learning.py:507] global step 637: loss = 0.3260 (3.008 sec/step)\n",
            "I0619 19:54:45.685297 139649090267008 learning.py:507] global step 637: loss = 0.3268 (2.969 sec/step)\n",
            "I0619 19:54:48.663958 139649090267008 learning.py:507] global step 637: loss = 0.3126 (2.977 sec/step)\n",
            "I0619 19:54:51.708418 139649090267008 learning.py:507] global step 637: loss = 0.3331 (3.043 sec/step)\n",
            "I0619 19:54:54.778218 139649090267008 learning.py:507] global step 637: loss = 0.3436 (3.068 sec/step)\n",
            "I0619 19:54:57.806463 139649090267008 learning.py:507] global step 637: loss = 0.3410 (3.026 sec/step)\n",
            "I0619 19:55:00.797825 139649090267008 learning.py:507] global step 638: loss = 0.3060 (2.990 sec/step)\n",
            "I0619 19:55:03.793505 139649090267008 learning.py:507] global step 638: loss = 0.2941 (2.993 sec/step)\n",
            "I0619 19:55:06.700181 139649090267008 learning.py:507] global step 638: loss = 0.2717 (2.905 sec/step)\n",
            "I0619 19:55:10.025450 139649090267008 learning.py:507] global step 638: loss = 0.4182 (3.323 sec/step)\n",
            "I0619 19:55:12.992777 139649090267008 learning.py:507] global step 638: loss = 0.3155 (2.966 sec/step)\n",
            "I0619 19:55:15.969200 139649090267008 learning.py:507] global step 638: loss = 0.2778 (2.975 sec/step)\n",
            "I0619 19:55:18.942905 139649090267008 learning.py:507] global step 638: loss = 0.3543 (2.972 sec/step)\n",
            "I0619 19:55:21.947227 139649090267008 learning.py:507] global step 638: loss = 0.3863 (3.003 sec/step)\n",
            "I0619 19:55:24.909976 139649090267008 learning.py:507] global step 639: loss = 0.3306 (2.960 sec/step)\n",
            "I0619 19:55:28.195039 139649090267008 learning.py:507] global step 639: loss = 0.3115 (3.283 sec/step)\n",
            "I0619 19:55:31.245047 139649090267008 learning.py:507] global step 639: loss = 0.2992 (3.048 sec/step)\n",
            "I0619 19:55:34.313144 139649090267008 learning.py:507] global step 639: loss = 0.3776 (3.066 sec/step)\n",
            "I0619 19:55:37.534552 139649090267008 learning.py:507] global step 639: loss = 0.3498 (3.219 sec/step)\n",
            "I0619 19:55:40.465883 139649090267008 learning.py:507] global step 639: loss = 0.3040 (2.929 sec/step)\n",
            "I0619 19:55:43.456367 139649090267008 learning.py:507] global step 639: loss = 0.4105 (2.989 sec/step)\n",
            "I0619 19:55:46.441046 139649090267008 learning.py:507] global step 639: loss = 0.3105 (2.983 sec/step)\n",
            "I0619 19:55:49.355885 139649090267008 learning.py:507] global step 640: loss = 0.3165 (2.913 sec/step)\n",
            "I0619 19:55:52.359071 139649090267008 learning.py:507] global step 640: loss = 0.3249 (3.001 sec/step)\n",
            "I0619 19:55:55.671900 139649090267008 learning.py:507] global step 640: loss = 0.2850 (3.311 sec/step)\n",
            "I0619 19:55:58.684372 139649090267008 learning.py:507] global step 640: loss = 0.3618 (3.011 sec/step)\n",
            "I0619 19:56:01.631114 139649090267008 learning.py:507] global step 640: loss = 0.3336 (2.945 sec/step)\n",
            "I0619 19:56:04.751286 139649090267008 learning.py:507] global step 640: loss = 0.2594 (3.118 sec/step)\n",
            "I0619 19:56:07.777726 139649090267008 learning.py:507] global step 640: loss = 0.3957 (3.025 sec/step)\n",
            "I0619 19:56:10.745071 139649090267008 learning.py:507] global step 640: loss = 0.3387 (2.966 sec/step)\n",
            "I0619 19:56:13.779567 139649090267008 learning.py:507] global step 641: loss = 0.2445 (3.033 sec/step)\n",
            "I0619 19:56:16.750817 139649090267008 learning.py:507] global step 641: loss = 0.2727 (2.969 sec/step)\n",
            "I0619 19:56:19.687727 139649090267008 learning.py:507] global step 641: loss = 0.3160 (2.935 sec/step)\n",
            "I0619 19:56:24.463506 139649090267008 learning.py:507] global step 641: loss = 0.2793 (4.764 sec/step)\n",
            "I0619 19:56:26.107696 139646017689344 supervisor.py:1050] Recording summary at step 641.\n",
            "I0619 19:56:27.860031 139649090267008 learning.py:507] global step 641: loss = 0.2747 (3.395 sec/step)\n",
            "I0619 19:56:30.989292 139649090267008 learning.py:507] global step 641: loss = 0.2510 (3.128 sec/step)\n",
            "I0619 19:56:33.922076 139649090267008 learning.py:507] global step 641: loss = 0.3165 (2.931 sec/step)\n",
            "I0619 19:56:36.908108 139649090267008 learning.py:507] global step 641: loss = 0.2428 (2.984 sec/step)\n",
            "I0619 19:56:39.836478 139649090267008 learning.py:507] global step 642: loss = 0.3250 (2.926 sec/step)\n",
            "I0619 19:56:42.767682 139649090267008 learning.py:507] global step 642: loss = 0.3236 (2.930 sec/step)\n",
            "I0619 19:56:45.687829 139649090267008 learning.py:507] global step 642: loss = 0.3256 (2.919 sec/step)\n",
            "I0619 19:56:48.957644 139649090267008 learning.py:507] global step 642: loss = 0.2764 (3.268 sec/step)\n",
            "I0619 19:56:51.970488 139649090267008 learning.py:507] global step 642: loss = 0.2487 (3.011 sec/step)\n",
            "I0619 19:56:54.961597 139649090267008 learning.py:507] global step 642: loss = 0.2832 (2.989 sec/step)\n",
            "I0619 19:56:58.028318 139649090267008 learning.py:507] global step 642: loss = 0.2817 (3.065 sec/step)\n",
            "I0619 19:57:01.033444 139649090267008 learning.py:507] global step 642: loss = 0.2604 (3.003 sec/step)\n",
            "I0619 19:57:04.108283 139649090267008 learning.py:507] global step 643: loss = 0.3288 (3.071 sec/step)\n",
            "I0619 19:57:07.100646 139649090267008 learning.py:507] global step 643: loss = 0.3131 (2.991 sec/step)\n",
            "I0619 19:57:10.055453 139649090267008 learning.py:507] global step 643: loss = 0.4191 (2.953 sec/step)\n",
            "I0619 19:57:12.998909 139649090267008 learning.py:507] global step 643: loss = 0.3018 (2.942 sec/step)\n",
            "I0619 19:57:15.957340 139649090267008 learning.py:507] global step 643: loss = 0.3203 (2.957 sec/step)\n",
            "I0619 19:57:18.911830 139649090267008 learning.py:507] global step 643: loss = 0.2797 (2.953 sec/step)\n",
            "I0619 19:57:21.901765 139649090267008 learning.py:507] global step 643: loss = 0.3228 (2.988 sec/step)\n",
            "I0619 19:57:24.850506 139649090267008 learning.py:507] global step 643: loss = 0.3179 (2.947 sec/step)\n",
            "I0619 19:57:27.915947 139649090267008 learning.py:507] global step 644: loss = 0.3085 (3.063 sec/step)\n",
            "I0619 19:57:30.858613 139649090267008 learning.py:507] global step 644: loss = 0.2804 (2.940 sec/step)\n",
            "I0619 19:57:33.734599 139649090267008 learning.py:507] global step 644: loss = 0.2870 (2.874 sec/step)\n",
            "I0619 19:57:36.661873 139649090267008 learning.py:507] global step 644: loss = 0.2714 (2.926 sec/step)\n",
            "I0619 19:57:39.625434 139649090267008 learning.py:507] global step 644: loss = 0.3205 (2.962 sec/step)\n",
            "I0619 19:57:42.543748 139649090267008 learning.py:507] global step 644: loss = 0.2929 (2.916 sec/step)\n",
            "I0619 19:57:45.627817 139649090267008 learning.py:507] global step 644: loss = 0.3112 (3.082 sec/step)\n",
            "I0619 19:57:48.621064 139649090267008 learning.py:507] global step 644: loss = 0.2731 (2.992 sec/step)\n",
            "I0619 19:57:51.656783 139649090267008 learning.py:507] global step 645: loss = 0.2689 (3.033 sec/step)\n",
            "I0619 19:57:54.654255 139649090267008 learning.py:507] global step 645: loss = 0.3738 (2.996 sec/step)\n",
            "I0619 19:57:57.677348 139649090267008 learning.py:507] global step 645: loss = 0.3164 (3.021 sec/step)\n",
            "I0619 19:58:00.679236 139649090267008 learning.py:507] global step 645: loss = 0.3561 (3.000 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 19:58:03.643572 139649090267008 learning.py:507] global step 645: loss = 0.2201 (2.963 sec/step)\n",
            "I0619 19:58:06.589129 139649090267008 learning.py:507] global step 645: loss = 0.3228 (2.944 sec/step)\n",
            "I0619 19:58:09.583915 139649090267008 learning.py:507] global step 645: loss = 0.3164 (2.993 sec/step)\n",
            "I0619 19:58:12.577602 139649090267008 learning.py:507] global step 645: loss = 0.2991 (2.992 sec/step)\n",
            "I0619 19:58:15.570033 139649090267008 learning.py:507] global step 646: loss = 0.3219 (2.990 sec/step)\n",
            "I0619 19:58:18.493172 139649090267008 learning.py:507] global step 646: loss = 0.2997 (2.921 sec/step)\n",
            "I0619 19:58:21.742119 139649090267008 learning.py:507] global step 646: loss = 0.3545 (3.234 sec/step)\n",
            "I0619 19:58:25.965399 139646017689344 supervisor.py:1050] Recording summary at step 646.\n",
            "I0619 19:58:26.516284 139649090267008 learning.py:507] global step 646: loss = 0.2973 (4.749 sec/step)\n",
            "I0619 19:58:29.478088 139649090267008 learning.py:507] global step 646: loss = 0.2693 (2.960 sec/step)\n",
            "I0619 19:58:32.467241 139649090267008 learning.py:507] global step 646: loss = 0.2817 (2.987 sec/step)\n",
            "I0619 19:58:35.423694 139649090267008 learning.py:507] global step 646: loss = 0.3011 (2.955 sec/step)\n",
            "I0619 19:58:38.374857 139649090267008 learning.py:507] global step 646: loss = 0.2599 (2.950 sec/step)\n",
            "I0619 19:58:41.279809 139649090267008 learning.py:507] global step 647: loss = 0.2997 (2.902 sec/step)\n",
            "I0619 19:58:44.243396 139649090267008 learning.py:507] global step 647: loss = 0.3102 (2.962 sec/step)\n",
            "I0619 19:58:47.148914 139649090267008 learning.py:507] global step 647: loss = 0.3045 (2.904 sec/step)\n",
            "I0619 19:58:50.087043 139649090267008 learning.py:507] global step 647: loss = 0.3100 (2.936 sec/step)\n",
            "I0619 19:58:53.110357 139649090267008 learning.py:507] global step 647: loss = 0.2754 (3.021 sec/step)\n",
            "I0619 19:58:56.020103 139649090267008 learning.py:507] global step 647: loss = 0.3248 (2.908 sec/step)\n",
            "I0619 19:58:58.989219 139649090267008 learning.py:507] global step 647: loss = 0.2697 (2.967 sec/step)\n",
            "I0619 19:59:01.977022 139649090267008 learning.py:507] global step 647: loss = 0.2808 (2.986 sec/step)\n",
            "I0619 19:59:05.012739 139649090267008 learning.py:507] global step 648: loss = 0.4277 (3.033 sec/step)\n",
            "I0619 19:59:07.965686 139649090267008 learning.py:507] global step 648: loss = 0.4364 (2.951 sec/step)\n",
            "I0619 19:59:10.932041 139649090267008 learning.py:507] global step 648: loss = 0.2866 (2.965 sec/step)\n",
            "I0619 19:59:13.977359 139649090267008 learning.py:507] global step 648: loss = 0.3191 (3.044 sec/step)\n",
            "I0619 19:59:16.918915 139649090267008 learning.py:507] global step 648: loss = 0.2411 (2.940 sec/step)\n",
            "I0619 19:59:19.850710 139649090267008 learning.py:507] global step 648: loss = 0.3217 (2.930 sec/step)\n",
            "I0619 19:59:22.847473 139649090267008 learning.py:507] global step 648: loss = 0.2614 (2.995 sec/step)\n",
            "I0619 19:59:25.762363 139649090267008 learning.py:507] global step 648: loss = 0.2778 (2.913 sec/step)\n",
            "I0619 19:59:28.738761 139649090267008 learning.py:507] global step 649: loss = 0.2778 (2.974 sec/step)\n",
            "I0619 19:59:31.823133 139649090267008 learning.py:507] global step 649: loss = 0.3103 (3.083 sec/step)\n",
            "I0619 19:59:34.776374 139649090267008 learning.py:507] global step 649: loss = 0.2801 (2.951 sec/step)\n",
            "I0619 19:59:37.768570 139649090267008 learning.py:507] global step 649: loss = 0.3247 (2.990 sec/step)\n",
            "I0619 19:59:40.683241 139649090267008 learning.py:507] global step 649: loss = 0.3252 (2.913 sec/step)\n",
            "I0619 19:59:43.640051 139649090267008 learning.py:507] global step 649: loss = 0.2614 (2.955 sec/step)\n",
            "I0619 19:59:46.578865 139649090267008 learning.py:507] global step 649: loss = 0.2871 (2.937 sec/step)\n",
            "I0619 19:59:49.464743 139649090267008 learning.py:507] global step 649: loss = 0.2509 (2.884 sec/step)\n",
            "I0619 19:59:52.365867 139649090267008 learning.py:507] global step 650: loss = 0.2415 (2.899 sec/step)\n",
            "I0619 19:59:55.301708 139649090267008 learning.py:507] global step 650: loss = 0.2767 (2.934 sec/step)\n",
            "I0619 19:59:58.227908 139649090267008 learning.py:507] global step 650: loss = 0.2670 (2.925 sec/step)\n",
            "I0619 20:00:01.191793 139649090267008 learning.py:507] global step 650: loss = 0.4112 (2.962 sec/step)\n",
            "I0619 20:00:04.193268 139649090267008 learning.py:507] global step 650: loss = 0.2933 (3.000 sec/step)\n",
            "I0619 20:00:07.299479 139649090267008 learning.py:507] global step 650: loss = 0.2742 (3.104 sec/step)\n",
            "I0619 20:00:10.232206 139649090267008 learning.py:507] global step 650: loss = 0.3121 (2.931 sec/step)\n",
            "I0619 20:00:13.147400 139649090267008 learning.py:507] global step 650: loss = 0.3615 (2.913 sec/step)\n",
            "I0619 20:00:16.143344 139649090267008 learning.py:507] global step 651: loss = 0.3477 (2.994 sec/step)\n",
            "I0619 20:00:19.045243 139649090267008 learning.py:507] global step 651: loss = 0.2647 (2.900 sec/step)\n",
            "I0619 20:00:20.931338 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:00:24.354384 139649090267008 learning.py:507] global step 651: loss = 0.3848 (5.297 sec/step)\n",
            "I0619 20:00:27.652930 139646017689344 supervisor.py:1050] Recording summary at step 651.\n",
            "I0619 20:00:28.870167 139649090267008 learning.py:507] global step 651: loss = 0.2845 (4.513 sec/step)\n",
            "I0619 20:00:31.819598 139649090267008 learning.py:507] global step 651: loss = 0.3231 (2.948 sec/step)\n",
            "I0619 20:00:34.751920 139649090267008 learning.py:507] global step 651: loss = 0.2486 (2.931 sec/step)\n",
            "I0619 20:00:37.762133 139649090267008 learning.py:507] global step 651: loss = 0.3820 (3.008 sec/step)\n",
            "I0619 20:00:40.690279 139649090267008 learning.py:507] global step 651: loss = 0.2945 (2.926 sec/step)\n",
            "I0619 20:00:43.643198 139649090267008 learning.py:507] global step 652: loss = 0.4078 (2.950 sec/step)\n",
            "I0619 20:00:46.502781 139649090267008 learning.py:507] global step 652: loss = 0.2379 (2.858 sec/step)\n",
            "I0619 20:00:49.456497 139649090267008 learning.py:507] global step 652: loss = 0.3506 (2.952 sec/step)\n",
            "I0619 20:00:52.450459 139649090267008 learning.py:507] global step 652: loss = 0.3846 (2.992 sec/step)\n",
            "I0619 20:00:55.376343 139649090267008 learning.py:507] global step 652: loss = 0.2907 (2.924 sec/step)\n",
            "I0619 20:00:58.383006 139649090267008 learning.py:507] global step 652: loss = 0.2637 (3.005 sec/step)\n",
            "I0619 20:01:01.349860 139649090267008 learning.py:507] global step 652: loss = 0.3891 (2.965 sec/step)\n",
            "I0619 20:01:04.245815 139649090267008 learning.py:507] global step 652: loss = 0.3171 (2.894 sec/step)\n",
            "I0619 20:01:07.247585 139649090267008 learning.py:507] global step 653: loss = 0.3074 (2.999 sec/step)\n",
            "I0619 20:01:10.246804 139649090267008 learning.py:507] global step 653: loss = 0.2600 (2.998 sec/step)\n",
            "I0619 20:01:13.173916 139649090267008 learning.py:507] global step 653: loss = 0.2758 (2.925 sec/step)\n",
            "I0619 20:01:16.150756 139649090267008 learning.py:507] global step 653: loss = 0.2931 (2.975 sec/step)\n",
            "I0619 20:01:19.109002 139649090267008 learning.py:507] global step 653: loss = 0.2864 (2.957 sec/step)\n",
            "I0619 20:01:22.060473 139649090267008 learning.py:507] global step 653: loss = 0.2703 (2.950 sec/step)\n",
            "I0619 20:01:25.045041 139649090267008 learning.py:507] global step 653: loss = 0.2594 (2.983 sec/step)\n",
            "I0619 20:01:28.009012 139649090267008 learning.py:507] global step 653: loss = 0.3952 (2.962 sec/step)\n",
            "I0619 20:01:31.006429 139649090267008 learning.py:507] global step 654: loss = 0.3264 (2.996 sec/step)\n",
            "I0619 20:01:33.872318 139649090267008 learning.py:507] global step 654: loss = 0.2862 (2.864 sec/step)\n",
            "I0619 20:01:36.902276 139649090267008 learning.py:507] global step 654: loss = 0.3136 (3.028 sec/step)\n",
            "I0619 20:01:39.924365 139649090267008 learning.py:507] global step 654: loss = 0.3449 (3.020 sec/step)\n",
            "I0619 20:01:42.919827 139649090267008 learning.py:507] global step 654: loss = 0.3217 (2.994 sec/step)\n",
            "I0619 20:01:45.848824 139649090267008 learning.py:507] global step 654: loss = 0.2537 (2.927 sec/step)\n",
            "I0619 20:01:48.842097 139649090267008 learning.py:507] global step 654: loss = 0.3102 (2.992 sec/step)\n",
            "I0619 20:01:51.833535 139649090267008 learning.py:507] global step 654: loss = 0.3333 (2.990 sec/step)\n",
            "I0619 20:01:55.150954 139649090267008 learning.py:507] global step 655: loss = 0.3005 (3.315 sec/step)\n",
            "I0619 20:01:58.089901 139649090267008 learning.py:507] global step 655: loss = 0.2769 (2.937 sec/step)\n",
            "I0619 20:02:01.205989 139649090267008 learning.py:507] global step 655: loss = 0.2815 (3.114 sec/step)\n",
            "I0619 20:02:04.210119 139649090267008 learning.py:507] global step 655: loss = 0.4257 (3.002 sec/step)\n",
            "I0619 20:02:07.235125 139649090267008 learning.py:507] global step 655: loss = 0.2740 (3.023 sec/step)\n",
            "I0619 20:02:10.305910 139649090267008 learning.py:507] global step 655: loss = 0.3638 (3.069 sec/step)\n",
            "I0619 20:02:13.536745 139649090267008 learning.py:507] global step 655: loss = 0.3229 (3.229 sec/step)\n",
            "I0619 20:02:16.538027 139649090267008 learning.py:507] global step 655: loss = 0.2539 (2.998 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:02:19.656146 139649090267008 learning.py:507] global step 656: loss = 0.3280 (3.115 sec/step)\n",
            "I0619 20:02:24.445163 139649090267008 learning.py:507] global step 656: loss = 0.2725 (4.786 sec/step)\n",
            "I0619 20:02:26.218677 139646017689344 supervisor.py:1050] Recording summary at step 656.\n",
            "I0619 20:02:27.791199 139649090267008 learning.py:507] global step 656: loss = 0.2453 (3.341 sec/step)\n",
            "I0619 20:02:30.822383 139649090267008 learning.py:507] global step 656: loss = 0.2392 (3.029 sec/step)\n",
            "I0619 20:02:33.789886 139649090267008 learning.py:507] global step 656: loss = 0.3794 (2.966 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:02:36.695991 139649090267008 learning.py:507] global step 656: loss = 0.2459 (2.904 sec/step)\n",
            "I0619 20:02:39.630776 139649090267008 learning.py:507] global step 656: loss = 0.2849 (2.933 sec/step)\n",
            "I0619 20:02:42.657360 139649090267008 learning.py:507] global step 656: loss = 0.2965 (3.024 sec/step)\n",
            "I0619 20:02:45.556846 139649090267008 learning.py:507] global step 657: loss = 0.2637 (2.898 sec/step)\n",
            "I0619 20:02:48.514078 139649090267008 learning.py:507] global step 657: loss = 0.3057 (2.955 sec/step)\n",
            "I0619 20:02:51.549611 139649090267008 learning.py:507] global step 657: loss = 0.3168 (3.034 sec/step)\n",
            "I0619 20:02:54.595128 139649090267008 learning.py:507] global step 657: loss = 0.2508 (3.044 sec/step)\n",
            "I0619 20:02:57.511240 139649090267008 learning.py:507] global step 657: loss = 0.3271 (2.914 sec/step)\n",
            "I0619 20:03:00.444594 139649090267008 learning.py:507] global step 657: loss = 0.3941 (2.932 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:03:03.446148 139649090267008 learning.py:507] global step 657: loss = 0.4381 (3.000 sec/step)\n",
            "I0619 20:03:06.476494 139649090267008 learning.py:507] global step 657: loss = 0.2996 (3.028 sec/step)\n",
            "I0619 20:03:09.459582 139649090267008 learning.py:507] global step 658: loss = 0.3483 (2.979 sec/step)\n",
            "I0619 20:03:12.418785 139649090267008 learning.py:507] global step 658: loss = 0.3045 (2.955 sec/step)\n",
            "I0619 20:03:15.432387 139649090267008 learning.py:507] global step 658: loss = 0.3266 (3.012 sec/step)\n",
            "I0619 20:03:18.418233 139649090267008 learning.py:507] global step 658: loss = 0.2764 (2.984 sec/step)\n",
            "I0619 20:03:21.328751 139649090267008 learning.py:507] global step 658: loss = 0.2884 (2.909 sec/step)\n",
            "I0619 20:03:24.269328 139649090267008 learning.py:507] global step 658: loss = 0.3332 (2.939 sec/step)\n",
            "I0619 20:03:27.263760 139649090267008 learning.py:507] global step 658: loss = 0.3194 (2.993 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:03:30.273042 139649090267008 learning.py:507] global step 658: loss = 0.2729 (3.008 sec/step)\n",
            "I0619 20:03:33.282598 139649090267008 learning.py:507] global step 659: loss = 0.2833 (3.007 sec/step)\n",
            "I0619 20:03:36.321261 139649090267008 learning.py:507] global step 659: loss = 0.2787 (3.037 sec/step)\n",
            "I0619 20:03:39.304811 139649090267008 learning.py:507] global step 659: loss = 0.2959 (2.982 sec/step)\n",
            "I0619 20:03:42.295233 139649090267008 learning.py:507] global step 659: loss = 0.2813 (2.989 sec/step)\n",
            "I0619 20:03:45.249799 139649090267008 learning.py:507] global step 659: loss = 0.3002 (2.953 sec/step)\n",
            "I0619 20:03:48.241551 139649090267008 learning.py:507] global step 659: loss = 0.3375 (2.990 sec/step)\n",
            "I0619 20:03:51.195902 139649090267008 learning.py:507] global step 659: loss = 0.3149 (2.952 sec/step)\n",
            "I0619 20:03:54.126370 139649090267008 learning.py:507] global step 659: loss = 0.2580 (2.928 sec/step)\n",
            "I0619 20:03:57.106581 139649090267008 learning.py:507] global step 660: loss = 0.2790 (2.978 sec/step)\n",
            "I0619 20:04:00.141591 139649090267008 learning.py:507] global step 660: loss = 0.3794 (3.033 sec/step)\n",
            "I0619 20:04:03.214083 139649090267008 learning.py:507] global step 660: loss = 0.2558 (3.071 sec/step)\n",
            "I0619 20:04:06.172092 139649090267008 learning.py:507] global step 660: loss = 0.2594 (2.956 sec/step)\n",
            "I0619 20:04:09.116815 139649090267008 learning.py:507] global step 660: loss = 0.2755 (2.943 sec/step)\n",
            "I0619 20:04:12.084795 139649090267008 learning.py:507] global step 660: loss = 0.2807 (2.966 sec/step)\n",
            "I0619 20:04:15.058644 139649090267008 learning.py:507] global step 660: loss = 0.2886 (2.971 sec/step)\n",
            "I0619 20:04:18.096532 139649090267008 learning.py:507] global step 660: loss = 0.3148 (3.036 sec/step)\n",
            "I0619 20:04:21.218563 139649090267008 learning.py:507] global step 661: loss = 0.2457 (3.119 sec/step)\n",
            "I0619 20:04:25.880723 139646017689344 supervisor.py:1050] Recording summary at step 661.\n",
            "I0619 20:04:26.515264 139649090267008 learning.py:507] global step 661: loss = 0.4001 (5.295 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:04:29.444143 139649090267008 learning.py:507] global step 661: loss = 0.2812 (2.927 sec/step)\n",
            "I0619 20:04:32.384598 139649090267008 learning.py:507] global step 661: loss = 0.2505 (2.939 sec/step)\n",
            "I0619 20:04:35.316320 139649090267008 learning.py:507] global step 661: loss = 0.3319 (2.930 sec/step)\n",
            "I0619 20:04:38.268165 139649090267008 learning.py:507] global step 661: loss = 0.2375 (2.950 sec/step)\n",
            "I0619 20:04:41.202452 139649090267008 learning.py:507] global step 661: loss = 0.2780 (2.933 sec/step)\n",
            "I0619 20:04:44.129653 139649090267008 learning.py:507] global step 661: loss = 0.3183 (2.926 sec/step)\n",
            "I0619 20:04:47.117156 139649090267008 learning.py:507] global step 662: loss = 0.3121 (2.986 sec/step)\n",
            "I0619 20:04:50.063372 139649090267008 learning.py:507] global step 662: loss = 0.3324 (2.944 sec/step)\n",
            "I0619 20:04:53.097990 139649090267008 learning.py:507] global step 662: loss = 0.2974 (3.033 sec/step)\n",
            "I0619 20:04:56.110689 139649090267008 learning.py:507] global step 662: loss = 0.3340 (3.011 sec/step)\n",
            "I0619 20:04:59.065952 139649090267008 learning.py:507] global step 662: loss = 0.2968 (2.954 sec/step)\n",
            "I0619 20:05:02.016294 139649090267008 learning.py:507] global step 662: loss = 0.2479 (2.949 sec/step)\n",
            "I0619 20:05:04.986683 139649090267008 learning.py:507] global step 662: loss = 0.3132 (2.968 sec/step)\n",
            "I0619 20:05:08.005016 139649090267008 learning.py:507] global step 662: loss = 0.3114 (3.017 sec/step)\n",
            "I0619 20:05:10.974402 139649090267008 learning.py:507] global step 663: loss = 0.3381 (2.967 sec/step)\n",
            "I0619 20:05:13.970886 139649090267008 learning.py:507] global step 663: loss = 0.3329 (2.995 sec/step)\n",
            "I0619 20:05:16.931135 139649090267008 learning.py:507] global step 663: loss = 0.3898 (2.959 sec/step)\n",
            "I0619 20:05:19.852201 139649090267008 learning.py:507] global step 663: loss = 0.3640 (2.919 sec/step)\n",
            "I0619 20:05:22.855866 139649090267008 learning.py:507] global step 663: loss = 0.3102 (3.002 sec/step)\n",
            "I0619 20:05:25.820806 139649090267008 learning.py:507] global step 663: loss = 0.2452 (2.963 sec/step)\n",
            "I0619 20:05:28.872633 139649090267008 learning.py:507] global step 663: loss = 0.2904 (3.050 sec/step)\n",
            "I0619 20:05:31.839706 139649090267008 learning.py:507] global step 663: loss = 0.2755 (2.965 sec/step)\n",
            "I0619 20:05:34.764656 139649090267008 learning.py:507] global step 664: loss = 0.2611 (2.923 sec/step)\n",
            "I0619 20:05:37.748848 139649090267008 learning.py:507] global step 664: loss = 0.3488 (2.982 sec/step)\n",
            "I0619 20:05:40.658101 139649090267008 learning.py:507] global step 664: loss = 0.3916 (2.908 sec/step)\n",
            "I0619 20:05:43.800649 139649090267008 learning.py:507] global step 664: loss = 0.3384 (3.141 sec/step)\n",
            "I0619 20:05:46.736250 139649090267008 learning.py:507] global step 664: loss = 0.2453 (2.934 sec/step)\n",
            "I0619 20:05:49.724780 139649090267008 learning.py:507] global step 664: loss = 0.2970 (2.987 sec/step)\n",
            "I0619 20:05:52.671085 139649090267008 learning.py:507] global step 664: loss = 0.2504 (2.945 sec/step)\n",
            "I0619 20:05:55.618440 139649090267008 learning.py:507] global step 664: loss = 0.2728 (2.946 sec/step)\n",
            "I0619 20:05:58.560628 139649090267008 learning.py:507] global step 665: loss = 0.2976 (2.940 sec/step)\n",
            "I0619 20:06:01.840898 139649090267008 learning.py:507] global step 665: loss = 0.3945 (3.279 sec/step)\n",
            "I0619 20:06:04.787545 139649090267008 learning.py:507] global step 665: loss = 0.3615 (2.945 sec/step)\n",
            "I0619 20:06:07.765427 139649090267008 learning.py:507] global step 665: loss = 0.2553 (2.976 sec/step)\n",
            "I0619 20:06:10.780261 139649090267008 learning.py:507] global step 665: loss = 0.2528 (3.013 sec/step)\n",
            "I0619 20:06:13.794669 139649090267008 learning.py:507] global step 665: loss = 0.3419 (3.013 sec/step)\n",
            "I0619 20:06:16.785818 139649090267008 learning.py:507] global step 665: loss = 0.2439 (2.989 sec/step)\n",
            "I0619 20:06:19.736144 139649090267008 learning.py:507] global step 665: loss = 0.2849 (2.948 sec/step)\n",
            "I0619 20:06:24.670541 139649090267008 learning.py:507] global step 666: loss = 0.2791 (4.926 sec/step)\n",
            "I0619 20:06:26.400785 139646017689344 supervisor.py:1050] Recording summary at step 666.\n",
            "I0619 20:06:28.021664 139649090267008 learning.py:507] global step 666: loss = 0.2888 (3.349 sec/step)\n",
            "I0619 20:06:30.986234 139649090267008 learning.py:507] global step 666: loss = 0.3109 (2.963 sec/step)\n",
            "I0619 20:06:33.937794 139649090267008 learning.py:507] global step 666: loss = 0.2257 (2.950 sec/step)\n",
            "I0619 20:06:36.896556 139649090267008 learning.py:507] global step 666: loss = 0.2565 (2.957 sec/step)\n",
            "I0619 20:06:39.843136 139649090267008 learning.py:507] global step 666: loss = 0.4339 (2.945 sec/step)\n",
            "I0619 20:06:43.000790 139649090267008 learning.py:507] global step 666: loss = 0.2532 (3.156 sec/step)\n",
            "I0619 20:06:46.003335 139649090267008 learning.py:507] global step 666: loss = 0.3659 (3.001 sec/step)\n",
            "I0619 20:06:48.943209 139649090267008 learning.py:507] global step 667: loss = 0.3252 (2.938 sec/step)\n",
            "I0619 20:06:51.990506 139649090267008 learning.py:507] global step 667: loss = 0.3784 (3.045 sec/step)\n",
            "I0619 20:06:54.978704 139649090267008 learning.py:507] global step 667: loss = 0.3322 (2.987 sec/step)\n",
            "I0619 20:06:57.894282 139649090267008 learning.py:507] global step 667: loss = 0.3441 (2.914 sec/step)\n",
            "I0619 20:07:00.923794 139649090267008 learning.py:507] global step 667: loss = 0.3071 (3.028 sec/step)\n",
            "I0619 20:07:03.883988 139649090267008 learning.py:507] global step 667: loss = 0.3985 (2.959 sec/step)\n",
            "I0619 20:07:06.949707 139649090267008 learning.py:507] global step 667: loss = 0.2308 (3.064 sec/step)\n",
            "I0619 20:07:09.846313 139649090267008 learning.py:507] global step 667: loss = 0.3196 (2.895 sec/step)\n",
            "I0619 20:07:12.915454 139649090267008 learning.py:507] global step 668: loss = 0.3028 (3.067 sec/step)\n",
            "I0619 20:07:16.070127 139649090267008 learning.py:507] global step 668: loss = 0.2333 (3.153 sec/step)\n",
            "I0619 20:07:19.099452 139649090267008 learning.py:507] global step 668: loss = 0.2823 (3.027 sec/step)\n",
            "I0619 20:07:22.134390 139649090267008 learning.py:507] global step 668: loss = 0.2744 (3.032 sec/step)\n",
            "I0619 20:07:25.161950 139649090267008 learning.py:507] global step 668: loss = 0.3475 (3.026 sec/step)\n",
            "I0619 20:07:28.118985 139649090267008 learning.py:507] global step 668: loss = 0.3093 (2.956 sec/step)\n",
            "I0619 20:07:31.124489 139649090267008 learning.py:507] global step 668: loss = 0.3522 (3.004 sec/step)\n",
            "I0619 20:07:34.256009 139649090267008 learning.py:507] global step 668: loss = 0.2800 (3.130 sec/step)\n",
            "I0619 20:07:37.243344 139649090267008 learning.py:507] global step 669: loss = 0.2914 (2.985 sec/step)\n",
            "I0619 20:07:40.180796 139649090267008 learning.py:507] global step 669: loss = 0.3946 (2.936 sec/step)\n",
            "I0619 20:07:43.115536 139649090267008 learning.py:507] global step 669: loss = 0.2831 (2.933 sec/step)\n",
            "I0619 20:07:46.079916 139649090267008 learning.py:507] global step 669: loss = 0.3128 (2.963 sec/step)\n",
            "I0619 20:07:48.997381 139649090267008 learning.py:507] global step 669: loss = 0.3275 (2.916 sec/step)\n",
            "I0619 20:07:51.928498 139649090267008 learning.py:507] global step 669: loss = 0.2696 (2.929 sec/step)\n",
            "I0619 20:07:55.033078 139649090267008 learning.py:507] global step 669: loss = 0.3215 (3.103 sec/step)\n",
            "I0619 20:07:57.972594 139649090267008 learning.py:507] global step 669: loss = 0.2678 (2.938 sec/step)\n",
            "I0619 20:08:00.972894 139649090267008 learning.py:507] global step 670: loss = 0.3514 (2.998 sec/step)\n",
            "I0619 20:08:03.999272 139649090267008 learning.py:507] global step 670: loss = 0.2808 (3.025 sec/step)\n",
            "I0619 20:08:06.910259 139649090267008 learning.py:507] global step 670: loss = 0.3060 (2.909 sec/step)\n",
            "I0619 20:08:09.843167 139649090267008 learning.py:507] global step 670: loss = 0.5152 (2.931 sec/step)\n",
            "I0619 20:08:12.904781 139649090267008 learning.py:507] global step 670: loss = 0.3341 (3.060 sec/step)\n",
            "I0619 20:08:15.905514 139649090267008 learning.py:507] global step 670: loss = 0.3059 (2.999 sec/step)\n",
            "I0619 20:08:18.921702 139649090267008 learning.py:507] global step 670: loss = 0.3066 (3.014 sec/step)\n",
            "I0619 20:08:23.256768 139649090267008 learning.py:507] global step 670: loss = 0.2807 (4.307 sec/step)\n",
            "I0619 20:08:26.147145 139646017689344 supervisor.py:1050] Recording summary at step 670.\n",
            "I0619 20:08:27.173725 139649090267008 learning.py:507] global step 671: loss = 0.2894 (3.914 sec/step)\n",
            "I0619 20:08:30.083251 139649090267008 learning.py:507] global step 671: loss = 0.4039 (2.908 sec/step)\n",
            "I0619 20:08:32.988641 139649090267008 learning.py:507] global step 671: loss = 0.3420 (2.904 sec/step)\n",
            "I0619 20:08:35.941223 139649090267008 learning.py:507] global step 671: loss = 0.3201 (2.951 sec/step)\n",
            "I0619 20:08:38.909684 139649090267008 learning.py:507] global step 671: loss = 0.2451 (2.965 sec/step)\n",
            "I0619 20:08:41.874241 139649090267008 learning.py:507] global step 671: loss = 0.3303 (2.963 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:08:44.827946 139649090267008 learning.py:507] global step 671: loss = 0.3496 (2.952 sec/step)\n",
            "I0619 20:08:47.773301 139649090267008 learning.py:507] global step 671: loss = 0.2713 (2.944 sec/step)\n",
            "I0619 20:08:50.740936 139649090267008 learning.py:507] global step 672: loss = 0.2479 (2.965 sec/step)\n",
            "I0619 20:08:53.755623 139649090267008 learning.py:507] global step 672: loss = 0.3149 (3.013 sec/step)\n",
            "I0619 20:08:56.758236 139649090267008 learning.py:507] global step 672: loss = 0.2853 (3.001 sec/step)\n",
            "I0619 20:08:59.717150 139649090267008 learning.py:507] global step 672: loss = 0.2931 (2.956 sec/step)\n",
            "I0619 20:09:02.691069 139649090267008 learning.py:507] global step 672: loss = 0.3293 (2.972 sec/step)\n",
            "I0619 20:09:05.842158 139649090267008 learning.py:507] global step 672: loss = 0.3359 (3.149 sec/step)\n",
            "I0619 20:09:08.844599 139649090267008 learning.py:507] global step 672: loss = 0.3931 (3.001 sec/step)\n",
            "I0619 20:09:11.803716 139649090267008 learning.py:507] global step 672: loss = 0.3720 (2.957 sec/step)\n",
            "I0619 20:09:14.756650 139649090267008 learning.py:507] global step 673: loss = 0.2968 (2.951 sec/step)\n",
            "I0619 20:09:17.764566 139649090267008 learning.py:507] global step 673: loss = 0.2663 (3.006 sec/step)\n",
            "I0619 20:09:20.692608 139649090267008 learning.py:507] global step 673: loss = 0.3765 (2.926 sec/step)\n",
            "I0619 20:09:23.761277 139649090267008 learning.py:507] global step 673: loss = 0.2946 (3.066 sec/step)\n",
            "I0619 20:09:26.702470 139649090267008 learning.py:507] global step 673: loss = 0.2799 (2.939 sec/step)\n",
            "I0619 20:09:29.648625 139649090267008 learning.py:507] global step 673: loss = 0.5111 (2.944 sec/step)\n",
            "I0619 20:09:32.594300 139649090267008 learning.py:507] global step 673: loss = 0.3574 (2.944 sec/step)\n",
            "I0619 20:09:35.670566 139649090267008 learning.py:507] global step 673: loss = 0.3861 (3.075 sec/step)\n",
            "I0619 20:09:38.680848 139649090267008 learning.py:507] global step 674: loss = 0.2992 (3.008 sec/step)\n",
            "I0619 20:09:41.607703 139649090267008 learning.py:507] global step 674: loss = 0.2990 (2.925 sec/step)\n",
            "I0619 20:09:44.543171 139649090267008 learning.py:507] global step 674: loss = 0.2794 (2.934 sec/step)\n",
            "I0619 20:09:47.485006 139649090267008 learning.py:507] global step 674: loss = 0.2704 (2.940 sec/step)\n",
            "I0619 20:09:50.438646 139649090267008 learning.py:507] global step 674: loss = 0.3160 (2.952 sec/step)\n",
            "I0619 20:09:53.630303 139649090267008 learning.py:507] global step 674: loss = 0.3449 (3.190 sec/step)\n",
            "I0619 20:09:56.559056 139649090267008 learning.py:507] global step 674: loss = 0.2930 (2.927 sec/step)\n",
            "I0619 20:09:59.532977 139649090267008 learning.py:507] global step 674: loss = 0.2516 (2.972 sec/step)\n",
            "I0619 20:10:02.513663 139649090267008 learning.py:507] global step 675: loss = 0.2903 (2.979 sec/step)\n",
            "I0619 20:10:05.459661 139649090267008 learning.py:507] global step 675: loss = 0.3416 (2.943 sec/step)\n",
            "I0619 20:10:08.445263 139649090267008 learning.py:507] global step 675: loss = 0.3543 (2.984 sec/step)\n",
            "I0619 20:10:11.437323 139649090267008 learning.py:507] global step 675: loss = 0.2862 (2.990 sec/step)\n",
            "I0619 20:10:14.399456 139649090267008 learning.py:507] global step 675: loss = 0.5081 (2.960 sec/step)\n",
            "I0619 20:10:17.385881 139649090267008 learning.py:507] global step 675: loss = 0.3409 (2.985 sec/step)\n",
            "I0619 20:10:20.366164 139649090267008 learning.py:507] global step 675: loss = 0.3380 (2.979 sec/step)\n",
            "I0619 20:10:20.931298 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:10:27.042618 139646017689344 supervisor.py:1050] Recording summary at step 675.\n",
            "I0619 20:10:27.047621 139649090267008 learning.py:507] global step 675: loss = 0.2799 (6.676 sec/step)\n",
            "I0619 20:10:30.044023 139649090267008 learning.py:507] global step 676: loss = 0.3574 (2.992 sec/step)\n",
            "I0619 20:10:33.008432 139649090267008 learning.py:507] global step 676: loss = 0.2909 (2.963 sec/step)\n",
            "I0619 20:10:35.943224 139649090267008 learning.py:507] global step 676: loss = 0.3741 (2.933 sec/step)\n",
            "I0619 20:10:38.876235 139649090267008 learning.py:507] global step 676: loss = 0.2901 (2.931 sec/step)\n",
            "I0619 20:10:41.942761 139649090267008 learning.py:507] global step 676: loss = 0.3763 (3.065 sec/step)\n",
            "I0619 20:10:44.925046 139649090267008 learning.py:507] global step 676: loss = 0.3046 (2.981 sec/step)\n",
            "I0619 20:10:47.904323 139649090267008 learning.py:507] global step 676: loss = 0.2848 (2.977 sec/step)\n",
            "I0619 20:10:50.890943 139649090267008 learning.py:507] global step 676: loss = 0.2598 (2.983 sec/step)\n",
            "I0619 20:10:53.837084 139649090267008 learning.py:507] global step 677: loss = 0.2644 (2.944 sec/step)\n",
            "I0619 20:10:56.834662 139649090267008 learning.py:507] global step 677: loss = 0.2800 (2.996 sec/step)\n",
            "I0619 20:10:59.781821 139649090267008 learning.py:507] global step 677: loss = 0.3575 (2.945 sec/step)\n",
            "I0619 20:11:02.662522 139649090267008 learning.py:507] global step 677: loss = 0.3181 (2.879 sec/step)\n",
            "I0619 20:11:05.632624 139649090267008 learning.py:507] global step 677: loss = 0.2541 (2.968 sec/step)\n",
            "I0619 20:11:08.575895 139649090267008 learning.py:507] global step 677: loss = 0.2734 (2.942 sec/step)\n",
            "I0619 20:11:11.565112 139649090267008 learning.py:507] global step 677: loss = 0.2766 (2.987 sec/step)\n",
            "I0619 20:11:14.578091 139649090267008 learning.py:507] global step 677: loss = 0.2718 (3.011 sec/step)\n",
            "I0619 20:11:17.602037 139649090267008 learning.py:507] global step 678: loss = 0.2490 (3.022 sec/step)\n",
            "I0619 20:11:20.583189 139649090267008 learning.py:507] global step 678: loss = 0.2546 (2.979 sec/step)\n",
            "I0619 20:11:23.520946 139649090267008 learning.py:507] global step 678: loss = 0.3172 (2.936 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:11:26.519374 139649090267008 learning.py:507] global step 678: loss = 0.2575 (2.997 sec/step)\n",
            "I0619 20:11:29.462723 139649090267008 learning.py:507] global step 678: loss = 0.2481 (2.942 sec/step)\n",
            "I0619 20:11:32.430535 139649090267008 learning.py:507] global step 678: loss = 0.2748 (2.966 sec/step)\n",
            "I0619 20:11:35.420427 139649090267008 learning.py:507] global step 678: loss = 0.2835 (2.988 sec/step)\n",
            "I0619 20:11:38.337175 139649090267008 learning.py:507] global step 678: loss = 0.3030 (2.915 sec/step)\n",
            "I0619 20:11:41.351121 139649090267008 learning.py:507] global step 679: loss = 0.2839 (3.012 sec/step)\n",
            "I0619 20:11:44.322296 139649090267008 learning.py:507] global step 679: loss = 0.3241 (2.969 sec/step)\n",
            "I0619 20:11:47.254975 139649090267008 learning.py:507] global step 679: loss = 0.3330 (2.931 sec/step)\n",
            "I0619 20:11:50.220247 139649090267008 learning.py:507] global step 679: loss = 0.2739 (2.964 sec/step)\n",
            "I0619 20:11:53.148346 139649090267008 learning.py:507] global step 679: loss = 0.2514 (2.926 sec/step)\n",
            "I0619 20:11:56.080166 139649090267008 learning.py:507] global step 679: loss = 0.2775 (2.930 sec/step)\n",
            "I0619 20:11:59.071330 139649090267008 learning.py:507] global step 679: loss = 0.2695 (2.989 sec/step)\n",
            "I0619 20:12:01.968906 139649090267008 learning.py:507] global step 679: loss = 0.3038 (2.896 sec/step)\n",
            "I0619 20:12:04.904793 139649090267008 learning.py:507] global step 680: loss = 0.3276 (2.934 sec/step)\n",
            "I0619 20:12:07.780271 139649090267008 learning.py:507] global step 680: loss = 0.2533 (2.871 sec/step)\n",
            "I0619 20:12:10.741193 139649090267008 learning.py:507] global step 680: loss = 0.2639 (2.959 sec/step)\n",
            "I0619 20:12:13.699922 139649090267008 learning.py:507] global step 680: loss = 0.3147 (2.957 sec/step)\n",
            "I0619 20:12:16.717286 139649090267008 learning.py:507] global step 680: loss = 0.3294 (3.016 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:12:19.674841 139649090267008 learning.py:507] global step 680: loss = 0.2897 (2.956 sec/step)\n",
            "I0619 20:12:24.564461 139649090267008 learning.py:507] global step 680: loss = 0.2881 (4.885 sec/step)\n",
            "I0619 20:12:26.263947 139646017689344 supervisor.py:1050] Recording summary at step 680.\n",
            "I0619 20:12:27.892002 139649090267008 learning.py:507] global step 680: loss = 0.3034 (3.325 sec/step)\n",
            "I0619 20:12:30.872885 139649090267008 learning.py:507] global step 681: loss = 0.3612 (2.979 sec/step)\n",
            "I0619 20:12:33.850233 139649090267008 learning.py:507] global step 681: loss = 0.3202 (2.976 sec/step)\n",
            "I0619 20:12:36.773115 139649090267008 learning.py:507] global step 681: loss = 0.2511 (2.921 sec/step)\n",
            "I0619 20:12:39.724248 139649090267008 learning.py:507] global step 681: loss = 0.2894 (2.949 sec/step)\n",
            "I0619 20:12:42.663650 139649090267008 learning.py:507] global step 681: loss = 0.2291 (2.938 sec/step)\n",
            "I0619 20:12:45.600492 139649090267008 learning.py:507] global step 681: loss = 0.3144 (2.935 sec/step)\n",
            "I0619 20:12:48.479495 139649090267008 learning.py:507] global step 681: loss = 0.3480 (2.877 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:12:51.462909 139649090267008 learning.py:507] global step 681: loss = 0.2392 (2.982 sec/step)\n",
            "I0619 20:12:54.419581 139649090267008 learning.py:507] global step 682: loss = 0.3228 (2.954 sec/step)\n",
            "I0619 20:12:57.464385 139649090267008 learning.py:507] global step 682: loss = 0.2858 (3.043 sec/step)\n",
            "I0619 20:13:00.479266 139649090267008 learning.py:507] global step 682: loss = 0.3378 (3.013 sec/step)\n",
            "I0619 20:13:03.514757 139649090267008 learning.py:507] global step 682: loss = 0.2842 (3.034 sec/step)\n",
            "I0619 20:13:06.519040 139649090267008 learning.py:507] global step 682: loss = 0.3175 (3.003 sec/step)\n",
            "I0619 20:13:09.489504 139649090267008 learning.py:507] global step 682: loss = 0.3680 (2.969 sec/step)\n",
            "I0619 20:13:12.409284 139649090267008 learning.py:507] global step 682: loss = 0.3332 (2.918 sec/step)\n",
            "I0619 20:13:15.339901 139649090267008 learning.py:507] global step 682: loss = 0.2664 (2.929 sec/step)\n",
            "I0619 20:13:18.295299 139649090267008 learning.py:507] global step 683: loss = 0.3049 (2.952 sec/step)\n",
            "I0619 20:13:21.282929 139649090267008 learning.py:507] global step 683: loss = 0.4039 (2.986 sec/step)\n",
            "I0619 20:13:24.238580 139649090267008 learning.py:507] global step 683: loss = 0.3190 (2.953 sec/step)\n",
            "I0619 20:13:27.263921 139649090267008 learning.py:507] global step 683: loss = 0.2873 (3.024 sec/step)\n",
            "I0619 20:13:30.202744 139649090267008 learning.py:507] global step 683: loss = 0.3046 (2.937 sec/step)\n",
            "I0619 20:13:33.102771 139649090267008 learning.py:507] global step 683: loss = 0.2722 (2.898 sec/step)\n",
            "I0619 20:13:36.060253 139649090267008 learning.py:507] global step 683: loss = 0.3199 (2.956 sec/step)\n",
            "I0619 20:13:39.012129 139649090267008 learning.py:507] global step 683: loss = 0.2629 (2.950 sec/step)\n",
            "I0619 20:13:42.088570 139649090267008 learning.py:507] global step 684: loss = 0.2847 (3.074 sec/step)\n",
            "I0619 20:13:45.013887 139649090267008 learning.py:507] global step 684: loss = 0.2811 (2.924 sec/step)\n",
            "I0619 20:13:48.055554 139649090267008 learning.py:507] global step 684: loss = 0.3345 (3.040 sec/step)\n",
            "I0619 20:13:51.141140 139649090267008 learning.py:507] global step 684: loss = 0.3198 (3.084 sec/step)\n",
            "I0619 20:13:54.095597 139649090267008 learning.py:507] global step 684: loss = 0.2968 (2.953 sec/step)\n",
            "I0619 20:13:57.046151 139649090267008 learning.py:507] global step 684: loss = 0.3525 (2.949 sec/step)\n",
            "I0619 20:14:00.134751 139649090267008 learning.py:507] global step 684: loss = 0.3380 (3.087 sec/step)\n",
            "I0619 20:14:03.149375 139649090267008 learning.py:507] global step 684: loss = 0.2535 (3.013 sec/step)\n",
            "I0619 20:14:06.171801 139649090267008 learning.py:507] global step 685: loss = 0.2836 (3.021 sec/step)\n",
            "I0619 20:14:09.307256 139649090267008 learning.py:507] global step 685: loss = 0.4060 (3.134 sec/step)\n",
            "I0619 20:14:12.240868 139649090267008 learning.py:507] global step 685: loss = 0.3089 (2.932 sec/step)\n",
            "I0619 20:14:15.300836 139649090267008 learning.py:507] global step 685: loss = 0.3093 (3.058 sec/step)\n",
            "I0619 20:14:18.352421 139649090267008 learning.py:507] global step 685: loss = 0.2587 (3.050 sec/step)\n",
            "I0619 20:14:22.460251 139649090267008 learning.py:507] global step 685: loss = 0.2786 (4.079 sec/step)\n",
            "I0619 20:14:26.037058 139646017689344 supervisor.py:1050] Recording summary at step 685.\n",
            "I0619 20:14:26.920338 139649090267008 learning.py:507] global step 685: loss = 0.2757 (4.458 sec/step)\n",
            "I0619 20:14:29.902793 139649090267008 learning.py:507] global step 685: loss = 0.2819 (2.981 sec/step)\n",
            "I0619 20:14:32.838019 139649090267008 learning.py:507] global step 686: loss = 0.3273 (2.933 sec/step)\n",
            "I0619 20:14:35.817662 139649090267008 learning.py:507] global step 686: loss = 0.3094 (2.978 sec/step)\n",
            "I0619 20:14:39.194048 139649090267008 learning.py:507] global step 686: loss = 0.2922 (3.375 sec/step)\n",
            "I0619 20:14:42.092667 139649090267008 learning.py:507] global step 686: loss = 0.3644 (2.897 sec/step)\n",
            "I0619 20:14:45.007365 139649090267008 learning.py:507] global step 686: loss = 0.2925 (2.913 sec/step)\n",
            "I0619 20:14:47.917477 139649090267008 learning.py:507] global step 686: loss = 0.3625 (2.908 sec/step)\n",
            "I0619 20:14:50.888555 139649090267008 learning.py:507] global step 686: loss = 0.3117 (2.969 sec/step)\n",
            "I0619 20:14:53.945873 139649090267008 learning.py:507] global step 686: loss = 0.3390 (3.056 sec/step)\n",
            "I0619 20:14:56.841411 139649090267008 learning.py:507] global step 687: loss = 0.2629 (2.893 sec/step)\n",
            "I0619 20:14:59.809758 139649090267008 learning.py:507] global step 687: loss = 0.5146 (2.967 sec/step)\n",
            "I0619 20:15:02.811339 139649090267008 learning.py:507] global step 687: loss = 0.3233 (3.000 sec/step)\n",
            "I0619 20:15:05.729798 139649090267008 learning.py:507] global step 687: loss = 0.2900 (2.917 sec/step)\n",
            "I0619 20:15:08.646573 139649090267008 learning.py:507] global step 687: loss = 0.2913 (2.915 sec/step)\n",
            "I0619 20:15:11.794015 139649090267008 learning.py:507] global step 687: loss = 0.2835 (3.146 sec/step)\n",
            "I0619 20:15:14.681155 139649090267008 learning.py:507] global step 687: loss = 0.3808 (2.885 sec/step)\n",
            "I0619 20:15:17.664214 139649090267008 learning.py:507] global step 687: loss = 0.3289 (2.982 sec/step)\n",
            "I0619 20:15:20.645861 139649090267008 learning.py:507] global step 688: loss = 0.4291 (2.979 sec/step)\n",
            "I0619 20:15:23.590181 139649090267008 learning.py:507] global step 688: loss = 0.2541 (2.943 sec/step)\n",
            "I0619 20:15:26.590043 139649090267008 learning.py:507] global step 688: loss = 0.2896 (2.998 sec/step)\n",
            "I0619 20:15:29.549520 139649090267008 learning.py:507] global step 688: loss = 0.3034 (2.958 sec/step)\n",
            "I0619 20:15:32.565592 139649090267008 learning.py:507] global step 688: loss = 0.3952 (3.014 sec/step)\n",
            "I0619 20:15:35.501538 139649090267008 learning.py:507] global step 688: loss = 0.2478 (2.934 sec/step)\n",
            "I0619 20:15:38.475930 139649090267008 learning.py:507] global step 688: loss = 0.2986 (2.972 sec/step)\n",
            "I0619 20:15:41.477437 139649090267008 learning.py:507] global step 688: loss = 0.3507 (2.998 sec/step)\n",
            "I0619 20:15:44.408865 139649090267008 learning.py:507] global step 689: loss = 0.3366 (2.930 sec/step)\n",
            "I0619 20:15:47.378651 139649090267008 learning.py:507] global step 689: loss = 0.3115 (2.968 sec/step)\n",
            "I0619 20:15:50.315646 139649090267008 learning.py:507] global step 689: loss = 0.3508 (2.935 sec/step)\n",
            "I0619 20:15:53.280577 139649090267008 learning.py:507] global step 689: loss = 0.2739 (2.963 sec/step)\n",
            "I0619 20:15:56.261403 139649090267008 learning.py:507] global step 689: loss = 0.3261 (2.979 sec/step)\n",
            "I0619 20:15:59.222823 139649090267008 learning.py:507] global step 689: loss = 0.3673 (2.960 sec/step)\n",
            "I0619 20:16:02.199629 139649090267008 learning.py:507] global step 689: loss = 0.3082 (2.975 sec/step)\n",
            "I0619 20:16:05.181679 139649090267008 learning.py:507] global step 689: loss = 0.3026 (2.980 sec/step)\n",
            "I0619 20:16:08.091346 139649090267008 learning.py:507] global step 690: loss = 0.2663 (2.907 sec/step)\n",
            "I0619 20:16:11.079357 139649090267008 learning.py:507] global step 690: loss = 0.3247 (2.986 sec/step)\n",
            "I0619 20:16:13.964598 139649090267008 learning.py:507] global step 690: loss = 0.2772 (2.883 sec/step)\n",
            "I0619 20:16:16.946449 139649090267008 learning.py:507] global step 690: loss = 0.3158 (2.980 sec/step)\n",
            "I0619 20:16:19.914761 139649090267008 learning.py:507] global step 690: loss = 0.2839 (2.967 sec/step)\n",
            "I0619 20:16:24.881759 139649090267008 learning.py:507] global step 690: loss = 0.2923 (4.963 sec/step)\n",
            "I0619 20:16:26.385954 139646017689344 supervisor.py:1050] Recording summary at step 690.\n",
            "I0619 20:16:28.195184 139649090267008 learning.py:507] global step 690: loss = 0.3062 (3.311 sec/step)\n",
            "I0619 20:16:31.386816 139649090267008 learning.py:507] global step 690: loss = 0.3179 (3.190 sec/step)\n",
            "I0619 20:16:34.343095 139649090267008 learning.py:507] global step 691: loss = 0.3325 (2.954 sec/step)\n",
            "I0619 20:16:37.355676 139649090267008 learning.py:507] global step 691: loss = 0.2964 (3.011 sec/step)\n",
            "I0619 20:16:40.357293 139649090267008 learning.py:507] global step 691: loss = 0.2715 (3.000 sec/step)\n",
            "I0619 20:16:43.285906 139649090267008 learning.py:507] global step 691: loss = 0.2987 (2.927 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:16:46.211449 139649090267008 learning.py:507] global step 691: loss = 0.2969 (2.924 sec/step)\n",
            "I0619 20:16:49.401156 139649090267008 learning.py:507] global step 691: loss = 0.3272 (3.188 sec/step)\n",
            "I0619 20:16:52.371055 139649090267008 learning.py:507] global step 691: loss = 0.2881 (2.968 sec/step)\n",
            "I0619 20:16:55.395640 139649090267008 learning.py:507] global step 691: loss = 0.2669 (3.023 sec/step)\n",
            "I0619 20:16:58.378321 139649090267008 learning.py:507] global step 692: loss = 0.3168 (2.980 sec/step)\n",
            "I0619 20:17:01.343416 139649090267008 learning.py:507] global step 692: loss = 0.2857 (2.963 sec/step)\n",
            "I0619 20:17:04.301977 139649090267008 learning.py:507] global step 692: loss = 0.2787 (2.956 sec/step)\n",
            "I0619 20:17:07.309145 139649090267008 learning.py:507] global step 692: loss = 0.2614 (3.005 sec/step)\n",
            "I0619 20:17:10.267668 139649090267008 learning.py:507] global step 692: loss = 0.2930 (2.957 sec/step)\n",
            "I0619 20:17:13.205299 139649090267008 learning.py:507] global step 692: loss = 0.2540 (2.936 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:17:16.171338 139649090267008 learning.py:507] global step 692: loss = 0.2933 (2.964 sec/step)\n",
            "I0619 20:17:19.075656 139649090267008 learning.py:507] global step 692: loss = 0.3848 (2.903 sec/step)\n",
            "I0619 20:17:22.030529 139649090267008 learning.py:507] global step 693: loss = 0.2677 (2.953 sec/step)\n",
            "I0619 20:17:25.029901 139649090267008 learning.py:507] global step 693: loss = 0.2652 (2.997 sec/step)\n",
            "I0619 20:17:27.989425 139649090267008 learning.py:507] global step 693: loss = 0.2986 (2.958 sec/step)\n",
            "I0619 20:17:30.960772 139649090267008 learning.py:507] global step 693: loss = 0.2551 (2.969 sec/step)\n",
            "I0619 20:17:33.952119 139649090267008 learning.py:507] global step 693: loss = 0.4144 (2.989 sec/step)\n",
            "I0619 20:17:36.946921 139649090267008 learning.py:507] global step 693: loss = 0.2991 (2.993 sec/step)\n",
            "I0619 20:17:39.960140 139649090267008 learning.py:507] global step 693: loss = 0.3495 (3.011 sec/step)\n",
            "I0619 20:17:43.053685 139649090267008 learning.py:507] global step 693: loss = 0.2596 (3.092 sec/step)\n",
            "I0619 20:17:46.025378 139649090267008 learning.py:507] global step 694: loss = 0.2371 (2.970 sec/step)\n",
            "I0619 20:17:48.942279 139649090267008 learning.py:507] global step 694: loss = 0.2420 (2.915 sec/step)\n",
            "I0619 20:17:51.921584 139649090267008 learning.py:507] global step 694: loss = 0.2755 (2.978 sec/step)\n",
            "I0619 20:17:54.886636 139649090267008 learning.py:507] global step 694: loss = 0.2464 (2.963 sec/step)\n",
            "I0619 20:17:57.888394 139649090267008 learning.py:507] global step 694: loss = 0.3862 (3.000 sec/step)\n",
            "I0619 20:18:00.864980 139649090267008 learning.py:507] global step 694: loss = 0.3651 (2.975 sec/step)\n",
            "I0619 20:18:03.849149 139649090267008 learning.py:507] global step 694: loss = 0.2650 (2.982 sec/step)\n",
            "I0619 20:18:06.820749 139649090267008 learning.py:507] global step 694: loss = 0.3285 (2.970 sec/step)\n",
            "I0619 20:18:09.821381 139649090267008 learning.py:507] global step 695: loss = 0.2276 (2.999 sec/step)\n",
            "I0619 20:18:12.759341 139649090267008 learning.py:507] global step 695: loss = 0.2918 (2.936 sec/step)\n",
            "I0619 20:18:15.817975 139649090267008 learning.py:507] global step 695: loss = 0.3095 (3.057 sec/step)\n",
            "I0619 20:18:18.794936 139649090267008 learning.py:507] global step 695: loss = 0.4966 (2.975 sec/step)\n",
            "I0619 20:18:22.998647 139649090267008 learning.py:507] global step 695: loss = 0.2592 (4.197 sec/step)\n",
            "I0619 20:18:25.971704 139646017689344 supervisor.py:1050] Recording summary at step 695.\n",
            "I0619 20:18:27.028956 139649090267008 learning.py:507] global step 695: loss = 0.3785 (4.015 sec/step)\n",
            "I0619 20:18:30.145239 139649090267008 learning.py:507] global step 695: loss = 0.3159 (3.115 sec/step)\n",
            "I0619 20:18:33.082265 139649090267008 learning.py:507] global step 695: loss = 0.3297 (2.935 sec/step)\n",
            "I0619 20:18:36.279082 139649090267008 learning.py:507] global step 696: loss = 0.2671 (3.195 sec/step)\n",
            "I0619 20:18:39.301807 139649090267008 learning.py:507] global step 696: loss = 0.2810 (3.021 sec/step)\n",
            "I0619 20:18:42.228442 139649090267008 learning.py:507] global step 696: loss = 0.3385 (2.925 sec/step)\n",
            "I0619 20:18:45.164598 139649090267008 learning.py:507] global step 696: loss = 0.3250 (2.935 sec/step)\n",
            "I0619 20:18:48.295616 139649090267008 learning.py:507] global step 696: loss = 0.2986 (3.129 sec/step)\n",
            "I0619 20:18:52.465343 139649090267008 learning.py:507] global step 696: loss = 0.3189 (4.168 sec/step)\n",
            "I0619 20:18:55.967434 139649090267008 learning.py:507] global step 696: loss = 0.2740 (3.500 sec/step)\n",
            "I0619 20:18:59.059994 139649090267008 learning.py:507] global step 696: loss = 0.2646 (3.091 sec/step)\n",
            "I0619 20:19:02.049824 139649090267008 learning.py:507] global step 697: loss = 0.2592 (2.987 sec/step)\n",
            "I0619 20:19:05.087007 139649090267008 learning.py:507] global step 697: loss = 0.2606 (3.035 sec/step)\n",
            "I0619 20:19:08.311512 139649090267008 learning.py:507] global step 697: loss = 0.2363 (3.223 sec/step)\n",
            "I0619 20:19:11.876317 139649090267008 learning.py:507] global step 697: loss = 0.2940 (3.563 sec/step)\n",
            "I0619 20:19:14.802202 139649090267008 learning.py:507] global step 697: loss = 0.3159 (2.924 sec/step)\n",
            "I0619 20:19:17.803733 139649090267008 learning.py:507] global step 697: loss = 0.2961 (3.000 sec/step)\n",
            "I0619 20:19:20.813148 139649090267008 learning.py:507] global step 697: loss = 0.3636 (3.008 sec/step)\n",
            "I0619 20:19:23.922305 139649090267008 learning.py:507] global step 697: loss = 0.3782 (3.108 sec/step)\n",
            "I0619 20:19:26.887939 139649090267008 learning.py:507] global step 698: loss = 0.4664 (2.963 sec/step)\n",
            "I0619 20:19:29.880482 139649090267008 learning.py:507] global step 698: loss = 0.3723 (2.991 sec/step)\n",
            "I0619 20:19:32.805243 139649090267008 learning.py:507] global step 698: loss = 0.3248 (2.923 sec/step)\n",
            "I0619 20:19:35.822273 139649090267008 learning.py:507] global step 698: loss = 0.2693 (3.016 sec/step)\n",
            "I0619 20:19:38.823523 139649090267008 learning.py:507] global step 698: loss = 0.2462 (3.000 sec/step)\n",
            "I0619 20:19:42.030049 139649090267008 learning.py:507] global step 698: loss = 0.4229 (3.205 sec/step)\n",
            "I0619 20:19:45.004171 139649090267008 learning.py:507] global step 698: loss = 0.2946 (2.972 sec/step)\n",
            "I0619 20:19:47.988339 139649090267008 learning.py:507] global step 698: loss = 0.3036 (2.983 sec/step)\n",
            "I0619 20:19:50.948320 139649090267008 learning.py:507] global step 699: loss = 0.3527 (2.958 sec/step)\n",
            "I0619 20:19:53.883991 139649090267008 learning.py:507] global step 699: loss = 0.3211 (2.934 sec/step)\n",
            "I0619 20:19:56.804156 139649090267008 learning.py:507] global step 699: loss = 0.2350 (2.918 sec/step)\n",
            "I0619 20:19:59.732052 139649090267008 learning.py:507] global step 699: loss = 0.2347 (2.926 sec/step)\n",
            "I0619 20:20:02.753398 139649090267008 learning.py:507] global step 699: loss = 0.2437 (3.020 sec/step)\n",
            "I0619 20:20:05.710281 139649090267008 learning.py:507] global step 699: loss = 0.3392 (2.955 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:20:08.616169 139649090267008 learning.py:507] global step 699: loss = 0.2644 (2.904 sec/step)\n",
            "I0619 20:20:11.651076 139649090267008 learning.py:507] global step 699: loss = 0.2486 (3.033 sec/step)\n",
            "I0619 20:20:14.557086 139649090267008 learning.py:507] global step 700: loss = 0.3134 (2.904 sec/step)\n",
            "I0619 20:20:17.488862 139649090267008 learning.py:507] global step 700: loss = 0.3067 (2.930 sec/step)\n",
            "I0619 20:20:20.442815 139649090267008 learning.py:507] global step 700: loss = 0.2862 (2.952 sec/step)\n",
            "I0619 20:20:20.931287 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:20:27.237936 139649090267008 learning.py:507] global step 700: loss = 0.2478 (6.792 sec/step)\n",
            "I0619 20:20:27.909529 139646017689344 supervisor.py:1050] Recording summary at step 700.\n",
            "I0619 20:20:30.229904 139649090267008 learning.py:507] global step 700: loss = 0.2751 (2.990 sec/step)\n",
            "I0619 20:20:33.205636 139649090267008 learning.py:507] global step 700: loss = 0.2882 (2.974 sec/step)\n",
            "I0619 20:20:36.155344 139649090267008 learning.py:507] global step 700: loss = 0.2929 (2.948 sec/step)\n",
            "I0619 20:20:39.125906 139649090267008 learning.py:507] global step 700: loss = 0.2522 (2.969 sec/step)\n",
            "I0619 20:20:42.137387 139649090267008 learning.py:507] global step 701: loss = 0.3198 (3.009 sec/step)\n",
            "I0619 20:20:45.108363 139649090267008 learning.py:507] global step 701: loss = 0.3073 (2.969 sec/step)\n",
            "I0619 20:20:48.054118 139649090267008 learning.py:507] global step 701: loss = 0.2715 (2.944 sec/step)\n",
            "I0619 20:20:51.009909 139649090267008 learning.py:507] global step 701: loss = 0.3283 (2.954 sec/step)\n",
            "I0619 20:20:53.972956 139649090267008 learning.py:507] global step 701: loss = 0.3101 (2.961 sec/step)\n",
            "I0619 20:20:56.947087 139649090267008 learning.py:507] global step 701: loss = 0.3932 (2.972 sec/step)\n",
            "I0619 20:20:59.959213 139649090267008 learning.py:507] global step 701: loss = 0.2369 (3.010 sec/step)\n",
            "I0619 20:21:02.961707 139649090267008 learning.py:507] global step 701: loss = 0.3245 (3.001 sec/step)\n",
            "I0619 20:21:05.968797 139649090267008 learning.py:507] global step 702: loss = 0.3217 (3.005 sec/step)\n",
            "I0619 20:21:08.977786 139649090267008 learning.py:507] global step 702: loss = 0.2925 (3.007 sec/step)\n",
            "I0619 20:21:11.997135 139649090267008 learning.py:507] global step 702: loss = 0.2986 (3.018 sec/step)\n",
            "I0619 20:21:14.968034 139649090267008 learning.py:507] global step 702: loss = 0.2910 (2.969 sec/step)\n",
            "I0619 20:21:18.056819 139649090267008 learning.py:507] global step 702: loss = 0.3230 (3.087 sec/step)\n",
            "I0619 20:21:21.067544 139649090267008 learning.py:507] global step 702: loss = 0.2968 (3.009 sec/step)\n",
            "I0619 20:21:24.070392 139649090267008 learning.py:507] global step 702: loss = 0.3259 (3.001 sec/step)\n",
            "I0619 20:21:27.174508 139649090267008 learning.py:507] global step 702: loss = 0.2984 (3.102 sec/step)\n",
            "I0619 20:21:30.104373 139649090267008 learning.py:507] global step 703: loss = 0.2933 (2.927 sec/step)\n",
            "I0619 20:21:33.094948 139649090267008 learning.py:507] global step 703: loss = 0.3588 (2.989 sec/step)\n",
            "I0619 20:21:36.122601 139649090267008 learning.py:507] global step 703: loss = 0.2581 (3.026 sec/step)\n",
            "I0619 20:21:39.123358 139649090267008 learning.py:507] global step 703: loss = 0.3465 (2.999 sec/step)\n",
            "I0619 20:21:42.069652 139649090267008 learning.py:507] global step 703: loss = 0.2421 (2.945 sec/step)\n",
            "I0619 20:21:45.321286 139649090267008 learning.py:507] global step 703: loss = 0.2403 (3.250 sec/step)\n",
            "I0619 20:21:48.299911 139649090267008 learning.py:507] global step 703: loss = 0.3131 (2.977 sec/step)\n",
            "I0619 20:21:51.292716 139649090267008 learning.py:507] global step 703: loss = 0.3582 (2.991 sec/step)\n",
            "I0619 20:21:54.293515 139649090267008 learning.py:507] global step 704: loss = 0.2300 (2.998 sec/step)\n",
            "I0619 20:21:57.238185 139649090267008 learning.py:507] global step 704: loss = 0.4754 (2.943 sec/step)\n",
            "I0619 20:22:00.191654 139649090267008 learning.py:507] global step 704: loss = 0.3410 (2.952 sec/step)\n",
            "I0619 20:22:03.235359 139649090267008 learning.py:507] global step 704: loss = 0.2912 (3.042 sec/step)\n",
            "I0619 20:22:06.219662 139649090267008 learning.py:507] global step 704: loss = 0.3316 (2.983 sec/step)\n",
            "I0619 20:22:09.122484 139649090267008 learning.py:507] global step 704: loss = 0.2293 (2.901 sec/step)\n",
            "I0619 20:22:12.125766 139649090267008 learning.py:507] global step 704: loss = 0.2887 (3.002 sec/step)\n",
            "I0619 20:22:15.111621 139649090267008 learning.py:507] global step 704: loss = 0.2962 (2.984 sec/step)\n",
            "I0619 20:22:18.159532 139649090267008 learning.py:507] global step 705: loss = 0.2857 (3.046 sec/step)\n",
            "I0619 20:22:21.110798 139649090267008 learning.py:507] global step 705: loss = 0.3777 (2.942 sec/step)\n",
            "I0619 20:22:25.689731 139646017689344 supervisor.py:1050] Recording summary at step 705.\n",
            "I0619 20:22:26.384407 139649090267008 learning.py:507] global step 705: loss = 0.3691 (5.190 sec/step)\n",
            "I0619 20:22:29.291809 139649090267008 learning.py:507] global step 705: loss = 0.2741 (2.906 sec/step)\n",
            "I0619 20:22:32.212742 139649090267008 learning.py:507] global step 705: loss = 0.3600 (2.919 sec/step)\n",
            "I0619 20:22:35.328452 139649090267008 learning.py:507] global step 705: loss = 0.2687 (3.114 sec/step)\n",
            "I0619 20:22:38.298711 139649090267008 learning.py:507] global step 705: loss = 0.3375 (2.969 sec/step)\n",
            "I0619 20:22:41.292325 139649090267008 learning.py:507] global step 705: loss = 0.3405 (2.992 sec/step)\n",
            "I0619 20:22:44.404869 139649090267008 learning.py:507] global step 706: loss = 0.3478 (3.110 sec/step)\n",
            "I0619 20:22:47.402298 139649090267008 learning.py:507] global step 706: loss = 0.3058 (2.995 sec/step)\n",
            "I0619 20:22:50.476090 139649090267008 learning.py:507] global step 706: loss = 0.3134 (3.072 sec/step)\n",
            "I0619 20:22:53.517248 139649090267008 learning.py:507] global step 706: loss = 0.2295 (3.039 sec/step)\n",
            "I0619 20:22:56.443102 139649090267008 learning.py:507] global step 706: loss = 0.3154 (2.924 sec/step)\n",
            "I0619 20:23:00.156215 139649090267008 learning.py:507] global step 706: loss = 0.3117 (3.711 sec/step)\n",
            "I0619 20:23:03.092882 139649090267008 learning.py:507] global step 706: loss = 0.3003 (2.935 sec/step)\n",
            "I0619 20:23:06.112752 139649090267008 learning.py:507] global step 706: loss = 0.2453 (3.018 sec/step)\n",
            "I0619 20:23:09.052577 139649090267008 learning.py:507] global step 707: loss = 0.3281 (2.938 sec/step)\n",
            "I0619 20:23:12.080752 139649090267008 learning.py:507] global step 707: loss = 0.2895 (3.027 sec/step)\n",
            "I0619 20:23:15.037693 139649090267008 learning.py:507] global step 707: loss = 0.3098 (2.955 sec/step)\n",
            "I0619 20:23:18.667620 139649090267008 learning.py:507] global step 707: loss = 0.2760 (3.628 sec/step)\n",
            "I0619 20:23:21.633595 139649090267008 learning.py:507] global step 707: loss = 0.3424 (2.964 sec/step)\n",
            "I0619 20:23:24.700165 139649090267008 learning.py:507] global step 707: loss = 0.3138 (3.065 sec/step)\n",
            "I0619 20:23:27.650753 139649090267008 learning.py:507] global step 707: loss = 0.2644 (2.949 sec/step)\n",
            "I0619 20:23:30.601520 139649090267008 learning.py:507] global step 707: loss = 0.2528 (2.949 sec/step)\n",
            "I0619 20:23:33.608133 139649090267008 learning.py:507] global step 708: loss = 0.3537 (3.004 sec/step)\n",
            "I0619 20:23:36.642488 139649090267008 learning.py:507] global step 708: loss = 0.2259 (3.032 sec/step)\n",
            "I0619 20:23:39.660819 139649090267008 learning.py:507] global step 708: loss = 0.4032 (3.017 sec/step)\n",
            "I0619 20:23:42.706807 139649090267008 learning.py:507] global step 708: loss = 0.3536 (3.044 sec/step)\n",
            "I0619 20:23:45.651896 139649090267008 learning.py:507] global step 708: loss = 0.3028 (2.943 sec/step)\n",
            "I0619 20:23:48.555819 139649090267008 learning.py:507] global step 708: loss = 0.2598 (2.902 sec/step)\n",
            "I0619 20:23:51.557834 139649090267008 learning.py:507] global step 708: loss = 0.3034 (3.000 sec/step)\n",
            "I0619 20:23:54.532214 139649090267008 learning.py:507] global step 708: loss = 0.2447 (2.972 sec/step)\n",
            "I0619 20:23:57.463659 139649090267008 learning.py:507] global step 709: loss = 0.2881 (2.929 sec/step)\n",
            "I0619 20:24:00.457414 139649090267008 learning.py:507] global step 709: loss = 0.3610 (2.992 sec/step)\n",
            "I0619 20:24:03.433240 139649090267008 learning.py:507] global step 709: loss = 0.3451 (2.974 sec/step)\n",
            "I0619 20:24:06.397403 139649090267008 learning.py:507] global step 709: loss = 0.2587 (2.962 sec/step)\n",
            "I0619 20:24:09.359557 139649090267008 learning.py:507] global step 709: loss = 0.3957 (2.961 sec/step)\n",
            "I0619 20:24:12.495927 139649090267008 learning.py:507] global step 709: loss = 0.3794 (3.135 sec/step)\n",
            "I0619 20:24:15.421922 139649090267008 learning.py:507] global step 709: loss = 0.3365 (2.924 sec/step)\n",
            "I0619 20:24:18.364546 139649090267008 learning.py:507] global step 709: loss = 0.2672 (2.941 sec/step)\n",
            "I0619 20:24:21.493647 139649090267008 learning.py:507] global step 710: loss = 0.2757 (3.077 sec/step)\n",
            "I0619 20:24:25.956689 139646017689344 supervisor.py:1050] Recording summary at step 710.\n",
            "I0619 20:24:26.539428 139649090267008 learning.py:507] global step 710: loss = 0.2958 (4.997 sec/step)\n",
            "I0619 20:24:29.571425 139649090267008 learning.py:507] global step 710: loss = 0.2515 (3.030 sec/step)\n",
            "I0619 20:24:32.559478 139649090267008 learning.py:507] global step 710: loss = 0.2758 (2.986 sec/step)\n",
            "I0619 20:24:35.573996 139649090267008 learning.py:507] global step 710: loss = 0.2774 (3.013 sec/step)\n",
            "I0619 20:24:38.666780 139649090267008 learning.py:507] global step 710: loss = 0.2760 (3.091 sec/step)\n",
            "I0619 20:24:41.643199 139649090267008 learning.py:507] global step 710: loss = 0.2565 (2.975 sec/step)\n",
            "I0619 20:24:44.540842 139649090267008 learning.py:507] global step 710: loss = 0.2885 (2.896 sec/step)\n",
            "I0619 20:24:47.454660 139649090267008 learning.py:507] global step 711: loss = 0.3629 (2.911 sec/step)\n",
            "I0619 20:24:50.382505 139649090267008 learning.py:507] global step 711: loss = 0.2581 (2.926 sec/step)\n",
            "I0619 20:24:53.373787 139649090267008 learning.py:507] global step 711: loss = 0.2945 (2.990 sec/step)\n",
            "I0619 20:24:56.587597 139649090267008 learning.py:507] global step 711: loss = 0.3420 (3.212 sec/step)\n",
            "I0619 20:24:59.583196 139649090267008 learning.py:507] global step 711: loss = 0.3412 (2.994 sec/step)\n",
            "I0619 20:25:02.656838 139649090267008 learning.py:507] global step 711: loss = 0.2908 (3.072 sec/step)\n",
            "I0619 20:25:05.688168 139649090267008 learning.py:507] global step 711: loss = 0.2650 (3.030 sec/step)\n",
            "I0619 20:25:08.630760 139649090267008 learning.py:507] global step 711: loss = 0.2757 (2.941 sec/step)\n",
            "I0619 20:25:11.837062 139649090267008 learning.py:507] global step 712: loss = 0.3597 (3.204 sec/step)\n",
            "I0619 20:25:14.778798 139649090267008 learning.py:507] global step 712: loss = 0.4078 (2.940 sec/step)\n",
            "I0619 20:25:17.731134 139649090267008 learning.py:507] global step 712: loss = 0.2329 (2.951 sec/step)\n",
            "I0619 20:25:20.733907 139649090267008 learning.py:507] global step 712: loss = 0.2732 (3.001 sec/step)\n",
            "I0619 20:25:23.647501 139649090267008 learning.py:507] global step 712: loss = 0.2873 (2.912 sec/step)\n",
            "I0619 20:25:26.609323 139649090267008 learning.py:507] global step 712: loss = 0.3259 (2.960 sec/step)\n",
            "I0619 20:25:29.940024 139649090267008 learning.py:507] global step 712: loss = 0.2953 (3.329 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:25:33.190275 139649090267008 learning.py:507] global step 712: loss = 0.2448 (3.249 sec/step)\n",
            "I0619 20:25:36.112158 139649090267008 learning.py:507] global step 713: loss = 0.3129 (2.919 sec/step)\n",
            "I0619 20:25:39.090538 139649090267008 learning.py:507] global step 713: loss = 0.4871 (2.977 sec/step)\n",
            "I0619 20:25:42.009919 139649090267008 learning.py:507] global step 713: loss = 0.3035 (2.918 sec/step)\n",
            "I0619 20:25:44.992103 139649090267008 learning.py:507] global step 713: loss = 0.2986 (2.980 sec/step)\n",
            "I0619 20:25:47.928236 139649090267008 learning.py:507] global step 713: loss = 0.2473 (2.934 sec/step)\n",
            "I0619 20:25:51.299739 139649090267008 learning.py:507] global step 713: loss = 0.2760 (3.370 sec/step)\n",
            "I0619 20:25:54.325769 139649090267008 learning.py:507] global step 713: loss = 0.2914 (3.024 sec/step)\n",
            "I0619 20:25:57.285300 139649090267008 learning.py:507] global step 713: loss = 0.2794 (2.958 sec/step)\n",
            "I0619 20:26:00.256748 139649090267008 learning.py:507] global step 714: loss = 0.2493 (2.969 sec/step)\n",
            "I0619 20:26:03.197368 139649090267008 learning.py:507] global step 714: loss = 0.2646 (2.939 sec/step)\n",
            "I0619 20:26:06.107844 139649090267008 learning.py:507] global step 714: loss = 0.2961 (2.909 sec/step)\n",
            "I0619 20:26:09.128453 139649090267008 learning.py:507] global step 714: loss = 0.2897 (3.019 sec/step)\n",
            "I0619 20:26:12.172044 139649090267008 learning.py:507] global step 714: loss = 0.2708 (3.042 sec/step)\n",
            "I0619 20:26:15.166834 139649090267008 learning.py:507] global step 714: loss = 0.2748 (2.993 sec/step)\n",
            "I0619 20:26:18.088879 139649090267008 learning.py:507] global step 714: loss = 0.3446 (2.920 sec/step)\n",
            "I0619 20:26:21.182139 139649090267008 learning.py:507] global step 714: loss = 0.2965 (3.011 sec/step)\n",
            "I0619 20:26:25.771543 139646017689344 supervisor.py:1050] Recording summary at step 714.\n",
            "I0619 20:26:26.314513 139649090267008 learning.py:507] global step 715: loss = 0.2523 (4.994 sec/step)\n",
            "I0619 20:26:29.292268 139649090267008 learning.py:507] global step 715: loss = 0.2966 (2.976 sec/step)\n",
            "I0619 20:26:32.262134 139649090267008 learning.py:507] global step 715: loss = 0.3430 (2.968 sec/step)\n",
            "I0619 20:26:35.181331 139649090267008 learning.py:507] global step 715: loss = 0.3135 (2.918 sec/step)\n",
            "I0619 20:26:38.084766 139649090267008 learning.py:507] global step 715: loss = 0.2675 (2.902 sec/step)\n",
            "I0619 20:26:41.004257 139649090267008 learning.py:507] global step 715: loss = 0.2899 (2.918 sec/step)\n",
            "I0619 20:26:44.013644 139649090267008 learning.py:507] global step 715: loss = 0.2701 (3.008 sec/step)\n",
            "I0619 20:26:46.972063 139649090267008 learning.py:507] global step 715: loss = 0.3659 (2.957 sec/step)\n",
            "I0619 20:26:49.829879 139649090267008 learning.py:507] global step 716: loss = 0.2578 (2.856 sec/step)\n",
            "I0619 20:26:52.830248 139649090267008 learning.py:507] global step 716: loss = 0.2509 (2.999 sec/step)\n",
            "I0619 20:26:55.783498 139649090267008 learning.py:507] global step 716: loss = 0.2714 (2.951 sec/step)\n",
            "I0619 20:26:58.765625 139649090267008 learning.py:507] global step 716: loss = 0.3627 (2.980 sec/step)\n",
            "I0619 20:27:01.699932 139649090267008 learning.py:507] global step 716: loss = 0.2597 (2.933 sec/step)\n",
            "I0619 20:27:04.616464 139649090267008 learning.py:507] global step 716: loss = 0.2722 (2.915 sec/step)\n",
            "I0619 20:27:07.594704 139649090267008 learning.py:507] global step 716: loss = 0.2973 (2.976 sec/step)\n",
            "I0619 20:27:10.606426 139649090267008 learning.py:507] global step 716: loss = 0.2579 (3.010 sec/step)\n",
            "I0619 20:27:13.586976 139649090267008 learning.py:507] global step 717: loss = 0.2503 (2.978 sec/step)\n",
            "I0619 20:27:16.566762 139649090267008 learning.py:507] global step 717: loss = 0.3053 (2.978 sec/step)\n",
            "I0619 20:27:19.535159 139649090267008 learning.py:507] global step 717: loss = 0.2737 (2.967 sec/step)\n",
            "I0619 20:27:22.500339 139649090267008 learning.py:507] global step 717: loss = 0.2556 (2.963 sec/step)\n",
            "I0619 20:27:25.443574 139649090267008 learning.py:507] global step 717: loss = 0.2566 (2.942 sec/step)\n",
            "I0619 20:27:28.518607 139649090267008 learning.py:507] global step 717: loss = 0.2432 (3.073 sec/step)\n",
            "I0619 20:27:31.470324 139649090267008 learning.py:507] global step 717: loss = 0.2885 (2.950 sec/step)\n",
            "I0619 20:27:34.424782 139649090267008 learning.py:507] global step 717: loss = 0.2470 (2.953 sec/step)\n",
            "I0619 20:27:37.381759 139649090267008 learning.py:507] global step 718: loss = 0.2319 (2.955 sec/step)\n",
            "I0619 20:27:40.396878 139649090267008 learning.py:507] global step 718: loss = 0.2244 (3.013 sec/step)\n",
            "I0619 20:27:43.401938 139649090267008 learning.py:507] global step 718: loss = 0.3158 (3.003 sec/step)\n",
            "I0619 20:27:46.386476 139649090267008 learning.py:507] global step 718: loss = 0.2427 (2.983 sec/step)\n",
            "I0619 20:27:49.387029 139649090267008 learning.py:507] global step 718: loss = 0.3121 (2.999 sec/step)\n",
            "I0619 20:27:52.306207 139649090267008 learning.py:507] global step 718: loss = 0.2683 (2.917 sec/step)\n",
            "I0619 20:27:55.348427 139649090267008 learning.py:507] global step 718: loss = 0.2799 (3.040 sec/step)\n",
            "I0619 20:27:58.321652 139649090267008 learning.py:507] global step 718: loss = 0.2844 (2.971 sec/step)\n",
            "I0619 20:28:01.332294 139649090267008 learning.py:507] global step 719: loss = 0.3129 (3.007 sec/step)\n",
            "I0619 20:28:04.383665 139649090267008 learning.py:507] global step 719: loss = 0.2930 (3.049 sec/step)\n",
            "I0619 20:28:07.442514 139649090267008 learning.py:507] global step 719: loss = 0.2860 (3.057 sec/step)\n",
            "I0619 20:28:10.451989 139649090267008 learning.py:507] global step 719: loss = 0.3172 (3.008 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:28:13.418597 139649090267008 learning.py:507] global step 719: loss = 0.2869 (2.965 sec/step)\n",
            "I0619 20:28:16.433737 139649090267008 learning.py:507] global step 719: loss = 0.2152 (3.013 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:28:19.479619 139649090267008 learning.py:507] global step 719: loss = 0.3610 (3.044 sec/step)\n",
            "I0619 20:28:24.337147 139649090267008 learning.py:507] global step 719: loss = 0.3869 (4.852 sec/step)\n",
            "I0619 20:28:26.035624 139646017689344 supervisor.py:1050] Recording summary at step 719.\n",
            "I0619 20:28:27.706801 139649090267008 learning.py:507] global step 720: loss = 0.3439 (3.366 sec/step)\n",
            "I0619 20:28:30.715167 139649090267008 learning.py:507] global step 720: loss = 0.3390 (3.007 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:28:33.663675 139649090267008 learning.py:507] global step 720: loss = 0.2676 (2.947 sec/step)\n",
            "I0619 20:28:36.571414 139649090267008 learning.py:507] global step 720: loss = 0.2962 (2.906 sec/step)\n",
            "I0619 20:28:39.515075 139649090267008 learning.py:507] global step 720: loss = 0.3188 (2.942 sec/step)\n",
            "I0619 20:28:42.468795 139649090267008 learning.py:507] global step 720: loss = 0.2732 (2.952 sec/step)\n",
            "I0619 20:28:45.400161 139649090267008 learning.py:507] global step 720: loss = 0.2673 (2.929 sec/step)\n",
            "I0619 20:28:48.313699 139649090267008 learning.py:507] global step 720: loss = 0.2538 (2.912 sec/step)\n",
            "I0619 20:28:51.283029 139649090267008 learning.py:507] global step 721: loss = 0.2412 (2.967 sec/step)\n",
            "I0619 20:28:54.202262 139649090267008 learning.py:507] global step 721: loss = 0.2309 (2.917 sec/step)\n",
            "I0619 20:28:57.185281 139649090267008 learning.py:507] global step 721: loss = 0.3206 (2.981 sec/step)\n",
            "I0619 20:29:00.174396 139649090267008 learning.py:507] global step 721: loss = 0.3968 (2.987 sec/step)\n",
            "I0619 20:29:03.157780 139649090267008 learning.py:507] global step 721: loss = 0.2134 (2.982 sec/step)\n",
            "I0619 20:29:06.256215 139649090267008 learning.py:507] global step 721: loss = 0.3022 (3.097 sec/step)\n",
            "I0619 20:29:09.235515 139649090267008 learning.py:507] global step 721: loss = 0.2525 (2.978 sec/step)\n",
            "I0619 20:29:12.207690 139649090267008 learning.py:507] global step 721: loss = 0.2915 (2.970 sec/step)\n",
            "I0619 20:29:15.086702 139649090267008 learning.py:507] global step 722: loss = 0.3420 (2.876 sec/step)\n",
            "I0619 20:29:18.028250 139649090267008 learning.py:507] global step 722: loss = 0.2931 (2.939 sec/step)\n",
            "I0619 20:29:20.980752 139649090267008 learning.py:507] global step 722: loss = 0.3033 (2.951 sec/step)\n",
            "I0619 20:29:23.954635 139649090267008 learning.py:507] global step 722: loss = 0.2836 (2.972 sec/step)\n",
            "I0619 20:29:26.955409 139649090267008 learning.py:507] global step 722: loss = 0.2863 (2.999 sec/step)\n",
            "I0619 20:29:29.958137 139649090267008 learning.py:507] global step 722: loss = 0.3668 (3.001 sec/step)\n",
            "I0619 20:29:32.978538 139649090267008 learning.py:507] global step 722: loss = 0.3114 (3.019 sec/step)\n",
            "I0619 20:29:35.907518 139649090267008 learning.py:507] global step 722: loss = 0.2603 (2.927 sec/step)\n",
            "I0619 20:29:38.848288 139649090267008 learning.py:507] global step 723: loss = 0.3438 (2.938 sec/step)\n",
            "I0619 20:29:41.812039 139649090267008 learning.py:507] global step 723: loss = 0.6353 (2.962 sec/step)\n",
            "I0619 20:29:44.795314 139649090267008 learning.py:507] global step 723: loss = 0.2744 (2.982 sec/step)\n",
            "I0619 20:29:47.755702 139649090267008 learning.py:507] global step 723: loss = 0.2514 (2.959 sec/step)\n",
            "I0619 20:29:50.829974 139649090267008 learning.py:507] global step 723: loss = 0.5306 (3.073 sec/step)\n",
            "I0619 20:29:53.767333 139649090267008 learning.py:507] global step 723: loss = 0.2594 (2.936 sec/step)\n",
            "I0619 20:29:56.825173 139649090267008 learning.py:507] global step 723: loss = 0.2607 (3.056 sec/step)\n",
            "I0619 20:29:59.744689 139649090267008 learning.py:507] global step 723: loss = 0.2741 (2.918 sec/step)\n",
            "I0619 20:30:02.748329 139649090267008 learning.py:507] global step 724: loss = 0.2812 (3.001 sec/step)\n",
            "I0619 20:30:05.724981 139649090267008 learning.py:507] global step 724: loss = 0.2533 (2.975 sec/step)\n",
            "I0619 20:30:08.682817 139649090267008 learning.py:507] global step 724: loss = 0.2385 (2.956 sec/step)\n",
            "I0619 20:30:11.694525 139649090267008 learning.py:507] global step 724: loss = 0.2595 (3.010 sec/step)\n",
            "I0619 20:30:14.948366 139649090267008 learning.py:507] global step 724: loss = 0.2799 (3.252 sec/step)\n",
            "I0619 20:30:17.937543 139649090267008 learning.py:507] global step 724: loss = 0.3370 (2.987 sec/step)\n",
            "I0619 20:30:20.886021 139649090267008 learning.py:507] global step 724: loss = 0.2675 (2.947 sec/step)\n",
            "I0619 20:30:20.931638 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:30:27.585150 139649090267008 learning.py:507] global step 724: loss = 0.3033 (6.695 sec/step)\n",
            "I0619 20:30:28.224545 139646017689344 supervisor.py:1050] Recording summary at step 724.\n",
            "I0619 20:30:30.643519 139649090267008 learning.py:507] global step 725: loss = 0.3124 (3.055 sec/step)\n",
            "I0619 20:30:33.665973 139649090267008 learning.py:507] global step 725: loss = 0.3492 (3.021 sec/step)\n",
            "I0619 20:30:36.596466 139649090267008 learning.py:507] global step 725: loss = 0.2808 (2.929 sec/step)\n",
            "I0619 20:30:39.510230 139649090267008 learning.py:507] global step 725: loss = 0.2941 (2.912 sec/step)\n",
            "I0619 20:30:42.489334 139649090267008 learning.py:507] global step 725: loss = 0.3398 (2.977 sec/step)\n",
            "I0619 20:30:45.440307 139649090267008 learning.py:507] global step 725: loss = 0.2943 (2.949 sec/step)\n",
            "I0619 20:30:48.406945 139649090267008 learning.py:507] global step 725: loss = 0.2772 (2.965 sec/step)\n",
            "I0619 20:30:51.362260 139649090267008 learning.py:507] global step 725: loss = 0.3672 (2.953 sec/step)\n",
            "I0619 20:30:54.279744 139649090267008 learning.py:507] global step 726: loss = 0.3104 (2.915 sec/step)\n",
            "I0619 20:30:57.256864 139649090267008 learning.py:507] global step 726: loss = 0.3811 (2.975 sec/step)\n",
            "I0619 20:31:00.236243 139649090267008 learning.py:507] global step 726: loss = 0.3358 (2.978 sec/step)\n",
            "I0619 20:31:03.210696 139649090267008 learning.py:507] global step 726: loss = 0.2585 (2.972 sec/step)\n",
            "I0619 20:31:06.155623 139649090267008 learning.py:507] global step 726: loss = 0.3388 (2.941 sec/step)\n",
            "I0619 20:31:09.159684 139649090267008 learning.py:507] global step 726: loss = 0.2975 (3.002 sec/step)\n",
            "I0619 20:31:12.153888 139649090267008 learning.py:507] global step 726: loss = 0.3081 (2.993 sec/step)\n",
            "I0619 20:31:15.135158 139649090267008 learning.py:507] global step 726: loss = 0.3180 (2.980 sec/step)\n",
            "I0619 20:31:18.071707 139649090267008 learning.py:507] global step 727: loss = 0.3032 (2.934 sec/step)\n",
            "I0619 20:31:21.038611 139649090267008 learning.py:507] global step 727: loss = 0.2469 (2.965 sec/step)\n",
            "I0619 20:31:24.131290 139649090267008 learning.py:507] global step 727: loss = 0.2672 (3.091 sec/step)\n",
            "I0619 20:31:27.117815 139649090267008 learning.py:507] global step 727: loss = 0.3077 (2.985 sec/step)\n",
            "I0619 20:31:30.053026 139649090267008 learning.py:507] global step 727: loss = 0.2807 (2.934 sec/step)\n",
            "I0619 20:31:32.986089 139649090267008 learning.py:507] global step 727: loss = 0.3038 (2.931 sec/step)\n",
            "I0619 20:31:35.941824 139649090267008 learning.py:507] global step 727: loss = 0.2221 (2.954 sec/step)\n",
            "I0619 20:31:38.908532 139649090267008 learning.py:507] global step 727: loss = 0.2499 (2.965 sec/step)\n",
            "I0619 20:31:42.101785 139649090267008 learning.py:507] global step 728: loss = 0.3006 (3.191 sec/step)\n",
            "I0619 20:31:45.118218 139649090267008 learning.py:507] global step 728: loss = 0.2909 (3.015 sec/step)\n",
            "I0619 20:31:48.090651 139649090267008 learning.py:507] global step 728: loss = 0.3141 (2.971 sec/step)\n",
            "I0619 20:31:50.997883 139649090267008 learning.py:507] global step 728: loss = 0.2143 (2.905 sec/step)\n",
            "I0619 20:31:53.971994 139649090267008 learning.py:507] global step 728: loss = 0.2514 (2.972 sec/step)\n",
            "I0619 20:31:56.953057 139649090267008 learning.py:507] global step 728: loss = 0.2836 (2.979 sec/step)\n",
            "I0619 20:31:59.893206 139649090267008 learning.py:507] global step 728: loss = 0.2414 (2.939 sec/step)\n",
            "I0619 20:32:02.854631 139649090267008 learning.py:507] global step 728: loss = 0.2781 (2.960 sec/step)\n",
            "I0619 20:32:05.787750 139649090267008 learning.py:507] global step 729: loss = 0.3092 (2.931 sec/step)\n",
            "I0619 20:32:08.757368 139649090267008 learning.py:507] global step 729: loss = 0.2851 (2.968 sec/step)\n",
            "I0619 20:32:11.699851 139649090267008 learning.py:507] global step 729: loss = 0.3078 (2.941 sec/step)\n",
            "I0619 20:32:14.671023 139649090267008 learning.py:507] global step 729: loss = 0.3118 (2.969 sec/step)\n",
            "I0619 20:32:17.694759 139649090267008 learning.py:507] global step 729: loss = 0.2788 (3.022 sec/step)\n",
            "I0619 20:32:20.642773 139649090267008 learning.py:507] global step 729: loss = 0.3588 (2.946 sec/step)\n",
            "I0619 20:32:25.795110 139649090267008 learning.py:507] global step 729: loss = 0.2399 (5.148 sec/step)\n",
            "I0619 20:32:26.544457 139646017689344 supervisor.py:1050] Recording summary at step 729.\n",
            "I0619 20:32:28.823703 139649090267008 learning.py:507] global step 729: loss = 0.2433 (3.025 sec/step)\n",
            "I0619 20:32:31.727945 139649090267008 learning.py:507] global step 730: loss = 0.2463 (2.902 sec/step)\n",
            "I0619 20:32:34.627284 139649090267008 learning.py:507] global step 730: loss = 0.2559 (2.898 sec/step)\n",
            "I0619 20:32:37.544746 139649090267008 learning.py:507] global step 730: loss = 0.2698 (2.916 sec/step)\n",
            "I0619 20:32:40.525458 139649090267008 learning.py:507] global step 730: loss = 0.3484 (2.977 sec/step)\n",
            "I0619 20:32:43.499857 139649090267008 learning.py:507] global step 730: loss = 0.3113 (2.973 sec/step)\n",
            "I0619 20:32:46.521013 139649090267008 learning.py:507] global step 730: loss = 0.2412 (3.020 sec/step)\n",
            "I0619 20:32:49.456113 139649090267008 learning.py:507] global step 730: loss = 0.3956 (2.934 sec/step)\n",
            "I0619 20:32:52.415143 139649090267008 learning.py:507] global step 730: loss = 0.3214 (2.957 sec/step)\n",
            "I0619 20:32:55.360220 139649090267008 learning.py:507] global step 731: loss = 0.3109 (2.943 sec/step)\n",
            "I0619 20:32:58.311216 139649090267008 learning.py:507] global step 731: loss = 0.3529 (2.949 sec/step)\n",
            "I0619 20:33:01.319611 139649090267008 learning.py:507] global step 731: loss = 0.2376 (3.006 sec/step)\n",
            "I0619 20:33:04.323879 139649090267008 learning.py:507] global step 731: loss = 0.3267 (3.002 sec/step)\n",
            "I0619 20:33:07.313774 139649090267008 learning.py:507] global step 731: loss = 0.3025 (2.988 sec/step)\n",
            "I0619 20:33:10.354093 139649090267008 learning.py:507] global step 731: loss = 0.2342 (3.038 sec/step)\n",
            "I0619 20:33:13.522141 139649090267008 learning.py:507] global step 731: loss = 0.2789 (3.166 sec/step)\n",
            "I0619 20:33:16.577397 139649090267008 learning.py:507] global step 731: loss = 0.2537 (3.054 sec/step)\n",
            "I0619 20:33:19.526818 139649090267008 learning.py:507] global step 732: loss = 0.2654 (2.947 sec/step)\n",
            "I0619 20:33:22.517588 139649090267008 learning.py:507] global step 732: loss = 0.3325 (2.989 sec/step)\n",
            "I0619 20:33:25.458279 139649090267008 learning.py:507] global step 732: loss = 0.3377 (2.939 sec/step)\n",
            "I0619 20:33:28.415924 139649090267008 learning.py:507] global step 732: loss = 0.3666 (2.956 sec/step)\n",
            "I0619 20:33:31.694623 139649090267008 learning.py:507] global step 732: loss = 0.3442 (3.277 sec/step)\n",
            "I0619 20:33:34.585208 139649090267008 learning.py:507] global step 732: loss = 0.3069 (2.889 sec/step)\n",
            "I0619 20:33:37.561090 139649090267008 learning.py:507] global step 732: loss = 0.2501 (2.974 sec/step)\n",
            "I0619 20:33:40.569844 139649090267008 learning.py:507] global step 732: loss = 0.2241 (3.007 sec/step)\n",
            "I0619 20:33:43.611791 139649090267008 learning.py:507] global step 733: loss = 0.2615 (3.040 sec/step)\n",
            "I0619 20:33:46.838243 139649090267008 learning.py:507] global step 733: loss = 0.3066 (3.225 sec/step)\n",
            "I0619 20:33:49.827893 139649090267008 learning.py:507] global step 733: loss = 0.3360 (2.988 sec/step)\n",
            "I0619 20:33:52.775860 139649090267008 learning.py:507] global step 733: loss = 0.2721 (2.946 sec/step)\n",
            "I0619 20:33:55.752840 139649090267008 learning.py:507] global step 733: loss = 0.2890 (2.975 sec/step)\n",
            "I0619 20:33:58.704051 139649090267008 learning.py:507] global step 733: loss = 0.3048 (2.949 sec/step)\n",
            "I0619 20:34:02.074847 139649090267008 learning.py:507] global step 733: loss = 0.2672 (3.369 sec/step)\n",
            "I0619 20:34:05.305815 139649090267008 learning.py:507] global step 733: loss = 0.3214 (3.229 sec/step)\n",
            "I0619 20:34:08.288990 139649090267008 learning.py:507] global step 734: loss = 0.2561 (2.981 sec/step)\n",
            "I0619 20:34:11.197475 139649090267008 learning.py:507] global step 734: loss = 0.3204 (2.907 sec/step)\n",
            "I0619 20:34:14.152923 139649090267008 learning.py:507] global step 734: loss = 0.2699 (2.954 sec/step)\n",
            "I0619 20:34:17.269659 139649090267008 learning.py:507] global step 734: loss = 0.2822 (3.115 sec/step)\n",
            "I0619 20:34:20.481826 139649090267008 learning.py:507] global step 734: loss = 0.3021 (3.210 sec/step)\n",
            "I0619 20:34:26.007476 139649090267008 learning.py:507] global step 734: loss = 0.2686 (5.523 sec/step)\n",
            "I0619 20:34:26.539668 139646017689344 supervisor.py:1050] Recording summary at step 734.\n",
            "I0619 20:34:28.967294 139649090267008 learning.py:507] global step 734: loss = 0.2547 (2.956 sec/step)\n",
            "I0619 20:34:31.920692 139649090267008 learning.py:507] global step 734: loss = 0.3115 (2.950 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:34:35.011924 139649090267008 learning.py:507] global step 735: loss = 0.4456 (3.088 sec/step)\n",
            "I0619 20:34:37.989664 139649090267008 learning.py:507] global step 735: loss = 0.2931 (2.976 sec/step)\n",
            "I0619 20:34:41.271449 139649090267008 learning.py:507] global step 735: loss = 0.4618 (3.280 sec/step)\n",
            "I0619 20:34:44.269451 139649090267008 learning.py:507] global step 735: loss = 0.2949 (2.996 sec/step)\n",
            "I0619 20:34:47.202716 139649090267008 learning.py:507] global step 735: loss = 0.2497 (2.931 sec/step)\n",
            "I0619 20:34:50.184391 139649090267008 learning.py:507] global step 735: loss = 0.3038 (2.980 sec/step)\n",
            "I0619 20:34:53.140129 139649090267008 learning.py:507] global step 735: loss = 0.2737 (2.954 sec/step)\n",
            "I0619 20:34:56.040003 139649090267008 learning.py:507] global step 735: loss = 0.3279 (2.898 sec/step)\n",
            "I0619 20:34:58.970103 139649090267008 learning.py:507] global step 736: loss = 0.3317 (2.928 sec/step)\n",
            "I0619 20:35:01.956472 139649090267008 learning.py:507] global step 736: loss = 0.3944 (2.985 sec/step)\n",
            "I0619 20:35:05.009871 139649090267008 learning.py:507] global step 736: loss = 0.2599 (3.051 sec/step)\n",
            "I0619 20:35:07.938524 139649090267008 learning.py:507] global step 736: loss = 0.3323 (2.927 sec/step)\n",
            "I0619 20:35:10.914283 139649090267008 learning.py:507] global step 736: loss = 0.3652 (2.974 sec/step)\n",
            "I0619 20:35:13.905120 139649090267008 learning.py:507] global step 736: loss = 0.3750 (2.989 sec/step)\n",
            "I0619 20:35:16.875791 139649090267008 learning.py:507] global step 736: loss = 0.2933 (2.969 sec/step)\n",
            "I0619 20:35:19.852857 139649090267008 learning.py:507] global step 736: loss = 0.2786 (2.975 sec/step)\n",
            "I0619 20:35:22.847410 139649090267008 learning.py:507] global step 737: loss = 0.2293 (2.992 sec/step)\n",
            "I0619 20:35:26.164160 139649090267008 learning.py:507] global step 737: loss = 0.2820 (3.315 sec/step)\n",
            "I0619 20:35:29.172253 139649090267008 learning.py:507] global step 737: loss = 0.2758 (3.006 sec/step)\n",
            "I0619 20:35:32.171456 139649090267008 learning.py:507] global step 737: loss = 0.3111 (2.997 sec/step)\n",
            "I0619 20:35:35.161691 139649090267008 learning.py:507] global step 737: loss = 0.3472 (2.989 sec/step)\n",
            "I0619 20:35:38.173744 139649090267008 learning.py:507] global step 737: loss = 0.2623 (3.010 sec/step)\n",
            "I0619 20:35:41.213028 139649090267008 learning.py:507] global step 737: loss = 0.3617 (3.037 sec/step)\n",
            "I0619 20:35:44.722786 139649090267008 learning.py:507] global step 737: loss = 0.2517 (3.508 sec/step)\n",
            "I0619 20:35:47.748859 139649090267008 learning.py:507] global step 738: loss = 0.3565 (3.024 sec/step)\n",
            "I0619 20:35:50.672357 139649090267008 learning.py:507] global step 738: loss = 0.2271 (2.921 sec/step)\n",
            "I0619 20:35:53.542732 139649090267008 learning.py:507] global step 738: loss = 0.2315 (2.869 sec/step)\n",
            "I0619 20:35:56.469554 139649090267008 learning.py:507] global step 738: loss = 0.2769 (2.925 sec/step)\n",
            "I0619 20:35:59.356864 139649090267008 learning.py:507] global step 738: loss = 0.2803 (2.886 sec/step)\n",
            "I0619 20:36:02.397019 139649090267008 learning.py:507] global step 738: loss = 0.3181 (3.038 sec/step)\n",
            "I0619 20:36:05.360247 139649090267008 learning.py:507] global step 738: loss = 0.3134 (2.962 sec/step)\n",
            "I0619 20:36:08.264415 139649090267008 learning.py:507] global step 738: loss = 0.2415 (2.903 sec/step)\n",
            "I0619 20:36:11.194218 139649090267008 learning.py:507] global step 739: loss = 0.2512 (2.928 sec/step)\n",
            "I0619 20:36:14.142760 139649090267008 learning.py:507] global step 739: loss = 0.3390 (2.947 sec/step)\n",
            "I0619 20:36:17.132824 139649090267008 learning.py:507] global step 739: loss = 0.3240 (2.988 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:36:20.108044 139649090267008 learning.py:507] global step 739: loss = 0.2740 (2.973 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:36:24.999268 139649090267008 learning.py:507] global step 739: loss = 0.2703 (4.879 sec/step)\n",
            "I0619 20:36:26.417780 139646017689344 supervisor.py:1050] Recording summary at step 739.\n",
            "I0619 20:36:28.309756 139649090267008 learning.py:507] global step 739: loss = 0.2800 (3.309 sec/step)\n",
            "I0619 20:36:31.194597 139649090267008 learning.py:507] global step 739: loss = 0.3093 (2.883 sec/step)\n",
            "I0619 20:36:34.148120 139649090267008 learning.py:507] global step 739: loss = 0.2741 (2.952 sec/step)\n",
            "I0619 20:36:37.105335 139649090267008 learning.py:507] global step 740: loss = 0.2585 (2.955 sec/step)\n",
            "I0619 20:36:40.088313 139649090267008 learning.py:507] global step 740: loss = 0.3121 (2.981 sec/step)\n",
            "I0619 20:36:43.062990 139649090267008 learning.py:507] global step 740: loss = 0.3105 (2.973 sec/step)\n",
            "I0619 20:36:46.005576 139649090267008 learning.py:507] global step 740: loss = 0.2715 (2.941 sec/step)\n",
            "I0619 20:36:48.978308 139649090267008 learning.py:507] global step 740: loss = 0.2988 (2.971 sec/step)\n",
            "I0619 20:36:51.988632 139649090267008 learning.py:507] global step 740: loss = 0.2377 (3.009 sec/step)\n",
            "I0619 20:36:54.894217 139649090267008 learning.py:507] global step 740: loss = 0.3186 (2.904 sec/step)\n",
            "I0619 20:36:57.858117 139649090267008 learning.py:507] global step 740: loss = 0.2819 (2.962 sec/step)\n",
            "I0619 20:37:00.984688 139649090267008 learning.py:507] global step 741: loss = 0.3351 (3.124 sec/step)\n",
            "I0619 20:37:04.172757 139649090267008 learning.py:507] global step 741: loss = 0.3039 (3.186 sec/step)\n",
            "I0619 20:37:07.187778 139649090267008 learning.py:507] global step 741: loss = 0.2326 (3.013 sec/step)\n",
            "I0619 20:37:10.173916 139649090267008 learning.py:507] global step 741: loss = 0.3318 (2.984 sec/step)\n",
            "I0619 20:37:13.117841 139649090267008 learning.py:507] global step 741: loss = 0.2740 (2.942 sec/step)\n",
            "I0619 20:37:16.067794 139649090267008 learning.py:507] global step 741: loss = 0.2938 (2.948 sec/step)\n",
            "I0619 20:37:19.188229 139649090267008 learning.py:507] global step 741: loss = 0.2376 (3.119 sec/step)\n",
            "I0619 20:37:22.414749 139649090267008 learning.py:507] global step 741: loss = 0.3031 (3.225 sec/step)\n",
            "I0619 20:37:25.361922 139649090267008 learning.py:507] global step 742: loss = 0.3279 (2.945 sec/step)\n",
            "I0619 20:37:28.303887 139649090267008 learning.py:507] global step 742: loss = 0.3045 (2.940 sec/step)\n",
            "I0619 20:37:31.230598 139649090267008 learning.py:507] global step 742: loss = 0.2380 (2.925 sec/step)\n",
            "I0619 20:37:34.151736 139649090267008 learning.py:507] global step 742: loss = 0.2929 (2.919 sec/step)\n",
            "I0619 20:37:37.093901 139649090267008 learning.py:507] global step 742: loss = 0.2561 (2.940 sec/step)\n",
            "I0619 20:37:40.043689 139649090267008 learning.py:507] global step 742: loss = 0.3107 (2.948 sec/step)\n",
            "I0619 20:37:42.919976 139649090267008 learning.py:507] global step 742: loss = 0.3584 (2.875 sec/step)\n",
            "I0619 20:37:45.815927 139649090267008 learning.py:507] global step 742: loss = 0.3249 (2.894 sec/step)\n",
            "I0619 20:37:48.754207 139649090267008 learning.py:507] global step 743: loss = 0.2847 (2.936 sec/step)\n",
            "I0619 20:37:51.737228 139649090267008 learning.py:507] global step 743: loss = 0.2929 (2.981 sec/step)\n",
            "I0619 20:37:54.679069 139649090267008 learning.py:507] global step 743: loss = 0.2420 (2.940 sec/step)\n",
            "I0619 20:37:57.684805 139649090267008 learning.py:507] global step 743: loss = 0.3374 (3.004 sec/step)\n",
            "I0619 20:38:00.709473 139649090267008 learning.py:507] global step 743: loss = 0.2890 (3.023 sec/step)\n",
            "I0619 20:38:03.665844 139649090267008 learning.py:507] global step 743: loss = 0.3192 (2.954 sec/step)\n",
            "I0619 20:38:06.557175 139649090267008 learning.py:507] global step 743: loss = 0.2698 (2.889 sec/step)\n",
            "I0619 20:38:09.512840 139649090267008 learning.py:507] global step 743: loss = 0.2463 (2.954 sec/step)\n",
            "I0619 20:38:12.592796 139649090267008 learning.py:507] global step 744: loss = 0.3016 (3.078 sec/step)\n",
            "I0619 20:38:15.626302 139649090267008 learning.py:507] global step 744: loss = 0.2934 (3.032 sec/step)\n",
            "I0619 20:38:18.663546 139649090267008 learning.py:507] global step 744: loss = 0.2884 (3.035 sec/step)\n",
            "I0619 20:38:22.670346 139649090267008 learning.py:507] global step 744: loss = 0.2702 (3.994 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:38:26.143855 139646017689344 supervisor.py:1050] Recording summary at step 744.\n",
            "I0619 20:38:27.044915 139649090267008 learning.py:507] global step 744: loss = 0.3321 (4.373 sec/step)\n",
            "I0619 20:38:30.193331 139649090267008 learning.py:507] global step 744: loss = 0.2573 (3.146 sec/step)\n",
            "I0619 20:38:33.205981 139649090267008 learning.py:507] global step 744: loss = 0.2235 (3.011 sec/step)\n",
            "I0619 20:38:36.106473 139649090267008 learning.py:507] global step 744: loss = 0.3283 (2.899 sec/step)\n",
            "I0619 20:38:39.011691 139649090267008 learning.py:507] global step 745: loss = 0.3295 (2.903 sec/step)\n",
            "I0619 20:38:41.941957 139649090267008 learning.py:507] global step 745: loss = 0.2740 (2.929 sec/step)\n",
            "I0619 20:38:44.896081 139649090267008 learning.py:507] global step 745: loss = 0.3140 (2.952 sec/step)\n",
            "I0619 20:38:47.820574 139649090267008 learning.py:507] global step 745: loss = 0.3659 (2.923 sec/step)\n",
            "I0619 20:38:50.804140 139649090267008 learning.py:507] global step 745: loss = 0.2448 (2.982 sec/step)\n",
            "I0619 20:38:53.777316 139649090267008 learning.py:507] global step 745: loss = 0.2286 (2.971 sec/step)\n",
            "I0619 20:38:56.697934 139649090267008 learning.py:507] global step 745: loss = 0.2460 (2.919 sec/step)\n",
            "I0619 20:38:59.682743 139649090267008 learning.py:507] global step 745: loss = 0.3659 (2.983 sec/step)\n",
            "I0619 20:39:02.689236 139649090267008 learning.py:507] global step 746: loss = 0.2779 (3.004 sec/step)\n",
            "I0619 20:39:05.772659 139649090267008 learning.py:507] global step 746: loss = 0.2965 (3.081 sec/step)\n",
            "I0619 20:39:08.691273 139649090267008 learning.py:507] global step 746: loss = 0.2850 (2.917 sec/step)\n",
            "I0619 20:39:11.774328 139649090267008 learning.py:507] global step 746: loss = 0.2463 (3.081 sec/step)\n",
            "I0619 20:39:14.759698 139649090267008 learning.py:507] global step 746: loss = 0.3082 (2.984 sec/step)\n",
            "I0619 20:39:17.768865 139649090267008 learning.py:507] global step 746: loss = 0.2582 (3.008 sec/step)\n",
            "I0619 20:39:20.684391 139649090267008 learning.py:507] global step 746: loss = 0.2453 (2.914 sec/step)\n",
            "I0619 20:39:23.844181 139649090267008 learning.py:507] global step 746: loss = 0.2642 (3.158 sec/step)\n",
            "I0619 20:39:27.232381 139649090267008 learning.py:507] global step 747: loss = 0.2855 (3.387 sec/step)\n",
            "I0619 20:39:30.200059 139649090267008 learning.py:507] global step 747: loss = 0.2656 (2.966 sec/step)\n",
            "I0619 20:39:33.131314 139649090267008 learning.py:507] global step 747: loss = 0.2698 (2.929 sec/step)\n",
            "I0619 20:39:36.085111 139649090267008 learning.py:507] global step 747: loss = 0.2489 (2.952 sec/step)\n",
            "I0619 20:39:39.000865 139649090267008 learning.py:507] global step 747: loss = 0.2946 (2.914 sec/step)\n",
            "I0619 20:39:41.942384 139649090267008 learning.py:507] global step 747: loss = 0.2815 (2.940 sec/step)\n",
            "I0619 20:39:45.446732 139649090267008 learning.py:507] global step 747: loss = 0.2727 (3.503 sec/step)\n",
            "I0619 20:39:48.411318 139649090267008 learning.py:507] global step 747: loss = 0.3136 (2.963 sec/step)\n",
            "I0619 20:39:51.338822 139649090267008 learning.py:507] global step 748: loss = 0.2961 (2.925 sec/step)\n",
            "I0619 20:39:54.365838 139649090267008 learning.py:507] global step 748: loss = 0.2401 (3.025 sec/step)\n",
            "I0619 20:39:57.285796 139649090267008 learning.py:507] global step 748: loss = 0.3330 (2.918 sec/step)\n",
            "I0619 20:40:00.203257 139649090267008 learning.py:507] global step 748: loss = 0.2773 (2.916 sec/step)\n",
            "I0619 20:40:03.096297 139649090267008 learning.py:507] global step 748: loss = 0.2577 (2.891 sec/step)\n",
            "I0619 20:40:06.118461 139649090267008 learning.py:507] global step 748: loss = 0.3286 (3.020 sec/step)\n",
            "I0619 20:40:09.072916 139649090267008 learning.py:507] global step 748: loss = 0.2952 (2.953 sec/step)\n",
            "I0619 20:40:12.002578 139649090267008 learning.py:507] global step 748: loss = 0.2456 (2.928 sec/step)\n",
            "I0619 20:40:14.956803 139649090267008 learning.py:507] global step 749: loss = 0.2426 (2.952 sec/step)\n",
            "I0619 20:40:17.906000 139649090267008 learning.py:507] global step 749: loss = 0.2977 (2.948 sec/step)\n",
            "I0619 20:40:20.852257 139649090267008 learning.py:507] global step 749: loss = 0.2578 (2.945 sec/step)\n",
            "I0619 20:40:20.931591 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:40:27.495013 139646017689344 supervisor.py:1050] Recording summary at step 749.\n",
            "I0619 20:40:27.840351 139649090267008 learning.py:507] global step 749: loss = 0.3551 (6.986 sec/step)\n",
            "I0619 20:40:30.739030 139649090267008 learning.py:507] global step 749: loss = 0.2979 (2.897 sec/step)\n",
            "I0619 20:40:33.837511 139649090267008 learning.py:507] global step 749: loss = 0.3397 (3.097 sec/step)\n",
            "I0619 20:40:36.763343 139649090267008 learning.py:507] global step 749: loss = 0.2616 (2.924 sec/step)\n",
            "I0619 20:40:39.755644 139649090267008 learning.py:507] global step 749: loss = 0.2594 (2.991 sec/step)\n",
            "I0619 20:40:42.665502 139649090267008 learning.py:507] global step 750: loss = 0.3057 (2.907 sec/step)\n",
            "I0619 20:40:45.906375 139649090267008 learning.py:507] global step 750: loss = 0.3836 (3.239 sec/step)\n",
            "I0619 20:40:48.840462 139649090267008 learning.py:507] global step 750: loss = 0.2770 (2.932 sec/step)\n",
            "I0619 20:40:51.889327 139649090267008 learning.py:507] global step 750: loss = 0.2803 (3.047 sec/step)\n",
            "I0619 20:40:54.882896 139649090267008 learning.py:507] global step 750: loss = 0.3029 (2.992 sec/step)\n",
            "I0619 20:40:57.826331 139649090267008 learning.py:507] global step 750: loss = 0.3096 (2.942 sec/step)\n",
            "I0619 20:41:00.760307 139649090267008 learning.py:507] global step 750: loss = 0.3075 (2.932 sec/step)\n",
            "I0619 20:41:03.742275 139649090267008 learning.py:507] global step 750: loss = 0.2930 (2.980 sec/step)\n",
            "I0619 20:41:06.695368 139649090267008 learning.py:507] global step 751: loss = 0.3332 (2.951 sec/step)\n",
            "I0619 20:41:09.665858 139649090267008 learning.py:507] global step 751: loss = 0.2657 (2.969 sec/step)\n",
            "I0619 20:41:12.749190 139649090267008 learning.py:507] global step 751: loss = 0.2369 (3.082 sec/step)\n",
            "I0619 20:41:15.724593 139649090267008 learning.py:507] global step 751: loss = 0.3270 (2.974 sec/step)\n",
            "I0619 20:41:18.639198 139649090267008 learning.py:507] global step 751: loss = 0.2717 (2.913 sec/step)\n",
            "I0619 20:41:21.608552 139649090267008 learning.py:507] global step 751: loss = 0.3064 (2.968 sec/step)\n",
            "I0619 20:41:24.519450 139649090267008 learning.py:507] global step 751: loss = 0.3029 (2.909 sec/step)\n",
            "I0619 20:41:27.482596 139649090267008 learning.py:507] global step 751: loss = 0.2658 (2.961 sec/step)\n",
            "I0619 20:41:30.460645 139649090267008 learning.py:507] global step 752: loss = 0.2613 (2.976 sec/step)\n",
            "I0619 20:41:33.378107 139649090267008 learning.py:507] global step 752: loss = 0.2989 (2.916 sec/step)\n",
            "I0619 20:41:36.268557 139649090267008 learning.py:507] global step 752: loss = 0.2636 (2.889 sec/step)\n",
            "I0619 20:41:39.188706 139649090267008 learning.py:507] global step 752: loss = 0.2410 (2.919 sec/step)\n",
            "I0619 20:41:42.141271 139649090267008 learning.py:507] global step 752: loss = 0.3367 (2.951 sec/step)\n",
            "I0619 20:41:45.059147 139649090267008 learning.py:507] global step 752: loss = 0.3002 (2.916 sec/step)\n",
            "I0619 20:41:48.145539 139649090267008 learning.py:507] global step 752: loss = 0.3483 (3.085 sec/step)\n",
            "I0619 20:41:51.033272 139649090267008 learning.py:507] global step 752: loss = 0.2887 (2.886 sec/step)\n",
            "I0619 20:41:53.961470 139649090267008 learning.py:507] global step 753: loss = 0.3639 (2.926 sec/step)\n",
            "I0619 20:41:56.880202 139649090267008 learning.py:507] global step 753: loss = 0.2931 (2.917 sec/step)\n",
            "I0619 20:41:59.889410 139649090267008 learning.py:507] global step 753: loss = 0.3198 (3.007 sec/step)\n",
            "I0619 20:42:02.989629 139649090267008 learning.py:507] global step 753: loss = 0.2985 (3.099 sec/step)\n",
            "I0619 20:42:06.007619 139649090267008 learning.py:507] global step 753: loss = 0.3921 (3.016 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:42:09.032375 139649090267008 learning.py:507] global step 753: loss = 0.3236 (3.023 sec/step)\n",
            "I0619 20:42:12.051993 139649090267008 learning.py:507] global step 753: loss = 0.3431 (3.018 sec/step)\n",
            "I0619 20:42:15.019879 139649090267008 learning.py:507] global step 753: loss = 0.2460 (2.966 sec/step)\n",
            "I0619 20:42:17.897317 139649090267008 learning.py:507] global step 754: loss = 0.3632 (2.875 sec/step)\n",
            "I0619 20:42:21.081564 139649090267008 learning.py:507] global step 754: loss = 0.3177 (3.182 sec/step)\n",
            "I0619 20:42:26.150417 139649090267008 learning.py:507] global step 754: loss = 0.3037 (5.063 sec/step)\n",
            "I0619 20:42:26.925427 139646017689344 supervisor.py:1050] Recording summary at step 754.\n",
            "I0619 20:42:29.296348 139649090267008 learning.py:507] global step 754: loss = 0.2558 (3.143 sec/step)\n",
            "I0619 20:42:32.292925 139649090267008 learning.py:507] global step 754: loss = 0.2288 (2.995 sec/step)\n",
            "I0619 20:42:35.212104 139649090267008 learning.py:507] global step 754: loss = 0.2607 (2.917 sec/step)\n",
            "I0619 20:42:38.256730 139649090267008 learning.py:507] global step 754: loss = 0.3875 (3.043 sec/step)\n",
            "I0619 20:42:41.222309 139649090267008 learning.py:507] global step 754: loss = 0.2413 (2.964 sec/step)\n",
            "I0619 20:42:44.186197 139649090267008 learning.py:507] global step 755: loss = 0.3049 (2.962 sec/step)\n",
            "I0619 20:42:47.085925 139649090267008 learning.py:507] global step 755: loss = 0.2667 (2.898 sec/step)\n",
            "I0619 20:42:50.085866 139649090267008 learning.py:507] global step 755: loss = 0.2772 (2.998 sec/step)\n",
            "I0619 20:42:53.226151 139649090267008 learning.py:507] global step 755: loss = 0.2243 (3.139 sec/step)\n",
            "I0619 20:42:56.186340 139649090267008 learning.py:507] global step 755: loss = 0.3009 (2.958 sec/step)\n",
            "I0619 20:42:59.160017 139649090267008 learning.py:507] global step 755: loss = 0.3279 (2.972 sec/step)\n",
            "I0619 20:43:02.181283 139649090267008 learning.py:507] global step 755: loss = 0.2735 (3.019 sec/step)\n",
            "I0619 20:43:05.169857 139649090267008 learning.py:507] global step 755: loss = 0.3139 (2.987 sec/step)\n",
            "I0619 20:43:08.103340 139649090267008 learning.py:507] global step 756: loss = 0.3683 (2.931 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:43:11.354067 139649090267008 learning.py:507] global step 756: loss = 0.2573 (3.249 sec/step)\n",
            "I0619 20:43:14.359259 139649090267008 learning.py:507] global step 756: loss = 0.2424 (3.003 sec/step)\n",
            "I0619 20:43:17.404089 139649090267008 learning.py:507] global step 756: loss = 0.2749 (3.043 sec/step)\n",
            "I0619 20:43:20.345270 139649090267008 learning.py:507] global step 756: loss = 0.3662 (2.939 sec/step)\n",
            "I0619 20:43:23.254765 139649090267008 learning.py:507] global step 756: loss = 0.3539 (2.908 sec/step)\n",
            "I0619 20:43:26.289696 139649090267008 learning.py:507] global step 756: loss = 0.3076 (3.033 sec/step)\n",
            "I0619 20:43:29.334858 139649090267008 learning.py:507] global step 756: loss = 0.2617 (3.044 sec/step)\n",
            "I0619 20:43:32.366098 139649090267008 learning.py:507] global step 757: loss = 0.3686 (3.029 sec/step)\n",
            "I0619 20:43:35.495140 139649090267008 learning.py:507] global step 757: loss = 0.3671 (3.127 sec/step)\n",
            "I0619 20:43:38.442456 139649090267008 learning.py:507] global step 757: loss = 0.3018 (2.946 sec/step)\n",
            "I0619 20:43:41.356652 139649090267008 learning.py:507] global step 757: loss = 0.2718 (2.913 sec/step)\n",
            "I0619 20:43:44.296200 139649090267008 learning.py:507] global step 757: loss = 0.2731 (2.938 sec/step)\n",
            "I0619 20:43:47.301854 139649090267008 learning.py:507] global step 757: loss = 0.2768 (3.004 sec/step)\n",
            "I0619 20:43:50.283522 139649090267008 learning.py:507] global step 757: loss = 0.3304 (2.980 sec/step)\n",
            "I0619 20:43:53.239470 139649090267008 learning.py:507] global step 757: loss = 0.3493 (2.952 sec/step)\n",
            "I0619 20:43:56.205133 139649090267008 learning.py:507] global step 758: loss = 0.3429 (2.963 sec/step)\n",
            "I0619 20:43:59.147112 139649090267008 learning.py:507] global step 758: loss = 0.3347 (2.940 sec/step)\n",
            "I0619 20:44:02.135242 139649090267008 learning.py:507] global step 758: loss = 0.2640 (2.986 sec/step)\n",
            "I0619 20:44:05.128303 139649090267008 learning.py:507] global step 758: loss = 0.2646 (2.991 sec/step)\n",
            "I0619 20:44:08.264319 139649090267008 learning.py:507] global step 758: loss = 0.3815 (3.134 sec/step)\n",
            "I0619 20:44:11.237984 139649090267008 learning.py:507] global step 758: loss = 0.3212 (2.972 sec/step)\n",
            "I0619 20:44:14.281245 139649090267008 learning.py:507] global step 758: loss = 0.2490 (3.042 sec/step)\n",
            "I0619 20:44:17.238645 139649090267008 learning.py:507] global step 758: loss = 0.3070 (2.956 sec/step)\n",
            "I0619 20:44:20.196258 139649090267008 learning.py:507] global step 759: loss = 0.2858 (2.955 sec/step)\n",
            "I0619 20:44:25.505219 139649090267008 learning.py:507] global step 759: loss = 0.2568 (5.305 sec/step)\n",
            "I0619 20:44:26.239600 139646017689344 supervisor.py:1050] Recording summary at step 759.\n",
            "I0619 20:44:28.503995 139649090267008 learning.py:507] global step 759: loss = 0.2703 (2.992 sec/step)\n",
            "I0619 20:44:31.563172 139649090267008 learning.py:507] global step 759: loss = 0.3033 (3.057 sec/step)\n",
            "I0619 20:44:34.528734 139649090267008 learning.py:507] global step 759: loss = 0.2925 (2.963 sec/step)\n",
            "I0619 20:44:37.464673 139649090267008 learning.py:507] global step 759: loss = 0.2575 (2.934 sec/step)\n",
            "I0619 20:44:40.374423 139649090267008 learning.py:507] global step 759: loss = 0.2620 (2.908 sec/step)\n",
            "I0619 20:44:43.287954 139649090267008 learning.py:507] global step 759: loss = 0.2519 (2.912 sec/step)\n",
            "I0619 20:44:46.242722 139649090267008 learning.py:507] global step 760: loss = 0.3113 (2.953 sec/step)\n",
            "I0619 20:44:49.131826 139649090267008 learning.py:507] global step 760: loss = 0.2205 (2.887 sec/step)\n",
            "I0619 20:44:52.138276 139649090267008 learning.py:507] global step 760: loss = 0.2439 (3.005 sec/step)\n",
            "I0619 20:44:55.089706 139649090267008 learning.py:507] global step 760: loss = 0.2890 (2.950 sec/step)\n",
            "I0619 20:44:58.026983 139649090267008 learning.py:507] global step 760: loss = 0.2855 (2.936 sec/step)\n",
            "I0619 20:45:01.026896 139649090267008 learning.py:507] global step 760: loss = 0.2834 (2.998 sec/step)\n",
            "I0619 20:45:04.029730 139649090267008 learning.py:507] global step 760: loss = 0.2553 (3.001 sec/step)\n",
            "I0619 20:45:07.074292 139649090267008 learning.py:507] global step 760: loss = 0.2287 (3.043 sec/step)\n",
            "I0619 20:45:10.087823 139649090267008 learning.py:507] global step 761: loss = 0.3224 (3.010 sec/step)\n",
            "I0619 20:45:13.040042 139649090267008 learning.py:507] global step 761: loss = 0.2459 (2.951 sec/step)\n",
            "I0619 20:45:15.978543 139649090267008 learning.py:507] global step 761: loss = 0.2853 (2.937 sec/step)\n",
            "I0619 20:45:18.887475 139649090267008 learning.py:507] global step 761: loss = 0.2441 (2.907 sec/step)\n",
            "I0619 20:45:22.054951 139649090267008 learning.py:507] global step 761: loss = 0.3831 (3.166 sec/step)\n",
            "I0619 20:45:25.011379 139649090267008 learning.py:507] global step 761: loss = 0.2875 (2.955 sec/step)\n",
            "I0619 20:45:27.992363 139649090267008 learning.py:507] global step 761: loss = 0.2939 (2.979 sec/step)\n",
            "I0619 20:45:31.019090 139649090267008 learning.py:507] global step 761: loss = 0.2343 (3.025 sec/step)\n",
            "I0619 20:45:33.970051 139649090267008 learning.py:507] global step 762: loss = 0.3076 (2.949 sec/step)\n",
            "I0619 20:45:37.041689 139649090267008 learning.py:507] global step 762: loss = 0.3163 (3.070 sec/step)\n",
            "I0619 20:45:40.281692 139649090267008 learning.py:507] global step 762: loss = 0.3774 (3.238 sec/step)\n",
            "I0619 20:45:43.292448 139649090267008 learning.py:507] global step 762: loss = 0.3151 (3.009 sec/step)\n",
            "I0619 20:45:46.238618 139649090267008 learning.py:507] global step 762: loss = 0.2530 (2.944 sec/step)\n",
            "I0619 20:45:49.180304 139649090267008 learning.py:507] global step 762: loss = 0.4197 (2.940 sec/step)\n",
            "I0619 20:45:52.168372 139649090267008 learning.py:507] global step 762: loss = 0.3228 (2.986 sec/step)\n",
            "I0619 20:45:55.120762 139649090267008 learning.py:507] global step 762: loss = 0.2201 (2.951 sec/step)\n",
            "I0619 20:45:58.381570 139649090267008 learning.py:507] global step 763: loss = 0.2588 (3.259 sec/step)\n",
            "I0619 20:46:01.451201 139649090267008 learning.py:507] global step 763: loss = 0.3333 (3.068 sec/step)\n",
            "I0619 20:46:04.448192 139649090267008 learning.py:507] global step 763: loss = 0.2907 (2.995 sec/step)\n",
            "I0619 20:46:07.470058 139649090267008 learning.py:507] global step 763: loss = 0.2912 (3.020 sec/step)\n",
            "I0619 20:46:10.571288 139649090267008 learning.py:507] global step 763: loss = 0.2488 (3.099 sec/step)\n",
            "I0619 20:46:13.548636 139649090267008 learning.py:507] global step 763: loss = 0.3122 (2.976 sec/step)\n",
            "I0619 20:46:16.965236 139649090267008 learning.py:507] global step 763: loss = 0.3116 (3.415 sec/step)\n",
            "I0619 20:46:20.032735 139649090267008 learning.py:507] global step 763: loss = 0.2692 (3.066 sec/step)\n",
            "I0619 20:46:25.191895 139649090267008 learning.py:507] global step 764: loss = 0.2719 (5.146 sec/step)\n",
            "I0619 20:46:26.101521 139646017689344 supervisor.py:1050] Recording summary at step 764.\n",
            "I0619 20:46:28.439127 139649090267008 learning.py:507] global step 764: loss = 0.2780 (3.243 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:46:31.491019 139649090267008 learning.py:507] global step 764: loss = 0.2327 (3.050 sec/step)\n",
            "I0619 20:46:34.463543 139649090267008 learning.py:507] global step 764: loss = 0.2622 (2.971 sec/step)\n",
            "I0619 20:46:37.453366 139649090267008 learning.py:507] global step 764: loss = 0.2979 (2.988 sec/step)\n",
            "I0619 20:46:40.504208 139649090267008 learning.py:507] global step 764: loss = 0.2301 (3.049 sec/step)\n",
            "I0619 20:46:43.490242 139649090267008 learning.py:507] global step 764: loss = 0.2831 (2.984 sec/step)\n",
            "I0619 20:46:46.569805 139649090267008 learning.py:507] global step 764: loss = 0.3158 (3.078 sec/step)\n",
            "I0619 20:46:49.625719 139649090267008 learning.py:507] global step 765: loss = 0.3770 (3.054 sec/step)\n",
            "I0619 20:46:52.768953 139649090267008 learning.py:507] global step 765: loss = 0.2634 (3.141 sec/step)\n",
            "I0619 20:46:55.814982 139649090267008 learning.py:507] global step 765: loss = 0.3423 (3.044 sec/step)\n",
            "I0619 20:46:58.826896 139649090267008 learning.py:507] global step 765: loss = 0.2732 (3.010 sec/step)\n",
            "I0619 20:47:01.866631 139649090267008 learning.py:507] global step 765: loss = 0.2732 (3.038 sec/step)\n",
            "I0619 20:47:04.940543 139649090267008 learning.py:507] global step 765: loss = 0.2823 (3.072 sec/step)\n",
            "I0619 20:47:08.053407 139649090267008 learning.py:507] global step 765: loss = 0.2348 (3.111 sec/step)\n",
            "I0619 20:47:11.192270 139649090267008 learning.py:507] global step 765: loss = 0.2610 (3.137 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:47:14.282097 139649090267008 learning.py:507] global step 766: loss = 0.2861 (3.088 sec/step)\n",
            "I0619 20:47:17.338125 139649090267008 learning.py:507] global step 766: loss = 0.2864 (3.054 sec/step)\n",
            "I0619 20:47:20.590955 139649090267008 learning.py:507] global step 766: loss = 0.4344 (3.251 sec/step)\n",
            "I0619 20:47:23.643660 139649090267008 learning.py:507] global step 766: loss = 0.2371 (3.051 sec/step)\n",
            "I0619 20:47:26.750759 139649090267008 learning.py:507] global step 766: loss = 0.2696 (3.105 sec/step)\n",
            "I0619 20:47:29.756719 139649090267008 learning.py:507] global step 766: loss = 0.2547 (3.004 sec/step)\n",
            "I0619 20:47:32.846868 139649090267008 learning.py:507] global step 766: loss = 0.2556 (3.088 sec/step)\n",
            "I0619 20:47:35.916061 139649090267008 learning.py:507] global step 766: loss = 0.2952 (3.067 sec/step)\n",
            "I0619 20:47:39.302461 139649090267008 learning.py:507] global step 767: loss = 0.2763 (3.384 sec/step)\n",
            "I0619 20:47:42.383665 139649090267008 learning.py:507] global step 767: loss = 0.2541 (3.079 sec/step)\n",
            "I0619 20:47:45.498523 139649090267008 learning.py:507] global step 767: loss = 0.2955 (3.113 sec/step)\n",
            "I0619 20:47:48.571746 139649090267008 learning.py:507] global step 767: loss = 0.2384 (3.071 sec/step)\n",
            "I0619 20:47:51.612378 139649090267008 learning.py:507] global step 767: loss = 0.3584 (3.039 sec/step)\n",
            "I0619 20:47:54.653639 139649090267008 learning.py:507] global step 767: loss = 0.3412 (3.039 sec/step)\n",
            "I0619 20:47:57.692408 139649090267008 learning.py:507] global step 767: loss = 0.2833 (3.037 sec/step)\n",
            "I0619 20:48:00.736795 139649090267008 learning.py:507] global step 767: loss = 0.2961 (3.043 sec/step)\n",
            "I0619 20:48:03.826520 139649090267008 learning.py:507] global step 768: loss = 0.3227 (3.087 sec/step)\n",
            "I0619 20:48:06.906883 139649090267008 learning.py:507] global step 768: loss = 0.2731 (3.078 sec/step)\n",
            "I0619 20:48:09.973810 139649090267008 learning.py:507] global step 768: loss = 0.2674 (3.065 sec/step)\n",
            "I0619 20:48:12.973242 139649090267008 learning.py:507] global step 768: loss = 0.3136 (2.998 sec/step)\n",
            "I0619 20:48:15.984986 139649090267008 learning.py:507] global step 768: loss = 0.2707 (3.010 sec/step)\n",
            "I0619 20:48:19.027676 139649090267008 learning.py:507] global step 768: loss = 0.3391 (3.041 sec/step)\n",
            "I0619 20:48:23.690973 139649090267008 learning.py:507] global step 768: loss = 0.2703 (4.656 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:48:26.368625 139646017689344 supervisor.py:1050] Recording summary at step 768.\n",
            "I0619 20:48:27.565165 139649090267008 learning.py:507] global step 768: loss = 0.2469 (3.864 sec/step)\n",
            "I0619 20:48:30.646447 139649090267008 learning.py:507] global step 769: loss = 0.2285 (3.079 sec/step)\n",
            "I0619 20:48:33.723120 139649090267008 learning.py:507] global step 769: loss = 0.4848 (3.075 sec/step)\n",
            "I0619 20:48:36.854669 139649090267008 learning.py:507] global step 769: loss = 0.2262 (3.130 sec/step)\n",
            "I0619 20:48:39.927705 139649090267008 learning.py:507] global step 769: loss = 0.3321 (3.071 sec/step)\n",
            "I0619 20:48:43.019738 139649090267008 learning.py:507] global step 769: loss = 0.2794 (3.090 sec/step)\n",
            "I0619 20:48:46.106931 139649090267008 learning.py:507] global step 769: loss = 0.4636 (3.085 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:48:49.480640 139649090267008 learning.py:507] global step 769: loss = 0.3088 (3.372 sec/step)\n",
            "I0619 20:48:52.549750 139649090267008 learning.py:507] global step 769: loss = 0.2392 (3.067 sec/step)\n",
            "I0619 20:48:55.666376 139649090267008 learning.py:507] global step 770: loss = 0.2704 (3.114 sec/step)\n",
            "I0619 20:48:58.734754 139649090267008 learning.py:507] global step 770: loss = 0.2632 (3.067 sec/step)\n",
            "I0619 20:49:01.830856 139649090267008 learning.py:507] global step 770: loss = 0.3038 (3.094 sec/step)\n",
            "I0619 20:49:04.985185 139649090267008 learning.py:507] global step 770: loss = 0.2545 (3.152 sec/step)\n",
            "I0619 20:49:08.525624 139649090267008 learning.py:507] global step 770: loss = 0.2810 (3.539 sec/step)\n",
            "I0619 20:49:11.596033 139649090267008 learning.py:507] global step 770: loss = 0.2730 (3.068 sec/step)\n",
            "I0619 20:49:14.637104 139649090267008 learning.py:507] global step 770: loss = 0.3291 (3.039 sec/step)\n",
            "I0619 20:49:17.732524 139649090267008 learning.py:507] global step 770: loss = 0.3474 (3.093 sec/step)\n",
            "I0619 20:49:20.764863 139649090267008 learning.py:507] global step 771: loss = 0.3603 (3.029 sec/step)\n",
            "I0619 20:49:23.865404 139649090267008 learning.py:507] global step 771: loss = 0.3734 (3.099 sec/step)\n",
            "I0619 20:49:26.896284 139649090267008 learning.py:507] global step 771: loss = 0.2852 (3.029 sec/step)\n",
            "I0619 20:49:29.987340 139649090267008 learning.py:507] global step 771: loss = 0.2568 (3.089 sec/step)\n",
            "I0619 20:49:33.037622 139649090267008 learning.py:507] global step 771: loss = 0.2345 (3.049 sec/step)\n",
            "I0619 20:49:36.168176 139649090267008 learning.py:507] global step 771: loss = 0.3159 (3.129 sec/step)\n",
            "I0619 20:49:39.206915 139649090267008 learning.py:507] global step 771: loss = 0.2813 (3.037 sec/step)\n",
            "I0619 20:49:42.273328 139649090267008 learning.py:507] global step 771: loss = 0.2840 (3.065 sec/step)\n",
            "I0619 20:49:45.278471 139649090267008 learning.py:507] global step 772: loss = 0.2594 (3.003 sec/step)\n",
            "I0619 20:49:48.357957 139649090267008 learning.py:507] global step 772: loss = 0.2417 (3.077 sec/step)\n",
            "I0619 20:49:51.427327 139649090267008 learning.py:507] global step 772: loss = 0.3227 (3.068 sec/step)\n",
            "I0619 20:49:54.423669 139649090267008 learning.py:507] global step 772: loss = 0.2589 (2.995 sec/step)\n",
            "I0619 20:49:57.509782 139649090267008 learning.py:507] global step 772: loss = 0.2371 (3.084 sec/step)\n",
            "I0619 20:50:00.572504 139649090267008 learning.py:507] global step 772: loss = 0.2756 (3.061 sec/step)\n",
            "I0619 20:50:03.762050 139649090267008 learning.py:507] global step 772: loss = 0.3133 (3.188 sec/step)\n",
            "I0619 20:50:06.893669 139649090267008 learning.py:507] global step 772: loss = 0.2492 (3.130 sec/step)\n",
            "I0619 20:50:09.921866 139649090267008 learning.py:507] global step 773: loss = 0.2587 (3.026 sec/step)\n",
            "I0619 20:50:12.932171 139649090267008 learning.py:507] global step 773: loss = 0.3240 (3.009 sec/step)\n",
            "I0619 20:50:16.027632 139649090267008 learning.py:507] global step 773: loss = 0.2377 (3.094 sec/step)\n",
            "I0619 20:50:19.246935 139649090267008 learning.py:507] global step 773: loss = 0.2269 (3.218 sec/step)\n",
            "I0619 20:50:20.931631 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 20:50:25.571818 139649090267008 learning.py:507] global step 773: loss = 0.2582 (6.318 sec/step)\n",
            "I0619 20:50:27.633843 139646017689344 supervisor.py:1050] Recording summary at step 773.\n",
            "I0619 20:50:29.413298 139649090267008 learning.py:507] global step 773: loss = 0.2611 (3.831 sec/step)\n",
            "I0619 20:50:32.460530 139649090267008 learning.py:507] global step 773: loss = 0.2108 (3.045 sec/step)\n",
            "I0619 20:50:35.513265 139649090267008 learning.py:507] global step 773: loss = 0.3215 (3.051 sec/step)\n",
            "I0619 20:50:38.737345 139649090267008 learning.py:507] global step 774: loss = 0.2461 (3.222 sec/step)\n",
            "I0619 20:50:41.794132 139649090267008 learning.py:507] global step 774: loss = 0.3109 (3.055 sec/step)\n",
            "I0619 20:50:44.873160 139649090267008 learning.py:507] global step 774: loss = 0.2882 (3.077 sec/step)\n",
            "I0619 20:50:47.856229 139649090267008 learning.py:507] global step 774: loss = 0.3689 (2.981 sec/step)\n",
            "I0619 20:50:50.857665 139649090267008 learning.py:507] global step 774: loss = 0.2452 (3.000 sec/step)\n",
            "I0619 20:50:53.937289 139649090267008 learning.py:507] global step 774: loss = 0.2645 (3.078 sec/step)\n",
            "I0619 20:50:57.017291 139649090267008 learning.py:507] global step 774: loss = 0.2947 (3.078 sec/step)\n",
            "I0619 20:51:00.088625 139649090267008 learning.py:507] global step 774: loss = 0.3085 (3.070 sec/step)\n",
            "I0619 20:51:03.073211 139649090267008 learning.py:507] global step 775: loss = 0.2724 (2.979 sec/step)\n",
            "I0619 20:51:06.114262 139649090267008 learning.py:507] global step 775: loss = 0.2715 (3.039 sec/step)\n",
            "I0619 20:51:09.139643 139649090267008 learning.py:507] global step 775: loss = 0.2877 (3.024 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:51:12.164385 139649090267008 learning.py:507] global step 775: loss = 0.2638 (3.023 sec/step)\n",
            "I0619 20:51:15.144272 139649090267008 learning.py:507] global step 775: loss = 0.3120 (2.978 sec/step)\n",
            "I0619 20:51:18.162452 139649090267008 learning.py:507] global step 775: loss = 0.2690 (3.016 sec/step)\n",
            "I0619 20:51:21.205281 139649090267008 learning.py:507] global step 775: loss = 0.2836 (3.041 sec/step)\n",
            "I0619 20:51:24.247887 139649090267008 learning.py:507] global step 775: loss = 0.2579 (3.041 sec/step)\n",
            "I0619 20:51:27.283828 139649090267008 learning.py:507] global step 776: loss = 0.3172 (3.034 sec/step)\n",
            "I0619 20:51:30.352221 139649090267008 learning.py:507] global step 776: loss = 0.2584 (3.067 sec/step)\n",
            "I0619 20:51:33.369811 139649090267008 learning.py:507] global step 776: loss = 0.2209 (3.016 sec/step)\n",
            "I0619 20:51:36.421148 139649090267008 learning.py:507] global step 776: loss = 0.2653 (3.050 sec/step)\n",
            "I0619 20:51:39.406922 139649090267008 learning.py:507] global step 776: loss = 0.3528 (2.980 sec/step)\n",
            "I0619 20:51:42.464009 139649090267008 learning.py:507] global step 776: loss = 0.3243 (3.055 sec/step)\n",
            "I0619 20:51:45.509944 139649090267008 learning.py:507] global step 776: loss = 0.3123 (3.044 sec/step)\n",
            "I0619 20:51:48.629003 139649090267008 learning.py:507] global step 776: loss = 0.2470 (3.117 sec/step)\n",
            "I0619 20:51:51.718356 139649090267008 learning.py:507] global step 777: loss = 0.3156 (3.087 sec/step)\n",
            "I0619 20:51:54.711939 139649090267008 learning.py:507] global step 777: loss = 0.2798 (2.992 sec/step)\n",
            "I0619 20:51:57.713609 139649090267008 learning.py:507] global step 777: loss = 0.4034 (3.000 sec/step)\n",
            "I0619 20:52:00.782090 139649090267008 learning.py:507] global step 777: loss = 0.3614 (3.067 sec/step)\n",
            "I0619 20:52:03.786953 139649090267008 learning.py:507] global step 777: loss = 0.2793 (3.003 sec/step)\n",
            "I0619 20:52:06.812336 139649090267008 learning.py:507] global step 777: loss = 0.2549 (3.024 sec/step)\n",
            "I0619 20:52:09.848310 139649090267008 learning.py:507] global step 777: loss = 0.2469 (3.034 sec/step)\n",
            "I0619 20:52:12.915680 139649090267008 learning.py:507] global step 777: loss = 0.3270 (3.066 sec/step)\n",
            "I0619 20:52:15.918073 139649090267008 learning.py:507] global step 778: loss = 0.2968 (3.000 sec/step)\n",
            "I0619 20:52:18.939217 139649090267008 learning.py:507] global step 778: loss = 0.3337 (3.019 sec/step)\n",
            "I0619 20:52:23.350345 139649090267008 learning.py:507] global step 778: loss = 0.2806 (4.384 sec/step)\n",
            "I0619 20:52:26.284043 139646017689344 supervisor.py:1050] Recording summary at step 778.\n",
            "I0619 20:52:27.339624 139649090267008 learning.py:507] global step 778: loss = 0.4159 (3.987 sec/step)\n",
            "I0619 20:52:30.353442 139649090267008 learning.py:507] global step 778: loss = 0.3045 (3.012 sec/step)\n",
            "I0619 20:52:33.375311 139649090267008 learning.py:507] global step 778: loss = 0.2380 (3.020 sec/step)\n",
            "I0619 20:52:36.384137 139649090267008 learning.py:507] global step 778: loss = 0.2495 (3.007 sec/step)\n",
            "I0619 20:52:39.449316 139649090267008 learning.py:507] global step 778: loss = 0.2895 (3.063 sec/step)\n",
            "I0619 20:52:42.432378 139649090267008 learning.py:507] global step 779: loss = 0.2714 (2.981 sec/step)\n",
            "I0619 20:52:45.421386 139649090267008 learning.py:507] global step 779: loss = 0.3283 (2.987 sec/step)\n",
            "I0619 20:52:48.460721 139649090267008 learning.py:507] global step 779: loss = 0.2517 (3.038 sec/step)\n",
            "I0619 20:52:51.507283 139649090267008 learning.py:507] global step 779: loss = 0.2676 (3.045 sec/step)\n",
            "I0619 20:52:54.507856 139649090267008 learning.py:507] global step 779: loss = 0.3098 (2.999 sec/step)\n",
            "I0619 20:52:57.558511 139649090267008 learning.py:507] global step 779: loss = 0.2545 (3.049 sec/step)\n",
            "I0619 20:53:00.542814 139649090267008 learning.py:507] global step 779: loss = 0.3203 (2.982 sec/step)\n",
            "I0619 20:53:03.622718 139649090267008 learning.py:507] global step 779: loss = 0.2895 (3.078 sec/step)\n",
            "I0619 20:53:06.738670 139649090267008 learning.py:507] global step 780: loss = 0.2870 (3.113 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:53:09.944316 139649090267008 learning.py:507] global step 780: loss = 0.2678 (3.203 sec/step)\n",
            "I0619 20:53:13.043764 139649090267008 learning.py:507] global step 780: loss = 0.2983 (3.098 sec/step)\n",
            "I0619 20:53:16.083934 139649090267008 learning.py:507] global step 780: loss = 0.2784 (3.038 sec/step)\n",
            "I0619 20:53:19.092353 139649090267008 learning.py:507] global step 780: loss = 0.2633 (3.007 sec/step)\n",
            "I0619 20:53:22.108025 139649090267008 learning.py:507] global step 780: loss = 0.2613 (3.014 sec/step)\n",
            "I0619 20:53:25.157595 139649090267008 learning.py:507] global step 780: loss = 0.3953 (3.048 sec/step)\n",
            "I0619 20:53:28.152944 139649090267008 learning.py:507] global step 780: loss = 0.2487 (2.994 sec/step)\n",
            "I0619 20:53:31.167900 139649090267008 learning.py:507] global step 781: loss = 0.2879 (3.012 sec/step)\n",
            "I0619 20:53:34.216506 139649090267008 learning.py:507] global step 781: loss = 0.2679 (3.047 sec/step)\n",
            "I0619 20:53:37.203170 139649090267008 learning.py:507] global step 781: loss = 0.2576 (2.985 sec/step)\n",
            "I0619 20:53:40.271976 139649090267008 learning.py:507] global step 781: loss = 0.2559 (3.067 sec/step)\n",
            "I0619 20:53:43.328157 139649090267008 learning.py:507] global step 781: loss = 0.3476 (3.054 sec/step)\n",
            "I0619 20:53:46.357517 139649090267008 learning.py:507] global step 781: loss = 0.3217 (3.027 sec/step)\n",
            "I0619 20:53:49.494765 139649090267008 learning.py:507] global step 781: loss = 0.2879 (3.135 sec/step)\n",
            "I0619 20:53:52.589213 139649090267008 learning.py:507] global step 781: loss = 0.2567 (3.093 sec/step)\n",
            "I0619 20:53:55.817825 139649090267008 learning.py:507] global step 782: loss = 0.2877 (3.226 sec/step)\n",
            "I0619 20:53:58.872388 139649090267008 learning.py:507] global step 782: loss = 0.3053 (3.053 sec/step)\n",
            "I0619 20:54:01.869678 139649090267008 learning.py:507] global step 782: loss = 0.2519 (2.996 sec/step)\n",
            "I0619 20:54:04.947765 139649090267008 learning.py:507] global step 782: loss = 0.2870 (3.076 sec/step)\n",
            "I0619 20:54:07.932753 139649090267008 learning.py:507] global step 782: loss = 0.2964 (2.983 sec/step)\n",
            "I0619 20:54:11.222893 139649090267008 learning.py:507] global step 782: loss = 0.2833 (3.288 sec/step)\n",
            "I0619 20:54:14.297825 139649090267008 learning.py:507] global step 782: loss = 0.3168 (3.073 sec/step)\n",
            "I0619 20:54:17.401129 139649090267008 learning.py:507] global step 782: loss = 0.2604 (3.102 sec/step)\n",
            "I0619 20:54:20.412145 139649090267008 learning.py:507] global step 783: loss = 0.3487 (3.009 sec/step)\n",
            "I0619 20:54:25.743618 139649090267008 learning.py:507] global step 783: loss = 0.2430 (5.330 sec/step)\n",
            "I0619 20:54:26.824558 139646017689344 supervisor.py:1050] Recording summary at step 783.\n",
            "I0619 20:54:29.248594 139649090267008 learning.py:507] global step 783: loss = 0.3077 (3.501 sec/step)\n",
            "I0619 20:54:32.456153 139649090267008 learning.py:507] global step 783: loss = 0.2589 (3.206 sec/step)\n",
            "I0619 20:54:35.504573 139649090267008 learning.py:507] global step 783: loss = 0.2533 (3.047 sec/step)\n",
            "I0619 20:54:38.562180 139649090267008 learning.py:507] global step 783: loss = 0.4999 (3.056 sec/step)\n",
            "I0619 20:54:41.606516 139649090267008 learning.py:507] global step 783: loss = 0.2660 (3.043 sec/step)\n",
            "I0619 20:54:44.615000 139649090267008 learning.py:507] global step 783: loss = 0.2428 (3.007 sec/step)\n",
            "I0619 20:54:47.678328 139649090267008 learning.py:507] global step 784: loss = 0.2835 (3.061 sec/step)\n",
            "I0619 20:54:50.974452 139649090267008 learning.py:507] global step 784: loss = 0.3636 (3.294 sec/step)\n",
            "I0619 20:54:54.032869 139649090267008 learning.py:507] global step 784: loss = 0.2664 (3.057 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:54:57.050103 139649090267008 learning.py:507] global step 784: loss = 0.3152 (3.015 sec/step)\n",
            "I0619 20:55:00.069641 139649090267008 learning.py:507] global step 784: loss = 0.2523 (3.017 sec/step)\n",
            "I0619 20:55:03.114211 139649090267008 learning.py:507] global step 784: loss = 0.2309 (3.043 sec/step)\n",
            "I0619 20:55:06.125695 139649090267008 learning.py:507] global step 784: loss = 0.3343 (3.010 sec/step)\n",
            "I0619 20:55:09.188554 139649090267008 learning.py:507] global step 784: loss = 0.2268 (3.061 sec/step)\n",
            "I0619 20:55:12.259101 139649090267008 learning.py:507] global step 785: loss = 0.3047 (3.068 sec/step)\n",
            "I0619 20:55:15.271413 139649090267008 learning.py:507] global step 785: loss = 0.2780 (3.010 sec/step)\n",
            "I0619 20:55:18.331989 139649090267008 learning.py:507] global step 785: loss = 0.2853 (3.059 sec/step)\n",
            "I0619 20:55:21.375429 139649090267008 learning.py:507] global step 785: loss = 0.2352 (3.042 sec/step)\n",
            "I0619 20:55:24.448504 139649090267008 learning.py:507] global step 785: loss = 0.3680 (3.071 sec/step)\n",
            "I0619 20:55:27.437858 139649090267008 learning.py:507] global step 785: loss = 0.3001 (2.988 sec/step)\n",
            "I0619 20:55:30.642613 139649090267008 learning.py:507] global step 785: loss = 0.2747 (3.203 sec/step)\n",
            "I0619 20:55:33.639652 139649090267008 learning.py:507] global step 785: loss = 0.2886 (2.995 sec/step)\n",
            "I0619 20:55:36.741488 139649090267008 learning.py:507] global step 786: loss = 0.2657 (3.099 sec/step)\n",
            "I0619 20:55:39.792767 139649090267008 learning.py:507] global step 786: loss = 0.2446 (3.049 sec/step)\n",
            "I0619 20:55:42.812018 139649090267008 learning.py:507] global step 786: loss = 0.2828 (3.018 sec/step)\n",
            "I0619 20:55:45.837871 139649090267008 learning.py:507] global step 786: loss = 0.2862 (3.024 sec/step)\n",
            "I0619 20:55:48.916074 139649090267008 learning.py:507] global step 786: loss = 0.2252 (3.076 sec/step)\n",
            "I0619 20:55:52.006956 139649090267008 learning.py:507] global step 786: loss = 0.3213 (3.089 sec/step)\n",
            "I0619 20:55:55.079643 139649090267008 learning.py:507] global step 786: loss = 0.2408 (3.071 sec/step)\n",
            "I0619 20:55:58.099991 139649090267008 learning.py:507] global step 786: loss = 0.2972 (3.019 sec/step)\n",
            "I0619 20:56:01.065956 139649090267008 learning.py:507] global step 787: loss = 0.2910 (2.964 sec/step)\n",
            "I0619 20:56:04.075128 139649090267008 learning.py:507] global step 787: loss = 0.3597 (3.007 sec/step)\n",
            "I0619 20:56:07.143389 139649090267008 learning.py:507] global step 787: loss = 0.2722 (3.066 sec/step)\n",
            "I0619 20:56:10.353526 139649090267008 learning.py:507] global step 787: loss = 0.3291 (3.208 sec/step)\n",
            "I0619 20:56:13.384263 139649090267008 learning.py:507] global step 787: loss = 0.2900 (3.029 sec/step)\n",
            "I0619 20:56:16.410170 139649090267008 learning.py:507] global step 787: loss = 0.2621 (3.024 sec/step)\n",
            "I0619 20:56:19.437113 139649090267008 learning.py:507] global step 787: loss = 0.2569 (3.025 sec/step)\n",
            "I0619 20:56:24.635014 139649090267008 learning.py:507] global step 787: loss = 0.2553 (5.196 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 20:56:26.213364 139646017689344 supervisor.py:1050] Recording summary at step 787.\n",
            "I0619 20:56:28.027650 139649090267008 learning.py:507] global step 788: loss = 0.2421 (3.382 sec/step)\n",
            "I0619 20:56:31.140845 139649090267008 learning.py:507] global step 788: loss = 0.3922 (3.110 sec/step)\n",
            "I0619 20:56:34.149125 139649090267008 learning.py:507] global step 788: loss = 0.2551 (3.007 sec/step)\n",
            "I0619 20:56:37.164689 139649090267008 learning.py:507] global step 788: loss = 0.2797 (3.014 sec/step)\n",
            "I0619 20:56:40.121659 139649090267008 learning.py:507] global step 788: loss = 0.2399 (2.955 sec/step)\n",
            "I0619 20:56:43.557873 139649090267008 learning.py:507] global step 788: loss = 0.2609 (3.434 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 20:56:46.792651 139649090267008 learning.py:507] global step 788: loss = 0.2515 (3.233 sec/step)\n",
            "I0619 20:56:49.789836 139649090267008 learning.py:507] global step 788: loss = 0.2482 (2.995 sec/step)\n",
            "I0619 20:56:52.829419 139649090267008 learning.py:507] global step 789: loss = 0.2602 (3.037 sec/step)\n",
            "I0619 20:56:55.823423 139649090267008 learning.py:507] global step 789: loss = 0.2339 (2.992 sec/step)\n",
            "I0619 20:56:58.864187 139649090267008 learning.py:507] global step 789: loss = 0.3175 (3.039 sec/step)\n",
            "I0619 20:57:02.093772 139649090267008 learning.py:507] global step 789: loss = 0.3004 (3.228 sec/step)\n",
            "I0619 20:57:05.446898 139649090267008 learning.py:507] global step 789: loss = 0.2832 (3.351 sec/step)\n",
            "I0619 20:57:08.514718 139649090267008 learning.py:507] global step 789: loss = 0.2415 (3.066 sec/step)\n",
            "I0619 20:57:11.556023 139649090267008 learning.py:507] global step 789: loss = 0.2731 (3.040 sec/step)\n",
            "I0619 20:57:14.606450 139649090267008 learning.py:507] global step 789: loss = 0.2657 (3.049 sec/step)\n",
            "I0619 20:57:17.610001 139649090267008 learning.py:507] global step 790: loss = 0.2142 (3.001 sec/step)\n",
            "I0619 20:57:20.642738 139649090267008 learning.py:507] global step 790: loss = 0.2554 (3.031 sec/step)\n",
            "I0619 20:57:23.694986 139649090267008 learning.py:507] global step 790: loss = 0.2726 (3.051 sec/step)\n",
            "I0619 20:57:26.703083 139649090267008 learning.py:507] global step 790: loss = 0.2573 (3.006 sec/step)\n",
            "I0619 20:57:29.713332 139649090267008 learning.py:507] global step 790: loss = 0.2571 (3.009 sec/step)\n",
            "I0619 20:57:32.694839 139649090267008 learning.py:507] global step 790: loss = 0.2722 (2.980 sec/step)\n",
            "I0619 20:57:35.818190 139649090267008 learning.py:507] global step 790: loss = 0.2820 (3.122 sec/step)\n",
            "I0619 20:57:38.748595 139649090267008 learning.py:507] global step 790: loss = 0.2677 (2.927 sec/step)\n",
            "I0619 20:57:41.685860 139649090267008 learning.py:507] global step 791: loss = 0.2622 (2.934 sec/step)\n",
            "I0619 20:57:44.669812 139649090267008 learning.py:507] global step 791: loss = 0.3004 (2.982 sec/step)\n",
            "I0619 20:57:47.812781 139649090267008 learning.py:507] global step 791: loss = 0.2041 (3.141 sec/step)\n",
            "I0619 20:57:50.819262 139649090267008 learning.py:507] global step 791: loss = 0.3267 (3.005 sec/step)\n",
            "I0619 20:57:54.022847 139649090267008 learning.py:507] global step 791: loss = 0.2741 (3.202 sec/step)\n",
            "I0619 20:57:57.018778 139649090267008 learning.py:507] global step 791: loss = 0.2343 (2.994 sec/step)\n",
            "I0619 20:57:59.946587 139649090267008 learning.py:507] global step 791: loss = 0.3332 (2.926 sec/step)\n",
            "I0619 20:58:02.941801 139649090267008 learning.py:507] global step 791: loss = 0.2804 (2.994 sec/step)\n",
            "I0619 20:58:06.189342 139649090267008 learning.py:507] global step 792: loss = 0.2482 (3.245 sec/step)\n",
            "I0619 20:58:09.167296 139649090267008 learning.py:507] global step 792: loss = 0.3525 (2.976 sec/step)\n",
            "I0619 20:58:12.153155 139649090267008 learning.py:507] global step 792: loss = 0.2379 (2.984 sec/step)\n",
            "I0619 20:58:15.175565 139649090267008 learning.py:507] global step 792: loss = 0.3589 (3.021 sec/step)\n",
            "I0619 20:58:18.194773 139649090267008 learning.py:507] global step 792: loss = 0.2422 (3.017 sec/step)\n",
            "I0619 20:58:21.168853 139649090267008 learning.py:507] global step 792: loss = 0.3604 (2.972 sec/step)\n",
            "I0619 20:58:26.096272 139646017689344 supervisor.py:1050] Recording summary at step 792.\n",
            "I0619 20:58:26.688994 139649090267008 learning.py:507] global step 792: loss = 0.2462 (5.280 sec/step)\n",
            "I0619 20:58:29.727804 139649090267008 learning.py:507] global step 792: loss = 0.2656 (3.037 sec/step)\n",
            "I0619 20:58:32.740544 139649090267008 learning.py:507] global step 793: loss = 0.3515 (3.010 sec/step)\n",
            "I0619 20:58:35.801954 139649090267008 learning.py:507] global step 793: loss = 0.2783 (3.060 sec/step)\n",
            "I0619 20:58:38.822769 139649090267008 learning.py:507] global step 793: loss = 0.2783 (3.019 sec/step)\n",
            "I0619 20:58:41.979420 139649090267008 learning.py:507] global step 793: loss = 0.2126 (3.155 sec/step)\n",
            "I0619 20:58:44.976850 139649090267008 learning.py:507] global step 793: loss = 0.2632 (2.996 sec/step)\n",
            "I0619 20:58:48.094028 139649090267008 learning.py:507] global step 793: loss = 0.2866 (3.115 sec/step)\n",
            "I0619 20:58:51.137786 139649090267008 learning.py:507] global step 793: loss = 0.2897 (3.042 sec/step)\n",
            "I0619 20:58:54.230878 139649090267008 learning.py:507] global step 793: loss = 0.2785 (3.091 sec/step)\n",
            "I0619 20:58:57.361527 139649090267008 learning.py:507] global step 794: loss = 0.2215 (3.128 sec/step)\n",
            "I0619 20:59:00.368196 139649090267008 learning.py:507] global step 794: loss = 0.2567 (3.005 sec/step)\n",
            "I0619 20:59:03.410998 139649090267008 learning.py:507] global step 794: loss = 0.3377 (3.041 sec/step)\n",
            "I0619 20:59:06.511899 139649090267008 learning.py:507] global step 794: loss = 0.2913 (3.099 sec/step)\n",
            "I0619 20:59:09.756992 139649090267008 learning.py:507] global step 794: loss = 0.2809 (3.243 sec/step)\n",
            "I0619 20:59:12.772418 139649090267008 learning.py:507] global step 794: loss = 0.2796 (3.014 sec/step)\n",
            "I0619 20:59:15.740084 139649090267008 learning.py:507] global step 794: loss = 0.2476 (2.966 sec/step)\n",
            "I0619 20:59:18.709937 139649090267008 learning.py:507] global step 794: loss = 0.2927 (2.968 sec/step)\n",
            "I0619 20:59:21.905894 139649090267008 learning.py:507] global step 795: loss = 0.3055 (3.193 sec/step)\n",
            "I0619 20:59:24.882509 139649090267008 learning.py:507] global step 795: loss = 0.3175 (2.975 sec/step)\n",
            "I0619 20:59:28.088878 139649090267008 learning.py:507] global step 795: loss = 0.3250 (3.205 sec/step)\n",
            "I0619 20:59:30.992292 139649090267008 learning.py:507] global step 795: loss = 0.2978 (2.902 sec/step)\n",
            "I0619 20:59:33.948445 139649090267008 learning.py:507] global step 795: loss = 0.2771 (2.955 sec/step)\n",
            "I0619 20:59:37.055792 139649090267008 learning.py:507] global step 795: loss = 0.2489 (3.106 sec/step)\n",
            "I0619 20:59:40.299044 139649090267008 learning.py:507] global step 795: loss = 0.2665 (3.242 sec/step)\n",
            "I0619 20:59:43.303003 139649090267008 learning.py:507] global step 795: loss = 0.2958 (3.002 sec/step)\n",
            "I0619 20:59:46.313203 139649090267008 learning.py:507] global step 796: loss = 0.2293 (3.008 sec/step)\n",
            "I0619 20:59:49.346253 139649090267008 learning.py:507] global step 796: loss = 0.2447 (3.031 sec/step)\n",
            "I0619 20:59:52.382116 139649090267008 learning.py:507] global step 796: loss = 0.3132 (3.034 sec/step)\n",
            "I0619 20:59:55.389349 139649090267008 learning.py:507] global step 796: loss = 0.3882 (3.006 sec/step)\n",
            "I0619 20:59:58.395755 139649090267008 learning.py:507] global step 796: loss = 0.2429 (3.005 sec/step)\n",
            "I0619 21:00:01.377053 139649090267008 learning.py:507] global step 796: loss = 0.2581 (2.980 sec/step)\n",
            "I0619 21:00:04.347526 139649090267008 learning.py:507] global step 796: loss = 0.3265 (2.969 sec/step)\n",
            "I0619 21:00:07.336324 139649090267008 learning.py:507] global step 796: loss = 0.2532 (2.987 sec/step)\n",
            "I0619 21:00:10.382556 139649090267008 learning.py:507] global step 797: loss = 0.2593 (3.044 sec/step)\n",
            "I0619 21:00:13.385862 139649090267008 learning.py:507] global step 797: loss = 0.2438 (3.002 sec/step)\n",
            "I0619 21:00:16.480599 139649090267008 learning.py:507] global step 797: loss = 0.2490 (3.093 sec/step)\n",
            "I0619 21:00:19.571659 139649090267008 learning.py:507] global step 797: loss = 0.2411 (3.089 sec/step)\n",
            "I0619 21:00:20.931301 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:00:25.747769 139649090267008 learning.py:507] global step 797: loss = 0.3107 (6.169 sec/step)\n",
            "I0619 21:00:27.452320 139646017689344 supervisor.py:1050] Recording summary at step 797.\n",
            "I0619 21:00:29.481359 139649090267008 learning.py:507] global step 797: loss = 0.2950 (3.731 sec/step)\n",
            "I0619 21:00:32.525998 139649090267008 learning.py:507] global step 797: loss = 0.3207 (3.043 sec/step)\n",
            "I0619 21:00:35.519860 139649090267008 learning.py:507] global step 797: loss = 0.3005 (2.992 sec/step)\n",
            "I0619 21:00:38.647573 139649090267008 learning.py:507] global step 798: loss = 0.2457 (3.125 sec/step)\n",
            "I0619 21:00:41.687888 139649090267008 learning.py:507] global step 798: loss = 0.2222 (3.038 sec/step)\n",
            "I0619 21:00:44.674908 139649090267008 learning.py:507] global step 798: loss = 0.2565 (2.985 sec/step)\n",
            "I0619 21:00:47.741413 139649090267008 learning.py:507] global step 798: loss = 0.3055 (3.065 sec/step)\n",
            "I0619 21:00:50.809077 139649090267008 learning.py:507] global step 798: loss = 0.2630 (3.065 sec/step)\n",
            "I0619 21:00:53.829994 139649090267008 learning.py:507] global step 798: loss = 0.2546 (3.019 sec/step)\n",
            "I0619 21:00:56.834004 139649090267008 learning.py:507] global step 798: loss = 0.2654 (3.002 sec/step)\n",
            "I0619 21:00:59.920131 139649090267008 learning.py:507] global step 798: loss = 0.2806 (3.084 sec/step)\n",
            "I0619 21:01:03.024790 139649090267008 learning.py:507] global step 799: loss = 0.2473 (3.102 sec/step)\n",
            "I0619 21:01:06.081254 139649090267008 learning.py:507] global step 799: loss = 0.3253 (3.055 sec/step)\n",
            "I0619 21:01:09.141001 139649090267008 learning.py:507] global step 799: loss = 0.2411 (3.058 sec/step)\n",
            "I0619 21:01:12.217632 139649090267008 learning.py:507] global step 799: loss = 0.3198 (3.075 sec/step)\n",
            "I0619 21:01:15.324105 139649090267008 learning.py:507] global step 799: loss = 0.3671 (3.105 sec/step)\n",
            "I0619 21:01:18.390933 139649090267008 learning.py:507] global step 799: loss = 0.2085 (3.065 sec/step)\n",
            "I0619 21:01:21.418043 139649090267008 learning.py:507] global step 799: loss = 0.2438 (3.025 sec/step)\n",
            "I0619 21:01:24.432168 139649090267008 learning.py:507] global step 799: loss = 0.2295 (3.012 sec/step)\n",
            "I0619 21:01:27.526043 139649090267008 learning.py:507] global step 800: loss = 0.2317 (3.091 sec/step)\n",
            "I0619 21:01:30.613787 139649090267008 learning.py:507] global step 800: loss = 0.3100 (3.086 sec/step)\n",
            "I0619 21:01:33.636592 139649090267008 learning.py:507] global step 800: loss = 0.2622 (3.021 sec/step)\n",
            "I0619 21:01:36.705610 139649090267008 learning.py:507] global step 800: loss = 0.2672 (3.067 sec/step)\n",
            "I0619 21:01:39.717324 139649090267008 learning.py:507] global step 800: loss = 0.2476 (3.010 sec/step)\n",
            "I0619 21:01:43.063706 139649090267008 learning.py:507] global step 800: loss = 0.2709 (3.345 sec/step)\n",
            "I0619 21:01:46.224899 139649090267008 learning.py:507] global step 800: loss = 0.3663 (3.159 sec/step)\n",
            "I0619 21:01:49.311201 139649090267008 learning.py:507] global step 800: loss = 0.3001 (3.085 sec/step)\n",
            "I0619 21:01:52.362085 139649090267008 learning.py:507] global step 801: loss = 0.3330 (3.049 sec/step)\n",
            "I0619 21:01:55.350975 139649090267008 learning.py:507] global step 801: loss = 0.3157 (2.987 sec/step)\n",
            "I0619 21:01:58.379028 139649090267008 learning.py:507] global step 801: loss = 0.3472 (3.026 sec/step)\n",
            "I0619 21:02:01.738128 139649090267008 learning.py:507] global step 801: loss = 0.2858 (3.357 sec/step)\n",
            "I0619 21:02:04.983192 139649090267008 learning.py:507] global step 801: loss = 0.3001 (3.243 sec/step)\n",
            "I0619 21:02:08.050009 139649090267008 learning.py:507] global step 801: loss = 0.2821 (3.065 sec/step)\n",
            "I0619 21:02:11.112685 139649090267008 learning.py:507] global step 801: loss = 0.2588 (3.061 sec/step)\n",
            "I0619 21:02:14.122917 139649090267008 learning.py:507] global step 801: loss = 0.2998 (3.009 sec/step)\n",
            "I0619 21:02:17.177407 139649090267008 learning.py:507] global step 802: loss = 0.3589 (3.052 sec/step)\n",
            "I0619 21:02:20.291188 139649090267008 learning.py:507] global step 802: loss = 0.2696 (3.112 sec/step)\n",
            "I0619 21:02:25.474119 139649090267008 learning.py:507] global step 802: loss = 0.3584 (5.181 sec/step)\n",
            "I0619 21:02:26.603694 139646017689344 supervisor.py:1050] Recording summary at step 802.\n",
            "I0619 21:02:28.739053 139649090267008 learning.py:507] global step 802: loss = 0.2329 (3.255 sec/step)\n",
            "I0619 21:02:31.820054 139649090267008 learning.py:507] global step 802: loss = 0.2548 (3.079 sec/step)\n",
            "I0619 21:02:34.957546 139649090267008 learning.py:507] global step 802: loss = 0.3528 (3.136 sec/step)\n",
            "I0619 21:02:38.061113 139649090267008 learning.py:507] global step 802: loss = 0.2600 (3.102 sec/step)\n",
            "I0619 21:02:41.082106 139649090267008 learning.py:507] global step 802: loss = 0.2488 (3.019 sec/step)\n",
            "I0619 21:02:44.148601 139649090267008 learning.py:507] global step 803: loss = 0.2540 (3.065 sec/step)\n",
            "I0619 21:02:47.296895 139649090267008 learning.py:507] global step 803: loss = 0.2796 (3.146 sec/step)\n",
            "I0619 21:02:50.398694 139649090267008 learning.py:507] global step 803: loss = 0.2912 (3.100 sec/step)\n",
            "I0619 21:02:53.385472 139649090267008 learning.py:507] global step 803: loss = 0.2665 (2.985 sec/step)\n",
            "I0619 21:02:56.483027 139649090267008 learning.py:507] global step 803: loss = 0.2655 (3.096 sec/step)\n",
            "I0619 21:02:59.530803 139649090267008 learning.py:507] global step 803: loss = 0.2594 (3.046 sec/step)\n",
            "I0619 21:03:02.573321 139649090267008 learning.py:507] global step 803: loss = 0.2905 (3.040 sec/step)\n",
            "I0619 21:03:05.826178 139649090267008 learning.py:507] global step 803: loss = 0.2248 (3.251 sec/step)\n",
            "I0619 21:03:08.930313 139649090267008 learning.py:507] global step 804: loss = 0.2391 (3.102 sec/step)\n",
            "I0619 21:03:11.989791 139649090267008 learning.py:507] global step 804: loss = 0.3240 (3.058 sec/step)\n",
            "I0619 21:03:15.056226 139649090267008 learning.py:507] global step 804: loss = 0.3410 (3.065 sec/step)\n",
            "I0619 21:03:18.142640 139649090267008 learning.py:507] global step 804: loss = 0.2740 (3.085 sec/step)\n",
            "I0619 21:03:21.223507 139649090267008 learning.py:507] global step 804: loss = 0.4181 (3.079 sec/step)\n",
            "I0619 21:03:24.274722 139649090267008 learning.py:507] global step 804: loss = 0.2740 (3.049 sec/step)\n",
            "I0619 21:03:27.492379 139649090267008 learning.py:507] global step 804: loss = 0.2308 (3.216 sec/step)\n",
            "I0619 21:03:30.549319 139649090267008 learning.py:507] global step 804: loss = 0.2307 (3.055 sec/step)\n",
            "I0619 21:03:33.755824 139649090267008 learning.py:507] global step 805: loss = 0.2725 (3.204 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:03:36.840027 139649090267008 learning.py:507] global step 805: loss = 0.2475 (3.082 sec/step)\n",
            "I0619 21:03:39.937639 139649090267008 learning.py:507] global step 805: loss = 0.2660 (3.096 sec/step)\n",
            "I0619 21:03:43.181737 139649090267008 learning.py:507] global step 805: loss = 0.2353 (3.242 sec/step)\n",
            "I0619 21:03:46.358374 139649090267008 learning.py:507] global step 805: loss = 0.2370 (3.175 sec/step)\n",
            "I0619 21:03:49.485767 139649090267008 learning.py:507] global step 805: loss = 0.2802 (3.125 sec/step)\n",
            "I0619 21:03:52.748366 139649090267008 learning.py:507] global step 805: loss = 0.2372 (3.261 sec/step)\n",
            "I0619 21:03:55.880883 139649090267008 learning.py:507] global step 805: loss = 0.2667 (3.131 sec/step)\n",
            "I0619 21:03:58.976563 139649090267008 learning.py:507] global step 806: loss = 0.3221 (3.093 sec/step)\n",
            "I0619 21:04:02.276298 139649090267008 learning.py:507] global step 806: loss = 0.2287 (3.298 sec/step)\n",
            "I0619 21:04:05.275498 139649090267008 learning.py:507] global step 806: loss = 0.2339 (2.997 sec/step)\n",
            "I0619 21:04:08.457605 139649090267008 learning.py:507] global step 806: loss = 0.2620 (3.180 sec/step)\n",
            "I0619 21:04:11.575133 139649090267008 learning.py:507] global step 806: loss = 0.2909 (3.115 sec/step)\n",
            "I0619 21:04:14.735688 139649090267008 learning.py:507] global step 806: loss = 0.2453 (3.159 sec/step)\n",
            "I0619 21:04:17.851628 139649090267008 learning.py:507] global step 806: loss = 0.2821 (3.114 sec/step)\n",
            "I0619 21:04:20.914858 139649090267008 learning.py:507] global step 806: loss = 0.2720 (3.061 sec/step)\n",
            "I0619 21:04:25.775900 139646017689344 supervisor.py:1050] Recording summary at step 806.\n",
            "I0619 21:04:26.366163 139649090267008 learning.py:507] global step 807: loss = 0.3127 (5.449 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:04:29.459975 139649090267008 learning.py:507] global step 807: loss = 0.3205 (3.092 sec/step)\n",
            "I0619 21:04:32.501815 139649090267008 learning.py:507] global step 807: loss = 0.2151 (3.040 sec/step)\n",
            "I0619 21:04:35.523254 139649090267008 learning.py:507] global step 807: loss = 0.2795 (3.020 sec/step)\n",
            "I0619 21:04:38.555224 139649090267008 learning.py:507] global step 807: loss = 0.2865 (3.030 sec/step)\n",
            "I0619 21:04:41.599957 139649090267008 learning.py:507] global step 807: loss = 0.3562 (3.043 sec/step)\n",
            "I0619 21:04:44.657415 139649090267008 learning.py:507] global step 807: loss = 0.2596 (3.055 sec/step)\n",
            "I0619 21:04:47.671016 139649090267008 learning.py:507] global step 807: loss = 0.3423 (3.012 sec/step)\n",
            "I0619 21:04:50.782267 139649090267008 learning.py:507] global step 808: loss = 0.3004 (3.109 sec/step)\n",
            "I0619 21:04:53.853163 139649090267008 learning.py:507] global step 808: loss = 0.3768 (3.069 sec/step)\n",
            "I0619 21:04:57.011023 139649090267008 learning.py:507] global step 808: loss = 0.2554 (3.156 sec/step)\n",
            "I0619 21:05:00.116799 139649090267008 learning.py:507] global step 808: loss = 0.2655 (3.104 sec/step)\n",
            "I0619 21:05:03.118242 139649090267008 learning.py:507] global step 808: loss = 0.4009 (3.000 sec/step)\n",
            "I0619 21:05:06.129144 139649090267008 learning.py:507] global step 808: loss = 0.2187 (3.009 sec/step)\n",
            "I0619 21:05:09.198447 139649090267008 learning.py:507] global step 808: loss = 0.2509 (3.068 sec/step)\n",
            "I0619 21:05:12.518981 139649090267008 learning.py:507] global step 808: loss = 0.2459 (3.319 sec/step)\n",
            "I0619 21:05:15.777016 139649090267008 learning.py:507] global step 809: loss = 0.2854 (3.256 sec/step)\n",
            "I0619 21:05:18.878908 139649090267008 learning.py:507] global step 809: loss = 0.2925 (3.100 sec/step)\n",
            "I0619 21:05:21.946263 139649090267008 learning.py:507] global step 809: loss = 0.2207 (3.066 sec/step)\n",
            "I0619 21:05:25.067638 139649090267008 learning.py:507] global step 809: loss = 0.2917 (3.120 sec/step)\n",
            "I0619 21:05:28.173946 139649090267008 learning.py:507] global step 809: loss = 0.3107 (3.105 sec/step)\n",
            "I0619 21:05:31.517811 139649090267008 learning.py:507] global step 809: loss = 0.2563 (3.336 sec/step)\n",
            "I0619 21:05:34.643799 139649090267008 learning.py:507] global step 809: loss = 0.3005 (3.123 sec/step)\n",
            "I0619 21:05:37.659656 139649090267008 learning.py:507] global step 809: loss = 0.2405 (3.014 sec/step)\n",
            "I0619 21:05:40.700868 139649090267008 learning.py:507] global step 810: loss = 0.2443 (3.039 sec/step)\n",
            "I0619 21:05:43.832706 139649090267008 learning.py:507] global step 810: loss = 0.2493 (3.130 sec/step)\n",
            "I0619 21:05:46.848550 139649090267008 learning.py:507] global step 810: loss = 0.3552 (3.014 sec/step)\n",
            "I0619 21:05:49.921520 139649090267008 learning.py:507] global step 810: loss = 0.2294 (3.071 sec/step)\n",
            "I0619 21:05:53.016164 139649090267008 learning.py:507] global step 810: loss = 0.2585 (3.092 sec/step)\n",
            "I0619 21:05:56.100839 139649090267008 learning.py:507] global step 810: loss = 0.2709 (3.083 sec/step)\n",
            "I0619 21:05:59.200555 139649090267008 learning.py:507] global step 810: loss = 0.2538 (3.098 sec/step)\n",
            "I0619 21:06:02.378438 139649090267008 learning.py:507] global step 810: loss = 0.2769 (3.176 sec/step)\n",
            "I0619 21:06:05.579679 139649090267008 learning.py:507] global step 811: loss = 0.3200 (3.199 sec/step)\n",
            "I0619 21:06:08.595080 139649090267008 learning.py:507] global step 811: loss = 0.2446 (3.014 sec/step)\n",
            "I0619 21:06:11.654926 139649090267008 learning.py:507] global step 811: loss = 0.2511 (3.058 sec/step)\n",
            "I0619 21:06:14.735562 139649090267008 learning.py:507] global step 811: loss = 0.2478 (3.079 sec/step)\n",
            "I0619 21:06:17.828576 139649090267008 learning.py:507] global step 811: loss = 0.2477 (3.091 sec/step)\n",
            "I0619 21:06:21.412101 139649090267008 learning.py:507] global step 811: loss = 0.3036 (3.211 sec/step)\n",
            "I0619 21:06:26.585450 139649090267008 learning.py:507] global step 811: loss = 0.2719 (4.983 sec/step)\n",
            "I0619 21:06:27.198563 139646017689344 supervisor.py:1050] Recording summary at step 811.\n",
            "I0619 21:06:29.697080 139649090267008 learning.py:507] global step 811: loss = 0.2145 (3.106 sec/step)\n",
            "I0619 21:06:32.866118 139649090267008 learning.py:507] global step 812: loss = 0.2499 (3.167 sec/step)\n",
            "I0619 21:06:35.912923 139649090267008 learning.py:507] global step 812: loss = 0.2775 (3.045 sec/step)\n",
            "I0619 21:06:38.980117 139649090267008 learning.py:507] global step 812: loss = 0.2526 (3.066 sec/step)\n",
            "I0619 21:06:42.018769 139649090267008 learning.py:507] global step 812: loss = 0.2422 (3.037 sec/step)\n",
            "I0619 21:06:45.151897 139649090267008 learning.py:507] global step 812: loss = 0.2536 (3.131 sec/step)\n",
            "I0619 21:06:48.289037 139649090267008 learning.py:507] global step 812: loss = 0.2867 (3.135 sec/step)\n",
            "I0619 21:06:51.383121 139649090267008 learning.py:507] global step 812: loss = 0.3000 (3.092 sec/step)\n",
            "I0619 21:06:54.498996 139649090267008 learning.py:507] global step 812: loss = 0.2747 (3.114 sec/step)\n",
            "I0619 21:06:57.609563 139649090267008 learning.py:507] global step 813: loss = 0.2996 (3.108 sec/step)\n",
            "I0619 21:07:00.688906 139649090267008 learning.py:507] global step 813: loss = 0.2782 (3.078 sec/step)\n",
            "I0619 21:07:03.853232 139649090267008 learning.py:507] global step 813: loss = 0.2830 (3.162 sec/step)\n",
            "I0619 21:07:06.968824 139649090267008 learning.py:507] global step 813: loss = 0.2789 (3.114 sec/step)\n",
            "I0619 21:07:09.987240 139649090267008 learning.py:507] global step 813: loss = 0.2859 (3.017 sec/step)\n",
            "I0619 21:07:13.056533 139649090267008 learning.py:507] global step 813: loss = 0.2357 (3.067 sec/step)\n",
            "I0619 21:07:16.138613 139649090267008 learning.py:507] global step 813: loss = 0.3141 (3.080 sec/step)\n",
            "I0619 21:07:19.304225 139649090267008 learning.py:507] global step 813: loss = 0.2498 (3.164 sec/step)\n",
            "I0619 21:07:22.441374 139649090267008 learning.py:507] global step 814: loss = 0.2693 (3.135 sec/step)\n",
            "I0619 21:07:25.520285 139649090267008 learning.py:507] global step 814: loss = 0.3096 (3.077 sec/step)\n",
            "I0619 21:07:28.584464 139649090267008 learning.py:507] global step 814: loss = 0.2558 (3.062 sec/step)\n",
            "I0619 21:07:31.699655 139649090267008 learning.py:507] global step 814: loss = 0.2564 (3.113 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:07:34.724765 139649090267008 learning.py:507] global step 814: loss = 0.2951 (3.023 sec/step)\n",
            "I0619 21:07:38.023880 139649090267008 learning.py:507] global step 814: loss = 0.2721 (3.297 sec/step)\n",
            "I0619 21:07:41.116327 139649090267008 learning.py:507] global step 814: loss = 0.2250 (3.090 sec/step)\n",
            "I0619 21:07:44.181204 139649090267008 learning.py:507] global step 814: loss = 0.3697 (3.063 sec/step)\n",
            "I0619 21:07:47.425622 139649090267008 learning.py:507] global step 815: loss = 0.2387 (3.242 sec/step)\n",
            "I0619 21:07:50.474067 139649090267008 learning.py:507] global step 815: loss = 0.2530 (3.046 sec/step)\n",
            "I0619 21:07:53.556221 139649090267008 learning.py:507] global step 815: loss = 0.2548 (3.080 sec/step)\n",
            "I0619 21:07:56.694686 139649090267008 learning.py:507] global step 815: loss = 0.3054 (3.137 sec/step)\n",
            "I0619 21:07:59.812260 139649090267008 learning.py:507] global step 815: loss = 0.2852 (3.116 sec/step)\n",
            "I0619 21:08:02.838988 139649090267008 learning.py:507] global step 815: loss = 0.2458 (3.025 sec/step)\n",
            "I0619 21:08:06.071776 139649090267008 learning.py:507] global step 815: loss = 0.2500 (3.231 sec/step)\n",
            "I0619 21:08:09.154884 139649090267008 learning.py:507] global step 815: loss = 0.2649 (3.081 sec/step)\n",
            "I0619 21:08:12.224372 139649090267008 learning.py:507] global step 816: loss = 0.2859 (3.067 sec/step)\n",
            "I0619 21:08:15.280378 139649090267008 learning.py:507] global step 816: loss = 0.2795 (3.054 sec/step)\n",
            "I0619 21:08:18.388634 139649090267008 learning.py:507] global step 816: loss = 0.2975 (3.106 sec/step)\n",
            "I0619 21:08:21.481583 139649090267008 learning.py:507] global step 816: loss = 0.2488 (3.091 sec/step)\n",
            "I0619 21:08:26.096275 139646017689344 supervisor.py:1050] Recording summary at step 816.\n",
            "I0619 21:08:26.717604 139649090267008 learning.py:507] global step 816: loss = 0.2586 (5.234 sec/step)\n",
            "I0619 21:08:29.809907 139649090267008 learning.py:507] global step 816: loss = 0.2868 (3.091 sec/step)\n",
            "I0619 21:08:32.886896 139649090267008 learning.py:507] global step 816: loss = 0.2197 (3.075 sec/step)\n",
            "I0619 21:08:35.960230 139649090267008 learning.py:507] global step 816: loss = 0.3685 (3.071 sec/step)\n",
            "I0619 21:08:39.084143 139649090267008 learning.py:507] global step 817: loss = 0.3091 (3.122 sec/step)\n",
            "I0619 21:08:42.200473 139649090267008 learning.py:507] global step 817: loss = 0.2303 (3.114 sec/step)\n",
            "I0619 21:08:45.527505 139649090267008 learning.py:507] global step 817: loss = 0.2073 (3.325 sec/step)\n",
            "I0619 21:08:48.641434 139649090267008 learning.py:507] global step 817: loss = 0.2422 (3.112 sec/step)\n",
            "I0619 21:08:51.744217 139649090267008 learning.py:507] global step 817: loss = 0.2501 (3.101 sec/step)\n",
            "I0619 21:08:54.826835 139649090267008 learning.py:507] global step 817: loss = 0.3099 (3.080 sec/step)\n",
            "I0619 21:08:57.980484 139649090267008 learning.py:507] global step 817: loss = 0.4138 (3.152 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:09:01.103846 139649090267008 learning.py:507] global step 817: loss = 0.2749 (3.122 sec/step)\n",
            "I0619 21:09:04.671827 139649090267008 learning.py:507] global step 818: loss = 0.2429 (3.566 sec/step)\n",
            "I0619 21:09:07.741323 139649090267008 learning.py:507] global step 818: loss = 0.3251 (3.068 sec/step)\n",
            "I0619 21:09:10.859753 139649090267008 learning.py:507] global step 818: loss = 0.2968 (3.116 sec/step)\n",
            "I0619 21:09:13.894803 139649090267008 learning.py:507] global step 818: loss = 0.2361 (3.033 sec/step)\n",
            "I0619 21:09:17.000877 139649090267008 learning.py:507] global step 818: loss = 0.3241 (3.104 sec/step)\n",
            "I0619 21:09:20.345400 139649090267008 learning.py:507] global step 818: loss = 0.2510 (3.343 sec/step)\n",
            "I0619 21:09:23.447325 139649090267008 learning.py:507] global step 818: loss = 0.3437 (3.100 sec/step)\n",
            "I0619 21:09:26.573397 139649090267008 learning.py:507] global step 818: loss = 0.2768 (3.124 sec/step)\n",
            "I0619 21:09:29.718587 139649090267008 learning.py:507] global step 819: loss = 0.3825 (3.143 sec/step)\n",
            "I0619 21:09:32.766572 139649090267008 learning.py:507] global step 819: loss = 0.2628 (3.046 sec/step)\n",
            "I0619 21:09:35.864510 139649090267008 learning.py:507] global step 819: loss = 0.2374 (3.095 sec/step)\n",
            "I0619 21:09:39.207947 139649090267008 learning.py:507] global step 819: loss = 0.3524 (3.342 sec/step)\n",
            "I0619 21:09:42.309948 139649090267008 learning.py:507] global step 819: loss = 0.2549 (3.100 sec/step)\n",
            "I0619 21:09:45.398117 139649090267008 learning.py:507] global step 819: loss = 0.2663 (3.086 sec/step)\n",
            "I0619 21:09:48.521901 139649090267008 learning.py:507] global step 819: loss = 0.2778 (3.122 sec/step)\n",
            "I0619 21:09:51.624758 139649090267008 learning.py:507] global step 819: loss = 0.2099 (3.101 sec/step)\n",
            "I0619 21:09:54.724605 139649090267008 learning.py:507] global step 820: loss = 0.2389 (3.097 sec/step)\n",
            "I0619 21:09:57.768587 139649090267008 learning.py:507] global step 820: loss = 0.3003 (3.042 sec/step)\n",
            "I0619 21:10:00.893898 139649090267008 learning.py:507] global step 820: loss = 0.3053 (3.124 sec/step)\n",
            "I0619 21:10:03.976127 139649090267008 learning.py:507] global step 820: loss = 0.2128 (3.081 sec/step)\n",
            "I0619 21:10:07.037654 139649090267008 learning.py:507] global step 820: loss = 0.2538 (3.060 sec/step)\n",
            "I0619 21:10:10.076922 139649090267008 learning.py:507] global step 820: loss = 0.2801 (3.037 sec/step)\n",
            "I0619 21:10:13.122010 139649090267008 learning.py:507] global step 820: loss = 0.2612 (3.043 sec/step)\n",
            "I0619 21:10:16.166357 139649090267008 learning.py:507] global step 820: loss = 0.3382 (3.043 sec/step)\n",
            "I0619 21:10:19.200832 139649090267008 learning.py:507] global step 821: loss = 0.2624 (3.033 sec/step)\n",
            "I0619 21:10:20.931444 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:10:25.465288 139649090267008 learning.py:507] global step 821: loss = 0.3475 (6.248 sec/step)\n",
            "I0619 21:10:27.196710 139646017689344 supervisor.py:1050] Recording summary at step 821.\n",
            "I0619 21:10:29.213167 139649090267008 learning.py:507] global step 821: loss = 0.2410 (3.743 sec/step)\n",
            "I0619 21:10:32.336650 139649090267008 learning.py:507] global step 821: loss = 0.2457 (3.122 sec/step)\n",
            "I0619 21:10:35.485120 139649090267008 learning.py:507] global step 821: loss = 0.2371 (3.147 sec/step)\n",
            "I0619 21:10:38.604358 139649090267008 learning.py:507] global step 821: loss = 0.2334 (3.117 sec/step)\n",
            "I0619 21:10:41.735086 139649090267008 learning.py:507] global step 821: loss = 0.2825 (3.129 sec/step)\n",
            "I0619 21:10:44.740665 139649090267008 learning.py:507] global step 821: loss = 0.2423 (3.004 sec/step)\n",
            "I0619 21:10:47.797909 139649090267008 learning.py:507] global step 822: loss = 0.3279 (3.055 sec/step)\n",
            "I0619 21:10:50.930053 139649090267008 learning.py:507] global step 822: loss = 0.2746 (3.130 sec/step)\n",
            "I0619 21:10:54.082532 139649090267008 learning.py:507] global step 822: loss = 0.2628 (3.151 sec/step)\n",
            "I0619 21:10:57.378299 139649090267008 learning.py:507] global step 822: loss = 0.3606 (3.294 sec/step)\n",
            "I0619 21:11:00.528702 139649090267008 learning.py:507] global step 822: loss = 0.3111 (3.149 sec/step)\n",
            "I0619 21:11:03.657613 139649090267008 learning.py:507] global step 822: loss = 0.2589 (3.127 sec/step)\n",
            "I0619 21:11:06.750844 139649090267008 learning.py:507] global step 822: loss = 0.3121 (3.091 sec/step)\n",
            "I0619 21:11:09.777974 139649090267008 learning.py:507] global step 822: loss = 0.2550 (3.025 sec/step)\n",
            "I0619 21:11:12.870198 139649090267008 learning.py:507] global step 823: loss = 0.2245 (3.090 sec/step)\n",
            "I0619 21:11:16.185089 139649090267008 learning.py:507] global step 823: loss = 0.3212 (3.313 sec/step)\n",
            "I0619 21:11:19.243785 139649090267008 learning.py:507] global step 823: loss = 0.3579 (3.057 sec/step)\n",
            "I0619 21:11:22.300190 139649090267008 learning.py:507] global step 823: loss = 0.3158 (3.055 sec/step)\n",
            "I0619 21:11:25.391233 139649090267008 learning.py:507] global step 823: loss = 0.3777 (3.089 sec/step)\n",
            "I0619 21:11:28.442178 139649090267008 learning.py:507] global step 823: loss = 0.2667 (3.049 sec/step)\n",
            "I0619 21:11:31.489563 139649090267008 learning.py:507] global step 823: loss = 0.2756 (3.046 sec/step)\n",
            "I0619 21:11:34.628581 139649090267008 learning.py:507] global step 823: loss = 0.2565 (3.137 sec/step)\n",
            "I0619 21:11:37.707649 139649090267008 learning.py:507] global step 824: loss = 0.2443 (3.077 sec/step)\n",
            "I0619 21:11:40.806373 139649090267008 learning.py:507] global step 824: loss = 0.2956 (3.097 sec/step)\n",
            "I0619 21:11:43.882663 139649090267008 learning.py:507] global step 824: loss = 0.2732 (3.075 sec/step)\n",
            "I0619 21:11:46.933676 139649090267008 learning.py:507] global step 824: loss = 0.3164 (3.049 sec/step)\n",
            "I0619 21:11:50.010721 139649090267008 learning.py:507] global step 824: loss = 0.2140 (3.075 sec/step)\n",
            "I0619 21:11:53.055224 139649090267008 learning.py:507] global step 824: loss = 0.2771 (3.043 sec/step)\n",
            "I0619 21:11:56.202797 139649090267008 learning.py:507] global step 824: loss = 0.2106 (3.145 sec/step)\n",
            "I0619 21:11:59.478469 139649090267008 learning.py:507] global step 824: loss = 0.2957 (3.274 sec/step)\n",
            "I0619 21:12:02.630606 139649090267008 learning.py:507] global step 825: loss = 0.2635 (3.150 sec/step)\n",
            "I0619 21:12:05.780065 139649090267008 learning.py:507] global step 825: loss = 0.2168 (3.148 sec/step)\n",
            "I0619 21:12:08.835207 139649090267008 learning.py:507] global step 825: loss = 0.3680 (3.053 sec/step)\n",
            "I0619 21:12:12.004560 139649090267008 learning.py:507] global step 825: loss = 0.2260 (3.168 sec/step)\n",
            "I0619 21:12:15.075297 139649090267008 learning.py:507] global step 825: loss = 0.3081 (3.069 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:12:18.382349 139649090267008 learning.py:507] global step 825: loss = 0.2414 (3.305 sec/step)\n",
            "I0619 21:12:21.757619 139649090267008 learning.py:507] global step 825: loss = 0.2474 (3.354 sec/step)\n",
            "I0619 21:12:26.299987 139646017689344 supervisor.py:1050] Recording summary at step 825.\n",
            "I0619 21:12:26.902423 139649090267008 learning.py:507] global step 825: loss = 0.2552 (5.129 sec/step)\n",
            "I0619 21:12:30.119956 139649090267008 learning.py:507] global step 826: loss = 0.2324 (3.215 sec/step)\n",
            "I0619 21:12:33.184700 139649090267008 learning.py:507] global step 826: loss = 0.2374 (3.063 sec/step)\n",
            "I0619 21:12:36.350210 139649090267008 learning.py:507] global step 826: loss = 0.2152 (3.164 sec/step)\n",
            "I0619 21:12:39.410711 139649090267008 learning.py:507] global step 826: loss = 0.4444 (3.059 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:12:42.547185 139649090267008 learning.py:507] global step 826: loss = 0.2615 (3.135 sec/step)\n",
            "I0619 21:12:45.594239 139649090267008 learning.py:507] global step 826: loss = 0.2432 (3.045 sec/step)\n",
            "I0619 21:12:48.692912 139649090267008 learning.py:507] global step 826: loss = 0.2482 (3.097 sec/step)\n",
            "I0619 21:12:51.828622 139649090267008 learning.py:507] global step 826: loss = 0.3000 (3.134 sec/step)\n",
            "I0619 21:12:54.949324 139649090267008 learning.py:507] global step 827: loss = 0.3663 (3.118 sec/step)\n",
            "I0619 21:12:58.131721 139649090267008 learning.py:507] global step 827: loss = 0.2561 (3.180 sec/step)\n",
            "I0619 21:13:01.242008 139649090267008 learning.py:507] global step 827: loss = 0.2749 (3.108 sec/step)\n",
            "I0619 21:13:04.308088 139649090267008 learning.py:507] global step 827: loss = 0.3405 (3.064 sec/step)\n",
            "I0619 21:13:07.559901 139649090267008 learning.py:507] global step 827: loss = 0.2704 (3.250 sec/step)\n",
            "I0619 21:13:10.624646 139649090267008 learning.py:507] global step 827: loss = 0.2555 (3.062 sec/step)\n",
            "I0619 21:13:13.744373 139649090267008 learning.py:507] global step 827: loss = 0.3158 (3.118 sec/step)\n",
            "I0619 21:13:16.871460 139649090267008 learning.py:507] global step 827: loss = 0.2676 (3.125 sec/step)\n",
            "I0619 21:13:19.975301 139649090267008 learning.py:507] global step 828: loss = 0.2722 (3.094 sec/step)\n",
            "I0619 21:13:23.043423 139649090267008 learning.py:507] global step 828: loss = 0.2352 (3.066 sec/step)\n",
            "I0619 21:13:26.259066 139649090267008 learning.py:507] global step 828: loss = 0.3762 (3.214 sec/step)\n",
            "I0619 21:13:29.323668 139649090267008 learning.py:507] global step 828: loss = 0.2837 (3.062 sec/step)\n",
            "I0619 21:13:32.368359 139649090267008 learning.py:507] global step 828: loss = 0.2606 (3.040 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:13:35.475108 139649090267008 learning.py:507] global step 828: loss = 0.2514 (3.105 sec/step)\n",
            "I0619 21:13:38.582953 139649090267008 learning.py:507] global step 828: loss = 0.2201 (3.106 sec/step)\n",
            "I0619 21:13:41.616753 139649090267008 learning.py:507] global step 828: loss = 0.2631 (3.032 sec/step)\n",
            "I0619 21:13:44.665834 139649090267008 learning.py:507] global step 829: loss = 0.3022 (3.047 sec/step)\n",
            "I0619 21:13:47.688163 139649090267008 learning.py:507] global step 829: loss = 0.3261 (3.020 sec/step)\n",
            "I0619 21:13:50.808067 139649090267008 learning.py:507] global step 829: loss = 0.2329 (3.118 sec/step)\n",
            "I0619 21:13:53.827793 139649090267008 learning.py:507] global step 829: loss = 0.2717 (3.018 sec/step)\n",
            "I0619 21:13:56.930069 139649090267008 learning.py:507] global step 829: loss = 0.2688 (3.101 sec/step)\n",
            "I0619 21:13:59.979835 139649090267008 learning.py:507] global step 829: loss = 0.2117 (3.048 sec/step)\n",
            "I0619 21:14:03.106997 139649090267008 learning.py:507] global step 829: loss = 0.2773 (3.125 sec/step)\n",
            "I0619 21:14:06.242730 139649090267008 learning.py:507] global step 829: loss = 0.2347 (3.134 sec/step)\n",
            "I0619 21:14:09.348872 139649090267008 learning.py:507] global step 830: loss = 0.2900 (3.104 sec/step)\n",
            "I0619 21:14:12.424530 139649090267008 learning.py:507] global step 830: loss = 0.2588 (3.074 sec/step)\n",
            "I0619 21:14:15.457883 139649090267008 learning.py:507] global step 830: loss = 0.2586 (3.032 sec/step)\n",
            "I0619 21:14:18.489808 139649090267008 learning.py:507] global step 830: loss = 0.2580 (3.030 sec/step)\n",
            "I0619 21:14:22.376366 139649090267008 learning.py:507] global step 830: loss = 0.2420 (3.839 sec/step)\n",
            "I0619 21:14:26.049253 139646017689344 supervisor.py:1050] Recording summary at step 830.\n",
            "I0619 21:14:26.962694 139649090267008 learning.py:507] global step 830: loss = 0.2400 (4.584 sec/step)\n",
            "I0619 21:14:30.125202 139649090267008 learning.py:507] global step 830: loss = 0.2803 (3.160 sec/step)\n",
            "I0619 21:14:33.247550 139649090267008 learning.py:507] global step 830: loss = 0.2192 (3.120 sec/step)\n",
            "I0619 21:14:36.358393 139649090267008 learning.py:507] global step 831: loss = 0.2284 (3.108 sec/step)\n",
            "I0619 21:14:39.506918 139649090267008 learning.py:507] global step 831: loss = 0.3652 (3.147 sec/step)\n",
            "I0619 21:14:42.750689 139649090267008 learning.py:507] global step 831: loss = 0.2245 (3.242 sec/step)\n",
            "I0619 21:14:45.796391 139649090267008 learning.py:507] global step 831: loss = 0.3863 (3.043 sec/step)\n",
            "I0619 21:14:48.839939 139649090267008 learning.py:507] global step 831: loss = 0.3242 (3.042 sec/step)\n",
            "I0619 21:14:51.959000 139649090267008 learning.py:507] global step 831: loss = 0.3949 (3.117 sec/step)\n",
            "I0619 21:14:54.996566 139649090267008 learning.py:507] global step 831: loss = 0.4480 (3.035 sec/step)\n",
            "I0619 21:14:58.100404 139649090267008 learning.py:507] global step 831: loss = 0.2448 (3.102 sec/step)\n",
            "I0619 21:15:01.201169 139649090267008 learning.py:507] global step 832: loss = 0.2295 (3.098 sec/step)\n",
            "I0619 21:15:04.506589 139649090267008 learning.py:507] global step 832: loss = 0.2772 (3.303 sec/step)\n",
            "I0619 21:15:07.589481 139649090267008 learning.py:507] global step 832: loss = 0.2710 (3.081 sec/step)\n",
            "I0619 21:15:10.631282 139649090267008 learning.py:507] global step 832: loss = 0.2353 (3.040 sec/step)\n",
            "I0619 21:15:13.871225 139649090267008 learning.py:507] global step 832: loss = 0.3539 (3.238 sec/step)\n",
            "I0619 21:15:16.978421 139649090267008 learning.py:507] global step 832: loss = 0.2601 (3.105 sec/step)\n",
            "I0619 21:15:20.056090 139649090267008 learning.py:507] global step 832: loss = 0.2398 (3.076 sec/step)\n",
            "I0619 21:15:23.402078 139649090267008 learning.py:507] global step 832: loss = 0.2526 (3.344 sec/step)\n",
            "I0619 21:15:26.463153 139649090267008 learning.py:507] global step 833: loss = 0.2910 (3.059 sec/step)\n",
            "I0619 21:15:29.567772 139649090267008 learning.py:507] global step 833: loss = 0.3099 (3.103 sec/step)\n",
            "I0619 21:15:32.834249 139649090267008 learning.py:507] global step 833: loss = 0.2547 (3.265 sec/step)\n",
            "I0619 21:15:35.968201 139649090267008 learning.py:507] global step 833: loss = 0.2855 (3.132 sec/step)\n",
            "I0619 21:15:39.094909 139649090267008 learning.py:507] global step 833: loss = 0.2786 (3.124 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:15:42.188176 139649090267008 learning.py:507] global step 833: loss = 0.2980 (3.091 sec/step)\n",
            "I0619 21:15:45.278084 139649090267008 learning.py:507] global step 833: loss = 0.2510 (3.088 sec/step)\n",
            "I0619 21:15:48.331179 139649090267008 learning.py:507] global step 833: loss = 0.2591 (3.051 sec/step)\n",
            "I0619 21:15:51.406850 139649090267008 learning.py:507] global step 834: loss = 0.2641 (3.073 sec/step)\n",
            "I0619 21:15:54.523380 139649090267008 learning.py:507] global step 834: loss = 0.2676 (3.114 sec/step)\n",
            "I0619 21:15:57.674795 139649090267008 learning.py:507] global step 834: loss = 0.3343 (3.150 sec/step)\n",
            "I0619 21:16:00.818934 139649090267008 learning.py:507] global step 834: loss = 0.2263 (3.142 sec/step)\n",
            "I0619 21:16:03.929384 139649090267008 learning.py:507] global step 834: loss = 0.3069 (3.109 sec/step)\n",
            "I0619 21:16:07.014466 139649090267008 learning.py:507] global step 834: loss = 0.2321 (3.083 sec/step)\n",
            "I0619 21:16:10.029324 139649090267008 learning.py:507] global step 834: loss = 0.2314 (3.013 sec/step)\n",
            "I0619 21:16:13.127291 139649090267008 learning.py:507] global step 834: loss = 0.3798 (3.096 sec/step)\n",
            "I0619 21:16:16.237726 139649090267008 learning.py:507] global step 835: loss = 0.2854 (3.109 sec/step)\n",
            "I0619 21:16:19.297762 139649090267008 learning.py:507] global step 835: loss = 0.2257 (3.058 sec/step)\n",
            "I0619 21:16:24.280423 139649090267008 learning.py:507] global step 835: loss = 0.3131 (4.980 sec/step)\n",
            "I0619 21:16:26.209144 139646017689344 supervisor.py:1050] Recording summary at step 835.\n",
            "I0619 21:16:27.852797 139649090267008 learning.py:507] global step 835: loss = 0.2366 (3.568 sec/step)\n",
            "I0619 21:16:31.007089 139649090267008 learning.py:507] global step 835: loss = 0.3160 (3.153 sec/step)\n",
            "I0619 21:16:34.241277 139649090267008 learning.py:507] global step 835: loss = 0.2810 (3.232 sec/step)\n",
            "I0619 21:16:37.343327 139649090267008 learning.py:507] global step 835: loss = 0.2689 (3.100 sec/step)\n",
            "I0619 21:16:40.382792 139649090267008 learning.py:507] global step 835: loss = 0.2157 (3.038 sec/step)\n",
            "I0619 21:16:43.512040 139649090267008 learning.py:507] global step 836: loss = 0.3259 (3.127 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:16:46.595997 139649090267008 learning.py:507] global step 836: loss = 0.3479 (3.082 sec/step)\n",
            "I0619 21:16:49.633370 139649090267008 learning.py:507] global step 836: loss = 0.2588 (3.036 sec/step)\n",
            "I0619 21:16:52.818188 139649090267008 learning.py:507] global step 836: loss = 0.2668 (3.183 sec/step)\n",
            "I0619 21:16:55.922851 139649090267008 learning.py:507] global step 836: loss = 0.3011 (3.103 sec/step)\n",
            "I0619 21:16:58.949824 139649090267008 learning.py:507] global step 836: loss = 0.2337 (3.025 sec/step)\n",
            "I0619 21:17:01.993443 139649090267008 learning.py:507] global step 836: loss = 0.2501 (3.042 sec/step)\n",
            "I0619 21:17:04.994188 139649090267008 learning.py:507] global step 836: loss = 0.3159 (2.999 sec/step)\n",
            "I0619 21:17:08.076183 139649090267008 learning.py:507] global step 837: loss = 0.2669 (3.080 sec/step)\n",
            "I0619 21:17:11.156948 139649090267008 learning.py:507] global step 837: loss = 0.3264 (3.079 sec/step)\n",
            "I0619 21:17:14.254040 139649090267008 learning.py:507] global step 837: loss = 0.2799 (3.095 sec/step)\n",
            "I0619 21:17:17.358734 139649090267008 learning.py:507] global step 837: loss = 0.2290 (3.103 sec/step)\n",
            "I0619 21:17:20.481370 139649090267008 learning.py:507] global step 837: loss = 0.2253 (3.121 sec/step)\n",
            "I0619 21:17:23.494654 139649090267008 learning.py:507] global step 837: loss = 0.3330 (3.011 sec/step)\n",
            "I0619 21:17:26.567177 139649090267008 learning.py:507] global step 837: loss = 0.2873 (3.071 sec/step)\n",
            "I0619 21:17:29.614374 139649090267008 learning.py:507] global step 837: loss = 0.4118 (3.045 sec/step)\n",
            "I0619 21:17:32.672263 139649090267008 learning.py:507] global step 838: loss = 0.2440 (3.055 sec/step)\n",
            "I0619 21:17:35.722355 139649090267008 learning.py:507] global step 838: loss = 0.2682 (3.045 sec/step)\n",
            "I0619 21:17:38.809657 139649090267008 learning.py:507] global step 838: loss = 0.2612 (3.086 sec/step)\n",
            "I0619 21:17:41.912343 139649090267008 learning.py:507] global step 838: loss = 0.3288 (3.101 sec/step)\n",
            "I0619 21:17:44.995864 139649090267008 learning.py:507] global step 838: loss = 0.2458 (3.082 sec/step)\n",
            "I0619 21:17:48.068302 139649090267008 learning.py:507] global step 838: loss = 0.2222 (3.071 sec/step)\n",
            "I0619 21:17:51.196070 139649090267008 learning.py:507] global step 838: loss = 0.4093 (3.126 sec/step)\n",
            "I0619 21:17:54.245604 139649090267008 learning.py:507] global step 838: loss = 0.2531 (3.048 sec/step)\n",
            "I0619 21:17:57.331721 139649090267008 learning.py:507] global step 839: loss = 0.2545 (3.084 sec/step)\n",
            "I0619 21:18:00.388042 139649090267008 learning.py:507] global step 839: loss = 0.2337 (3.054 sec/step)\n",
            "I0619 21:18:03.463162 139649090267008 learning.py:507] global step 839: loss = 0.2623 (3.073 sec/step)\n",
            "I0619 21:18:06.560698 139649090267008 learning.py:507] global step 839: loss = 0.2936 (3.096 sec/step)\n",
            "I0619 21:18:09.735308 139649090267008 learning.py:507] global step 839: loss = 0.2595 (3.173 sec/step)\n",
            "I0619 21:18:12.818374 139649090267008 learning.py:507] global step 839: loss = 0.2594 (3.081 sec/step)\n",
            "I0619 21:18:15.873186 139649090267008 learning.py:507] global step 839: loss = 0.3485 (3.053 sec/step)\n",
            "I0619 21:18:19.002815 139649090267008 learning.py:507] global step 839: loss = 0.2266 (3.128 sec/step)\n",
            "I0619 21:18:23.552211 139649090267008 learning.py:507] global step 840: loss = 0.2166 (4.530 sec/step)\n",
            "I0619 21:18:26.521837 139646017689344 supervisor.py:1050] Recording summary at step 840.\n",
            "I0619 21:18:27.623313 139649090267008 learning.py:507] global step 840: loss = 0.2947 (4.069 sec/step)\n",
            "I0619 21:18:30.699723 139649090267008 learning.py:507] global step 840: loss = 0.2227 (3.074 sec/step)\n",
            "I0619 21:18:33.874007 139649090267008 learning.py:507] global step 840: loss = 0.2749 (3.173 sec/step)\n",
            "I0619 21:18:37.000815 139649090267008 learning.py:507] global step 840: loss = 0.2580 (3.125 sec/step)\n",
            "I0619 21:18:40.067456 139649090267008 learning.py:507] global step 840: loss = 0.2233 (3.065 sec/step)\n",
            "I0619 21:18:43.092237 139649090267008 learning.py:507] global step 840: loss = 0.2282 (3.023 sec/step)\n",
            "I0619 21:18:46.127267 139649090267008 learning.py:507] global step 840: loss = 0.2683 (3.033 sec/step)\n",
            "I0619 21:18:49.146317 139649090267008 learning.py:507] global step 841: loss = 0.2242 (3.017 sec/step)\n",
            "I0619 21:18:52.303723 139649090267008 learning.py:507] global step 841: loss = 0.2385 (3.156 sec/step)\n",
            "I0619 21:18:55.374319 139649090267008 learning.py:507] global step 841: loss = 0.3345 (3.069 sec/step)\n",
            "I0619 21:18:58.446272 139649090267008 learning.py:507] global step 841: loss = 0.2892 (3.070 sec/step)\n",
            "I0619 21:19:01.617325 139649090267008 learning.py:507] global step 841: loss = 0.2741 (3.169 sec/step)\n",
            "I0619 21:19:04.692695 139649090267008 learning.py:507] global step 841: loss = 0.3042 (3.074 sec/step)\n",
            "I0619 21:19:07.807785 139649090267008 learning.py:507] global step 841: loss = 0.2304 (3.113 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:19:11.038223 139649090267008 learning.py:507] global step 841: loss = 0.2462 (3.228 sec/step)\n",
            "I0619 21:19:14.165783 139649090267008 learning.py:507] global step 842: loss = 0.2812 (3.125 sec/step)\n",
            "I0619 21:19:17.612234 139649090267008 learning.py:507] global step 842: loss = 0.2733 (3.445 sec/step)\n",
            "I0619 21:19:20.689037 139649090267008 learning.py:507] global step 842: loss = 0.2256 (3.075 sec/step)\n",
            "I0619 21:19:23.777860 139649090267008 learning.py:507] global step 842: loss = 0.2842 (3.087 sec/step)\n",
            "I0619 21:19:26.881797 139649090267008 learning.py:507] global step 842: loss = 0.2403 (3.102 sec/step)\n",
            "I0619 21:19:29.978909 139649090267008 learning.py:507] global step 842: loss = 0.2566 (3.095 sec/step)\n",
            "I0619 21:19:33.061442 139649090267008 learning.py:507] global step 842: loss = 0.2944 (3.080 sec/step)\n",
            "I0619 21:19:36.662865 139649090267008 learning.py:507] global step 842: loss = 0.2546 (3.600 sec/step)\n",
            "I0619 21:19:39.806998 139649090267008 learning.py:507] global step 843: loss = 0.2764 (3.142 sec/step)\n",
            "I0619 21:19:42.929711 139649090267008 learning.py:507] global step 843: loss = 0.2497 (3.121 sec/step)\n",
            "I0619 21:19:46.044629 139649090267008 learning.py:507] global step 843: loss = 0.2454 (3.113 sec/step)\n",
            "I0619 21:19:49.172471 139649090267008 learning.py:507] global step 843: loss = 0.2452 (3.126 sec/step)\n",
            "I0619 21:19:52.273950 139649090267008 learning.py:507] global step 843: loss = 0.2765 (3.100 sec/step)\n",
            "I0619 21:19:55.369282 139649090267008 learning.py:507] global step 843: loss = 0.2764 (3.093 sec/step)\n",
            "I0619 21:19:58.502350 139649090267008 learning.py:507] global step 843: loss = 0.2432 (3.131 sec/step)\n",
            "I0619 21:20:01.589410 139649090267008 learning.py:507] global step 843: loss = 0.2313 (3.085 sec/step)\n",
            "I0619 21:20:04.635003 139649090267008 learning.py:507] global step 844: loss = 0.2830 (3.043 sec/step)\n",
            "I0619 21:20:07.697659 139649090267008 learning.py:507] global step 844: loss = 0.4121 (3.061 sec/step)\n",
            "I0619 21:20:10.776860 139649090267008 learning.py:507] global step 844: loss = 0.2779 (3.077 sec/step)\n",
            "I0619 21:20:13.917397 139649090267008 learning.py:507] global step 844: loss = 0.2543 (3.139 sec/step)\n",
            "I0619 21:20:16.984769 139649090267008 learning.py:507] global step 844: loss = 0.3690 (3.066 sec/step)\n",
            "I0619 21:20:19.998716 139649090267008 learning.py:507] global step 844: loss = 0.2270 (3.012 sec/step)\n",
            "I0619 21:20:20.931286 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:20:26.199754 139649090267008 learning.py:507] global step 844: loss = 0.2267 (6.199 sec/step)\n",
            "I0619 21:20:28.187119 139646017689344 supervisor.py:1050] Recording summary at step 844.\n",
            "I0619 21:20:30.055103 139649090267008 learning.py:507] global step 844: loss = 0.2489 (3.853 sec/step)\n",
            "I0619 21:20:33.114355 139649090267008 learning.py:507] global step 845: loss = 0.3136 (3.057 sec/step)\n",
            "I0619 21:20:36.270618 139649090267008 learning.py:507] global step 845: loss = 0.2683 (3.154 sec/step)\n",
            "I0619 21:20:39.358557 139649090267008 learning.py:507] global step 845: loss = 0.3689 (3.086 sec/step)\n",
            "I0619 21:20:42.392422 139649090267008 learning.py:507] global step 845: loss = 0.2604 (3.032 sec/step)\n",
            "I0619 21:20:45.509810 139649090267008 learning.py:507] global step 845: loss = 0.2256 (3.115 sec/step)\n",
            "I0619 21:20:48.628588 139649090267008 learning.py:507] global step 845: loss = 0.2861 (3.117 sec/step)\n",
            "I0619 21:20:51.673096 139649090267008 learning.py:507] global step 845: loss = 0.3607 (3.043 sec/step)\n",
            "I0619 21:20:54.827357 139649090267008 learning.py:507] global step 845: loss = 0.3029 (3.153 sec/step)\n",
            "I0619 21:20:57.880609 139649090267008 learning.py:507] global step 846: loss = 0.2994 (3.050 sec/step)\n",
            "I0619 21:21:00.895901 139649090267008 learning.py:507] global step 846: loss = 0.2778 (3.013 sec/step)\n",
            "I0619 21:21:03.932834 139649090267008 learning.py:507] global step 846: loss = 0.2430 (3.035 sec/step)\n",
            "I0619 21:21:07.077978 139649090267008 learning.py:507] global step 846: loss = 0.2847 (3.143 sec/step)\n",
            "I0619 21:21:10.272987 139649090267008 learning.py:507] global step 846: loss = 0.2402 (3.193 sec/step)\n",
            "I0619 21:21:13.370605 139649090267008 learning.py:507] global step 846: loss = 0.2492 (3.096 sec/step)\n",
            "I0619 21:21:16.508300 139649090267008 learning.py:507] global step 846: loss = 0.2436 (3.136 sec/step)\n",
            "I0619 21:21:19.586742 139649090267008 learning.py:507] global step 846: loss = 0.3284 (3.076 sec/step)\n",
            "I0619 21:21:22.634565 139649090267008 learning.py:507] global step 847: loss = 0.2971 (3.045 sec/step)\n",
            "I0619 21:21:25.669453 139649090267008 learning.py:507] global step 847: loss = 0.3599 (3.033 sec/step)\n",
            "I0619 21:21:29.015212 139649090267008 learning.py:507] global step 847: loss = 0.2619 (3.344 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:21:32.152539 139649090267008 learning.py:507] global step 847: loss = 0.2846 (3.135 sec/step)\n",
            "I0619 21:21:35.185769 139649090267008 learning.py:507] global step 847: loss = 0.2930 (3.031 sec/step)\n",
            "I0619 21:21:38.386280 139649090267008 learning.py:507] global step 847: loss = 0.2287 (3.199 sec/step)\n",
            "I0619 21:21:41.500944 139649090267008 learning.py:507] global step 847: loss = 0.2520 (3.113 sec/step)\n",
            "I0619 21:21:44.587374 139649090267008 learning.py:507] global step 847: loss = 0.2656 (3.085 sec/step)\n",
            "I0619 21:21:47.697451 139649090267008 learning.py:507] global step 848: loss = 0.3836 (3.108 sec/step)\n",
            "I0619 21:21:50.790147 139649090267008 learning.py:507] global step 848: loss = 0.3006 (3.091 sec/step)\n",
            "I0619 21:21:53.871812 139649090267008 learning.py:507] global step 848: loss = 0.3275 (3.080 sec/step)\n",
            "I0619 21:21:57.106111 139649090267008 learning.py:507] global step 848: loss = 0.3193 (3.232 sec/step)\n",
            "I0619 21:22:00.142225 139649090267008 learning.py:507] global step 848: loss = 0.3097 (3.034 sec/step)\n",
            "I0619 21:22:03.214831 139649090267008 learning.py:507] global step 848: loss = 0.2427 (3.071 sec/step)\n",
            "I0619 21:22:06.327414 139649090267008 learning.py:507] global step 848: loss = 0.3165 (3.111 sec/step)\n",
            "I0619 21:22:09.372743 139649090267008 learning.py:507] global step 848: loss = 0.2561 (3.044 sec/step)\n",
            "I0619 21:22:12.426170 139649090267008 learning.py:507] global step 849: loss = 0.2603 (3.051 sec/step)\n",
            "I0619 21:22:15.456649 139649090267008 learning.py:507] global step 849: loss = 0.2751 (3.029 sec/step)\n",
            "I0619 21:22:18.553028 139649090267008 learning.py:507] global step 849: loss = 0.2679 (3.095 sec/step)\n",
            "I0619 21:22:22.587914 139649090267008 learning.py:507] global step 849: loss = 0.2486 (4.009 sec/step)\n",
            "I0619 21:22:26.150981 139646017689344 supervisor.py:1050] Recording summary at step 849.\n",
            "I0619 21:22:27.133661 139649090267008 learning.py:507] global step 849: loss = 0.2182 (4.535 sec/step)\n",
            "I0619 21:22:30.268765 139649090267008 learning.py:507] global step 849: loss = 0.2118 (3.133 sec/step)\n",
            "I0619 21:22:33.300185 139649090267008 learning.py:507] global step 849: loss = 0.2688 (3.030 sec/step)\n",
            "I0619 21:22:36.388213 139649090267008 learning.py:507] global step 849: loss = 0.2783 (3.086 sec/step)\n",
            "I0619 21:22:39.507580 139649090267008 learning.py:507] global step 850: loss = 0.2644 (3.117 sec/step)\n",
            "I0619 21:22:42.562317 139649090267008 learning.py:507] global step 850: loss = 0.2073 (3.053 sec/step)\n",
            "I0619 21:22:45.666229 139649090267008 learning.py:507] global step 850: loss = 0.2952 (3.102 sec/step)\n",
            "I0619 21:22:48.743755 139649090267008 learning.py:507] global step 850: loss = 0.2678 (3.076 sec/step)\n",
            "I0619 21:22:51.756763 139649090267008 learning.py:507] global step 850: loss = 0.2493 (3.011 sec/step)\n",
            "I0619 21:22:54.840409 139649090267008 learning.py:507] global step 850: loss = 0.2451 (3.082 sec/step)\n",
            "I0619 21:22:57.844144 139649090267008 learning.py:507] global step 850: loss = 0.3625 (3.002 sec/step)\n",
            "I0619 21:23:00.960826 139649090267008 learning.py:507] global step 850: loss = 0.3228 (3.115 sec/step)\n",
            "I0619 21:23:04.125356 139649090267008 learning.py:507] global step 851: loss = 0.3122 (3.162 sec/step)\n",
            "I0619 21:23:07.198866 139649090267008 learning.py:507] global step 851: loss = 0.2926 (3.072 sec/step)\n",
            "I0619 21:23:10.394185 139649090267008 learning.py:507] global step 851: loss = 0.2808 (3.194 sec/step)\n",
            "I0619 21:23:13.482317 139649090267008 learning.py:507] global step 851: loss = 0.3031 (3.086 sec/step)\n",
            "I0619 21:23:16.578013 139649090267008 learning.py:507] global step 851: loss = 0.2333 (3.094 sec/step)\n",
            "I0619 21:23:19.632251 139649090267008 learning.py:507] global step 851: loss = 0.2390 (3.052 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:23:22.706569 139649090267008 learning.py:507] global step 851: loss = 0.2823 (3.072 sec/step)\n",
            "I0619 21:23:25.731097 139649090267008 learning.py:507] global step 851: loss = 0.2406 (3.023 sec/step)\n",
            "I0619 21:23:29.001445 139649090267008 learning.py:507] global step 852: loss = 0.2438 (3.268 sec/step)\n",
            "I0619 21:23:32.088727 139649090267008 learning.py:507] global step 852: loss = 0.2330 (3.086 sec/step)\n",
            "I0619 21:23:35.135790 139649090267008 learning.py:507] global step 852: loss = 0.3646 (3.045 sec/step)\n",
            "I0619 21:23:38.203167 139649090267008 learning.py:507] global step 852: loss = 0.3255 (3.066 sec/step)\n",
            "I0619 21:23:41.279178 139649090267008 learning.py:507] global step 852: loss = 0.2514 (3.074 sec/step)\n",
            "I0619 21:23:44.383008 139649090267008 learning.py:507] global step 852: loss = 0.3477 (3.102 sec/step)\n",
            "I0619 21:23:47.389299 139649090267008 learning.py:507] global step 852: loss = 0.3272 (3.005 sec/step)\n",
            "I0619 21:23:50.432209 139649090267008 learning.py:507] global step 852: loss = 0.2457 (3.041 sec/step)\n",
            "I0619 21:23:53.588372 139649090267008 learning.py:507] global step 853: loss = 0.2298 (3.154 sec/step)\n",
            "I0619 21:23:56.809120 139649090267008 learning.py:507] global step 853: loss = 0.2600 (3.219 sec/step)\n",
            "I0619 21:23:59.960171 139649090267008 learning.py:507] global step 853: loss = 0.2530 (3.149 sec/step)\n",
            "I0619 21:24:03.159380 139649090267008 learning.py:507] global step 853: loss = 0.2954 (3.197 sec/step)\n",
            "I0619 21:24:06.183733 139649090267008 learning.py:507] global step 853: loss = 0.2858 (3.023 sec/step)\n",
            "I0619 21:24:09.251069 139649090267008 learning.py:507] global step 853: loss = 0.3076 (3.065 sec/step)\n",
            "I0619 21:24:12.431779 139649090267008 learning.py:507] global step 853: loss = 0.2677 (3.179 sec/step)\n",
            "I0619 21:24:15.698046 139649090267008 learning.py:507] global step 853: loss = 0.2780 (3.264 sec/step)\n",
            "I0619 21:24:18.906455 139649090267008 learning.py:507] global step 854: loss = 0.2899 (3.206 sec/step)\n",
            "I0619 21:24:23.157151 139649090267008 learning.py:507] global step 854: loss = 0.2111 (4.234 sec/step)\n",
            "I0619 21:24:26.436167 139646017689344 supervisor.py:1050] Recording summary at step 854.\n",
            "I0619 21:24:27.324789 139649090267008 learning.py:507] global step 854: loss = 0.2271 (4.155 sec/step)\n",
            "I0619 21:24:30.426365 139649090267008 learning.py:507] global step 854: loss = 0.3140 (3.100 sec/step)\n",
            "I0619 21:24:33.505482 139649090267008 learning.py:507] global step 854: loss = 0.2427 (3.077 sec/step)\n",
            "I0619 21:24:36.695924 139649090267008 learning.py:507] global step 854: loss = 0.2958 (3.189 sec/step)\n",
            "I0619 21:24:39.807055 139649090267008 learning.py:507] global step 854: loss = 0.2520 (3.109 sec/step)\n",
            "I0619 21:24:42.907347 139649090267008 learning.py:507] global step 854: loss = 0.2014 (3.098 sec/step)\n",
            "I0619 21:24:45.953366 139649090267008 learning.py:507] global step 855: loss = 0.2597 (3.044 sec/step)\n",
            "I0619 21:24:49.136155 139649090267008 learning.py:507] global step 855: loss = 0.3933 (3.181 sec/step)\n",
            "I0619 21:24:52.294749 139649090267008 learning.py:507] global step 855: loss = 0.2019 (3.157 sec/step)\n",
            "I0619 21:24:55.962836 139649090267008 learning.py:507] global step 855: loss = 0.2436 (3.666 sec/step)\n",
            "I0619 21:24:59.395348 139649090267008 learning.py:507] global step 855: loss = 0.3466 (3.431 sec/step)\n",
            "I0619 21:25:02.481559 139649090267008 learning.py:507] global step 855: loss = 0.2579 (3.083 sec/step)\n",
            "I0619 21:25:05.639728 139649090267008 learning.py:507] global step 855: loss = 0.2457 (3.155 sec/step)\n",
            "I0619 21:25:08.719635 139649090267008 learning.py:507] global step 855: loss = 0.2562 (3.077 sec/step)\n",
            "I0619 21:25:11.811348 139649090267008 learning.py:507] global step 856: loss = 0.3063 (3.089 sec/step)\n",
            "I0619 21:25:15.715097 139649090267008 learning.py:507] global step 856: loss = 0.2799 (3.902 sec/step)\n",
            "I0619 21:25:18.967906 139649090267008 learning.py:507] global step 856: loss = 0.3281 (3.251 sec/step)\n",
            "I0619 21:25:22.047780 139649090267008 learning.py:507] global step 856: loss = 0.3122 (3.078 sec/step)\n",
            "I0619 21:25:25.131542 139649090267008 learning.py:507] global step 856: loss = 0.2421 (3.082 sec/step)\n",
            "I0619 21:25:28.154029 139649090267008 learning.py:507] global step 856: loss = 0.2998 (3.021 sec/step)\n",
            "I0619 21:25:31.213783 139649090267008 learning.py:507] global step 856: loss = 0.2627 (3.058 sec/step)\n",
            "I0619 21:25:34.336186 139649090267008 learning.py:507] global step 856: loss = 0.2726 (3.121 sec/step)\n",
            "I0619 21:25:37.429312 139649090267008 learning.py:507] global step 857: loss = 0.2369 (3.091 sec/step)\n",
            "I0619 21:25:40.520282 139649090267008 learning.py:507] global step 857: loss = 0.2320 (3.089 sec/step)\n",
            "I0619 21:25:43.546547 139649090267008 learning.py:507] global step 857: loss = 0.3292 (3.024 sec/step)\n",
            "I0619 21:25:46.652307 139649090267008 learning.py:507] global step 857: loss = 0.2477 (3.104 sec/step)\n",
            "I0619 21:25:49.683378 139649090267008 learning.py:507] global step 857: loss = 0.2165 (3.029 sec/step)\n",
            "I0619 21:25:52.680828 139649090267008 learning.py:507] global step 857: loss = 0.2545 (2.995 sec/step)\n",
            "I0619 21:25:55.841660 139649090267008 learning.py:507] global step 857: loss = 0.2678 (3.159 sec/step)\n",
            "I0619 21:25:58.947906 139649090267008 learning.py:507] global step 857: loss = 0.2551 (3.104 sec/step)\n",
            "I0619 21:26:02.028987 139649090267008 learning.py:507] global step 858: loss = 0.2864 (3.079 sec/step)\n",
            "I0619 21:26:05.119642 139649090267008 learning.py:507] global step 858: loss = 0.2928 (3.089 sec/step)\n",
            "I0619 21:26:08.205872 139649090267008 learning.py:507] global step 858: loss = 0.2383 (3.084 sec/step)\n",
            "I0619 21:26:11.270426 139649090267008 learning.py:507] global step 858: loss = 0.3023 (3.063 sec/step)\n",
            "I0619 21:26:14.511622 139649090267008 learning.py:507] global step 858: loss = 0.3295 (3.239 sec/step)\n",
            "I0619 21:26:17.667532 139649090267008 learning.py:507] global step 858: loss = 0.2449 (3.154 sec/step)\n",
            "I0619 21:26:21.005336 139649090267008 learning.py:507] global step 858: loss = 0.2410 (3.332 sec/step)\n",
            "I0619 21:26:25.930095 139646017689344 supervisor.py:1050] Recording summary at step 858.\n",
            "I0619 21:26:26.500329 139649090267008 learning.py:507] global step 858: loss = 0.3560 (5.493 sec/step)\n",
            "I0619 21:26:29.554689 139649090267008 learning.py:507] global step 859: loss = 0.3400 (3.052 sec/step)\n",
            "I0619 21:26:33.068143 139649090267008 learning.py:507] global step 859: loss = 0.3517 (3.512 sec/step)\n",
            "I0619 21:26:36.229435 139649090267008 learning.py:507] global step 859: loss = 0.3316 (3.159 sec/step)\n",
            "I0619 21:26:39.615065 139649090267008 learning.py:507] global step 859: loss = 0.2461 (3.384 sec/step)\n",
            "I0619 21:26:42.728932 139649090267008 learning.py:507] global step 859: loss = 0.2707 (3.112 sec/step)\n",
            "I0619 21:26:45.780985 139649090267008 learning.py:507] global step 859: loss = 0.3330 (3.050 sec/step)\n",
            "I0619 21:26:48.865256 139649090267008 learning.py:507] global step 859: loss = 0.2795 (3.083 sec/step)\n",
            "I0619 21:26:52.086760 139649090267008 learning.py:507] global step 859: loss = 0.2346 (3.220 sec/step)\n",
            "I0619 21:26:55.282224 139649090267008 learning.py:507] global step 860: loss = 0.2316 (3.193 sec/step)\n",
            "I0619 21:26:58.354180 139649090267008 learning.py:507] global step 860: loss = 0.2463 (3.070 sec/step)\n",
            "I0619 21:27:01.410661 139649090267008 learning.py:507] global step 860: loss = 0.2669 (3.055 sec/step)\n",
            "I0619 21:27:04.511615 139649090267008 learning.py:507] global step 860: loss = 0.2205 (3.098 sec/step)\n",
            "I0619 21:27:07.614300 139649090267008 learning.py:507] global step 860: loss = 0.2375 (3.101 sec/step)\n",
            "I0619 21:27:10.657622 139649090267008 learning.py:507] global step 860: loss = 0.2306 (3.042 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:27:13.714836 139649090267008 learning.py:507] global step 860: loss = 0.3449 (3.055 sec/step)\n",
            "I0619 21:27:16.795384 139649090267008 learning.py:507] global step 860: loss = 0.2918 (3.079 sec/step)\n",
            "I0619 21:27:19.913238 139649090267008 learning.py:507] global step 861: loss = 0.2626 (3.116 sec/step)\n",
            "I0619 21:27:23.022393 139649090267008 learning.py:507] global step 861: loss = 0.3107 (3.107 sec/step)\n",
            "I0619 21:27:26.147032 139649090267008 learning.py:507] global step 861: loss = 0.3174 (3.123 sec/step)\n",
            "I0619 21:27:29.241634 139649090267008 learning.py:507] global step 861: loss = 0.3706 (3.093 sec/step)\n",
            "I0619 21:27:32.282745 139649090267008 learning.py:507] global step 861: loss = 0.2283 (3.039 sec/step)\n",
            "I0619 21:27:35.324307 139649090267008 learning.py:507] global step 861: loss = 0.2869 (3.039 sec/step)\n",
            "I0619 21:27:38.499127 139649090267008 learning.py:507] global step 861: loss = 0.2727 (3.173 sec/step)\n",
            "I0619 21:27:41.583323 139649090267008 learning.py:507] global step 861: loss = 0.2743 (3.082 sec/step)\n",
            "I0619 21:27:44.674544 139649090267008 learning.py:507] global step 862: loss = 0.2700 (3.089 sec/step)\n",
            "I0619 21:27:47.769600 139649090267008 learning.py:507] global step 862: loss = 0.2746 (3.093 sec/step)\n",
            "I0619 21:27:50.863798 139649090267008 learning.py:507] global step 862: loss = 0.2291 (3.088 sec/step)\n",
            "I0619 21:27:53.963058 139649090267008 learning.py:507] global step 862: loss = 0.2592 (3.097 sec/step)\n",
            "I0619 21:27:57.386791 139649090267008 learning.py:507] global step 862: loss = 0.3400 (3.421 sec/step)\n",
            "I0619 21:28:00.579870 139649090267008 learning.py:507] global step 862: loss = 0.2723 (3.191 sec/step)\n",
            "I0619 21:28:03.784928 139649090267008 learning.py:507] global step 862: loss = 0.2587 (3.203 sec/step)\n",
            "I0619 21:28:06.887313 139649090267008 learning.py:507] global step 862: loss = 0.3279 (3.100 sec/step)\n",
            "I0619 21:28:09.939440 139649090267008 learning.py:507] global step 863: loss = 0.2722 (3.050 sec/step)\n",
            "I0619 21:28:13.116503 139649090267008 learning.py:507] global step 863: loss = 0.3314 (3.175 sec/step)\n",
            "I0619 21:28:16.243138 139649090267008 learning.py:507] global step 863: loss = 0.3146 (3.125 sec/step)\n",
            "I0619 21:28:19.387509 139649090267008 learning.py:507] global step 863: loss = 0.2205 (3.143 sec/step)\n",
            "I0619 21:28:24.425821 139649090267008 learning.py:507] global step 863: loss = 0.2449 (5.034 sec/step)\n",
            "I0619 21:28:26.134284 139646017689344 supervisor.py:1050] Recording summary at step 863.\n",
            "I0619 21:28:27.959813 139649090267008 learning.py:507] global step 863: loss = 0.2870 (3.530 sec/step)\n",
            "I0619 21:28:31.190023 139649090267008 learning.py:507] global step 863: loss = 0.2960 (3.229 sec/step)\n",
            "I0619 21:28:34.325981 139649090267008 learning.py:507] global step 863: loss = 0.2727 (3.134 sec/step)\n",
            "I0619 21:28:37.411196 139649090267008 learning.py:507] global step 864: loss = 0.2872 (3.083 sec/step)\n",
            "I0619 21:28:40.518346 139649090267008 learning.py:507] global step 864: loss = 0.2340 (3.105 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:28:43.653753 139649090267008 learning.py:507] global step 864: loss = 0.5597 (3.134 sec/step)\n",
            "I0619 21:28:46.723780 139649090267008 learning.py:507] global step 864: loss = 0.2904 (3.068 sec/step)\n",
            "I0619 21:28:49.802115 139649090267008 learning.py:507] global step 864: loss = 0.2835 (3.076 sec/step)\n",
            "I0619 21:28:52.886891 139649090267008 learning.py:507] global step 864: loss = 0.3090 (3.083 sec/step)\n",
            "I0619 21:28:55.966850 139649090267008 learning.py:507] global step 864: loss = 0.2498 (3.078 sec/step)\n",
            "I0619 21:28:59.068375 139649090267008 learning.py:507] global step 864: loss = 0.2977 (3.099 sec/step)\n",
            "I0619 21:29:02.149857 139649090267008 learning.py:507] global step 865: loss = 0.2434 (3.079 sec/step)\n",
            "I0619 21:29:05.222345 139649090267008 learning.py:507] global step 865: loss = 0.2364 (3.071 sec/step)\n",
            "I0619 21:29:08.297395 139649090267008 learning.py:507] global step 865: loss = 0.3006 (3.073 sec/step)\n",
            "I0619 21:29:11.398300 139649090267008 learning.py:507] global step 865: loss = 0.3191 (3.099 sec/step)\n",
            "I0619 21:29:14.523505 139649090267008 learning.py:507] global step 865: loss = 0.2585 (3.123 sec/step)\n",
            "I0619 21:29:17.623828 139649090267008 learning.py:507] global step 865: loss = 0.3103 (3.099 sec/step)\n",
            "I0619 21:29:20.762664 139649090267008 learning.py:507] global step 865: loss = 0.2568 (3.137 sec/step)\n",
            "I0619 21:29:23.922420 139649090267008 learning.py:507] global step 865: loss = 0.2269 (3.158 sec/step)\n",
            "I0619 21:29:27.053414 139649090267008 learning.py:507] global step 866: loss = 0.3008 (3.129 sec/step)\n",
            "I0619 21:29:30.159290 139649090267008 learning.py:507] global step 866: loss = 0.2473 (3.104 sec/step)\n",
            "I0619 21:29:33.242472 139649090267008 learning.py:507] global step 866: loss = 0.2951 (3.081 sec/step)\n",
            "I0619 21:29:36.358406 139649090267008 learning.py:507] global step 866: loss = 0.2451 (3.114 sec/step)\n",
            "I0619 21:29:39.472165 139649090267008 learning.py:507] global step 866: loss = 0.2330 (3.112 sec/step)\n",
            "I0619 21:29:42.664314 139649090267008 learning.py:507] global step 866: loss = 0.2347 (3.190 sec/step)\n",
            "I0619 21:29:45.808594 139649090267008 learning.py:507] global step 866: loss = 0.2100 (3.142 sec/step)\n",
            "I0619 21:29:48.815147 139649090267008 learning.py:507] global step 866: loss = 0.2466 (3.005 sec/step)\n",
            "I0619 21:29:51.905567 139649090267008 learning.py:507] global step 867: loss = 0.2087 (3.088 sec/step)\n",
            "I0619 21:29:54.999162 139649090267008 learning.py:507] global step 867: loss = 0.2417 (3.091 sec/step)\n",
            "I0619 21:29:58.116544 139649090267008 learning.py:507] global step 867: loss = 0.3614 (3.116 sec/step)\n",
            "I0619 21:30:01.504290 139649090267008 learning.py:507] global step 867: loss = 0.3109 (3.386 sec/step)\n",
            "I0619 21:30:04.708553 139649090267008 learning.py:507] global step 867: loss = 0.2419 (3.202 sec/step)\n",
            "I0619 21:30:07.799206 139649090267008 learning.py:507] global step 867: loss = 0.2365 (3.089 sec/step)\n",
            "I0619 21:30:10.919979 139649090267008 learning.py:507] global step 867: loss = 0.2832 (3.119 sec/step)\n",
            "I0619 21:30:13.949752 139649090267008 learning.py:507] global step 867: loss = 0.3246 (3.028 sec/step)\n",
            "I0619 21:30:17.123844 139649090267008 learning.py:507] global step 868: loss = 0.2550 (3.172 sec/step)\n",
            "I0619 21:30:20.249755 139649090267008 learning.py:507] global step 868: loss = 0.2501 (3.124 sec/step)\n",
            "I0619 21:30:20.933866 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:30:27.121121 139646017689344 supervisor.py:1050] Recording summary at step 868.\n",
            "I0619 21:30:27.121489 139649090267008 learning.py:507] global step 868: loss = 0.2474 (6.658 sec/step)\n",
            "I0619 21:30:30.171327 139649090267008 learning.py:507] global step 868: loss = 0.2482 (3.041 sec/step)\n",
            "I0619 21:30:33.284726 139649090267008 learning.py:507] global step 868: loss = 0.2521 (3.112 sec/step)\n",
            "I0619 21:30:36.411190 139649090267008 learning.py:507] global step 868: loss = 0.2678 (3.124 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:30:39.513438 139649090267008 learning.py:507] global step 868: loss = 0.2988 (3.100 sec/step)\n",
            "I0619 21:30:42.557458 139649090267008 learning.py:507] global step 868: loss = 0.2491 (3.042 sec/step)\n",
            "I0619 21:30:45.623742 139649090267008 learning.py:507] global step 869: loss = 0.2649 (3.064 sec/step)\n",
            "I0619 21:30:48.622224 139649090267008 learning.py:507] global step 869: loss = 0.2375 (2.996 sec/step)\n",
            "I0619 21:30:51.663577 139649090267008 learning.py:507] global step 869: loss = 0.2938 (3.038 sec/step)\n",
            "I0619 21:30:54.691083 139649090267008 learning.py:507] global step 869: loss = 0.3173 (3.025 sec/step)\n",
            "I0619 21:30:57.772211 139649090267008 learning.py:507] global step 869: loss = 0.2747 (3.079 sec/step)\n",
            "I0619 21:31:00.857151 139649090267008 learning.py:507] global step 869: loss = 0.2516 (3.083 sec/step)\n",
            "I0619 21:31:03.962757 139649090267008 learning.py:507] global step 869: loss = 0.2658 (3.104 sec/step)\n",
            "I0619 21:31:07.085931 139649090267008 learning.py:507] global step 869: loss = 0.2297 (3.121 sec/step)\n",
            "I0619 21:31:10.150842 139649090267008 learning.py:507] global step 870: loss = 0.2556 (3.063 sec/step)\n",
            "I0619 21:31:13.227876 139649090267008 learning.py:507] global step 870: loss = 0.2949 (3.075 sec/step)\n",
            "I0619 21:31:16.312185 139649090267008 learning.py:507] global step 870: loss = 0.2285 (3.083 sec/step)\n",
            "I0619 21:31:19.398003 139649090267008 learning.py:507] global step 870: loss = 0.2153 (3.084 sec/step)\n",
            "I0619 21:31:22.488078 139649090267008 learning.py:507] global step 870: loss = 0.2650 (3.088 sec/step)\n",
            "I0619 21:31:25.569211 139649090267008 learning.py:507] global step 870: loss = 0.3172 (3.079 sec/step)\n",
            "I0619 21:31:28.623471 139649090267008 learning.py:507] global step 870: loss = 0.2708 (3.052 sec/step)\n",
            "I0619 21:31:31.708089 139649090267008 learning.py:507] global step 870: loss = 0.2605 (3.083 sec/step)\n",
            "I0619 21:31:34.742720 139649090267008 learning.py:507] global step 871: loss = 0.2430 (3.032 sec/step)\n",
            "I0619 21:31:37.901391 139649090267008 learning.py:507] global step 871: loss = 0.3037 (3.157 sec/step)\n",
            "I0619 21:31:40.993186 139649090267008 learning.py:507] global step 871: loss = 0.2668 (3.090 sec/step)\n",
            "I0619 21:31:44.047147 139649090267008 learning.py:507] global step 871: loss = 0.2899 (3.052 sec/step)\n",
            "I0619 21:31:47.254438 139649090267008 learning.py:507] global step 871: loss = 0.2579 (3.206 sec/step)\n",
            "I0619 21:31:50.317984 139649090267008 learning.py:507] global step 871: loss = 0.2735 (3.062 sec/step)\n",
            "I0619 21:31:53.360342 139649090267008 learning.py:507] global step 871: loss = 0.2505 (3.041 sec/step)\n",
            "I0619 21:31:56.399342 139649090267008 learning.py:507] global step 871: loss = 0.2248 (3.037 sec/step)\n",
            "I0619 21:31:59.562059 139649090267008 learning.py:507] global step 872: loss = 0.2278 (3.160 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:32:02.679565 139649090267008 learning.py:507] global step 872: loss = 0.3414 (3.116 sec/step)\n",
            "I0619 21:32:06.020583 139649090267008 learning.py:507] global step 872: loss = 0.2173 (3.339 sec/step)\n",
            "I0619 21:32:09.126243 139649090267008 learning.py:507] global step 872: loss = 0.3008 (3.104 sec/step)\n",
            "I0619 21:32:12.187639 139649090267008 learning.py:507] global step 872: loss = 0.2471 (3.060 sec/step)\n",
            "I0619 21:32:15.240644 139649090267008 learning.py:507] global step 872: loss = 0.2477 (3.051 sec/step)\n",
            "I0619 21:32:18.317637 139649090267008 learning.py:507] global step 872: loss = 0.2564 (3.075 sec/step)\n",
            "I0619 21:32:21.379398 139649090267008 learning.py:507] global step 872: loss = 0.2690 (3.060 sec/step)\n",
            "I0619 21:32:26.770105 139649090267008 learning.py:507] global step 873: loss = 0.2296 (5.383 sec/step)\n",
            "I0619 21:32:26.905038 139646017689344 supervisor.py:1050] Recording summary at step 873.\n",
            "I0619 21:32:29.950259 139649090267008 learning.py:507] global step 873: loss = 0.2680 (3.177 sec/step)\n",
            "I0619 21:32:33.528233 139649090267008 learning.py:507] global step 873: loss = 0.2697 (3.576 sec/step)\n",
            "I0619 21:32:36.593694 139649090267008 learning.py:507] global step 873: loss = 0.3007 (3.064 sec/step)\n",
            "I0619 21:32:39.678345 139649090267008 learning.py:507] global step 873: loss = 0.2728 (3.083 sec/step)\n",
            "I0619 21:32:42.677872 139649090267008 learning.py:507] global step 873: loss = 0.2142 (2.998 sec/step)\n",
            "I0619 21:32:45.737890 139649090267008 learning.py:507] global step 873: loss = 0.2368 (3.058 sec/step)\n",
            "I0619 21:32:48.926597 139649090267008 learning.py:507] global step 873: loss = 0.3088 (3.187 sec/step)\n",
            "I0619 21:32:52.591441 139649090267008 learning.py:507] global step 874: loss = 0.2574 (3.663 sec/step)\n",
            "I0619 21:32:55.593845 139649090267008 learning.py:507] global step 874: loss = 0.2339 (3.001 sec/step)\n",
            "I0619 21:32:58.671980 139649090267008 learning.py:507] global step 874: loss = 0.2360 (3.076 sec/step)\n",
            "I0619 21:33:01.754472 139649090267008 learning.py:507] global step 874: loss = 0.3730 (3.081 sec/step)\n",
            "I0619 21:33:04.802493 139649090267008 learning.py:507] global step 874: loss = 0.2605 (3.046 sec/step)\n",
            "I0619 21:33:08.009438 139649090267008 learning.py:507] global step 874: loss = 0.3026 (3.205 sec/step)\n",
            "I0619 21:33:11.004338 139649090267008 learning.py:507] global step 874: loss = 0.2560 (2.993 sec/step)\n",
            "I0619 21:33:14.160866 139649090267008 learning.py:507] global step 874: loss = 0.2767 (3.155 sec/step)\n",
            "I0619 21:33:17.243690 139649090267008 learning.py:507] global step 875: loss = 0.2479 (3.080 sec/step)\n",
            "I0619 21:33:20.313389 139649090267008 learning.py:507] global step 875: loss = 0.2878 (3.068 sec/step)\n",
            "I0619 21:33:23.377802 139649090267008 learning.py:507] global step 875: loss = 0.2204 (3.062 sec/step)\n",
            "I0619 21:33:26.417263 139649090267008 learning.py:507] global step 875: loss = 0.2736 (3.038 sec/step)\n",
            "I0619 21:33:29.723898 139649090267008 learning.py:507] global step 875: loss = 0.2285 (3.305 sec/step)\n",
            "I0619 21:33:32.821801 139649090267008 learning.py:507] global step 875: loss = 0.2567 (3.096 sec/step)\n",
            "I0619 21:33:35.861053 139649090267008 learning.py:507] global step 875: loss = 0.2688 (3.037 sec/step)\n",
            "I0619 21:33:38.897541 139649090267008 learning.py:507] global step 875: loss = 0.2352 (3.035 sec/step)\n",
            "I0619 21:33:41.957167 139649090267008 learning.py:507] global step 876: loss = 0.2886 (3.058 sec/step)\n",
            "I0619 21:33:45.139492 139649090267008 learning.py:507] global step 876: loss = 0.2765 (3.180 sec/step)\n",
            "I0619 21:33:48.487178 139649090267008 learning.py:507] global step 876: loss = 0.3741 (3.346 sec/step)\n",
            "I0619 21:33:51.522141 139649090267008 learning.py:507] global step 876: loss = 0.2724 (3.033 sec/step)\n",
            "I0619 21:33:54.719610 139649090267008 learning.py:507] global step 876: loss = 0.2327 (3.196 sec/step)\n",
            "I0619 21:33:57.760043 139649090267008 learning.py:507] global step 876: loss = 0.2273 (3.039 sec/step)\n",
            "I0619 21:34:00.849105 139649090267008 learning.py:507] global step 876: loss = 0.3351 (3.087 sec/step)\n",
            "I0619 21:34:03.990507 139649090267008 learning.py:507] global step 876: loss = 0.2375 (3.140 sec/step)\n",
            "I0619 21:34:07.093042 139649090267008 learning.py:507] global step 877: loss = 0.2512 (3.100 sec/step)\n",
            "I0619 21:34:10.380025 139649090267008 learning.py:507] global step 877: loss = 0.2636 (3.285 sec/step)\n",
            "I0619 21:34:13.440612 139649090267008 learning.py:507] global step 877: loss = 0.2957 (3.059 sec/step)\n",
            "I0619 21:34:16.582046 139649090267008 learning.py:507] global step 877: loss = 0.2865 (3.139 sec/step)\n",
            "I0619 21:34:19.723756 139649090267008 learning.py:507] global step 877: loss = 0.2356 (3.140 sec/step)\n",
            "I0619 21:34:24.922151 139649090267008 learning.py:507] global step 877: loss = 0.3054 (5.190 sec/step)\n",
            "I0619 21:34:26.444895 139646017689344 supervisor.py:1050] Recording summary at step 877.\n",
            "I0619 21:34:28.665297 139649090267008 learning.py:507] global step 877: loss = 0.3502 (3.737 sec/step)\n",
            "I0619 21:34:31.855681 139649090267008 learning.py:507] global step 877: loss = 0.2477 (3.188 sec/step)\n",
            "I0619 21:34:34.957746 139649090267008 learning.py:507] global step 878: loss = 0.2441 (3.099 sec/step)\n",
            "I0619 21:34:38.095276 139649090267008 learning.py:507] global step 878: loss = 0.2593 (3.136 sec/step)\n",
            "I0619 21:34:41.179098 139649090267008 learning.py:507] global step 878: loss = 0.2432 (3.082 sec/step)\n",
            "I0619 21:34:44.222896 139649090267008 learning.py:507] global step 878: loss = 0.2622 (3.042 sec/step)\n",
            "I0619 21:34:47.331987 139649090267008 learning.py:507] global step 878: loss = 0.2847 (3.107 sec/step)\n",
            "I0619 21:34:50.418078 139649090267008 learning.py:507] global step 878: loss = 0.3299 (3.084 sec/step)\n",
            "I0619 21:34:53.514841 139649090267008 learning.py:507] global step 878: loss = 0.2208 (3.095 sec/step)\n",
            "I0619 21:34:56.616301 139649090267008 learning.py:507] global step 878: loss = 0.2649 (3.100 sec/step)\n",
            "I0619 21:34:59.681265 139649090267008 learning.py:507] global step 879: loss = 0.2602 (3.063 sec/step)\n",
            "I0619 21:35:02.732572 139649090267008 learning.py:507] global step 879: loss = 0.2690 (3.050 sec/step)\n",
            "I0619 21:35:05.863323 139649090267008 learning.py:507] global step 879: loss = 0.2985 (3.129 sec/step)\n",
            "I0619 21:35:08.945976 139649090267008 learning.py:507] global step 879: loss = 0.2812 (3.081 sec/step)\n",
            "I0619 21:35:12.103504 139649090267008 learning.py:507] global step 879: loss = 0.2564 (3.156 sec/step)\n",
            "I0619 21:35:15.279285 139649090267008 learning.py:507] global step 879: loss = 0.2424 (3.174 sec/step)\n",
            "I0619 21:35:18.399879 139649090267008 learning.py:507] global step 879: loss = 0.3045 (3.119 sec/step)\n",
            "I0619 21:35:21.498326 139649090267008 learning.py:507] global step 879: loss = 0.2725 (3.097 sec/step)\n",
            "I0619 21:35:24.746479 139649090267008 learning.py:507] global step 880: loss = 0.3092 (3.246 sec/step)\n",
            "I0619 21:35:27.875299 139649090267008 learning.py:507] global step 880: loss = 0.2620 (3.127 sec/step)\n",
            "I0619 21:35:31.010507 139649090267008 learning.py:507] global step 880: loss = 0.2217 (3.133 sec/step)\n",
            "I0619 21:35:34.063544 139649090267008 learning.py:507] global step 880: loss = 0.2689 (3.051 sec/step)\n",
            "I0619 21:35:37.170606 139649090267008 learning.py:507] global step 880: loss = 0.2925 (3.105 sec/step)\n",
            "I0619 21:35:40.304197 139649090267008 learning.py:507] global step 880: loss = 0.2473 (3.132 sec/step)\n",
            "I0619 21:35:43.602753 139649090267008 learning.py:507] global step 880: loss = 0.2781 (3.297 sec/step)\n",
            "I0619 21:35:46.728353 139649090267008 learning.py:507] global step 880: loss = 0.4495 (3.124 sec/step)\n",
            "I0619 21:35:49.810124 139649090267008 learning.py:507] global step 881: loss = 0.1915 (3.079 sec/step)\n",
            "I0619 21:35:52.896837 139649090267008 learning.py:507] global step 881: loss = 0.2727 (3.085 sec/step)\n",
            "I0619 21:35:55.996568 139649090267008 learning.py:507] global step 881: loss = 0.2592 (3.098 sec/step)\n",
            "I0619 21:35:59.098317 139649090267008 learning.py:507] global step 881: loss = 0.3053 (3.100 sec/step)\n",
            "I0619 21:36:02.201442 139649090267008 learning.py:507] global step 881: loss = 0.2542 (3.101 sec/step)\n",
            "I0619 21:36:05.321403 139649090267008 learning.py:507] global step 881: loss = 0.2349 (3.118 sec/step)\n",
            "I0619 21:36:08.443671 139649090267008 learning.py:507] global step 881: loss = 0.2768 (3.120 sec/step)\n",
            "I0619 21:36:11.540286 139649090267008 learning.py:507] global step 881: loss = 0.2710 (3.095 sec/step)\n",
            "I0619 21:36:14.645400 139649090267008 learning.py:507] global step 882: loss = 0.3085 (3.103 sec/step)\n",
            "I0619 21:36:17.760377 139649090267008 learning.py:507] global step 882: loss = 0.2502 (3.113 sec/step)\n",
            "I0619 21:36:20.807544 139649090267008 learning.py:507] global step 882: loss = 0.2451 (3.045 sec/step)\n",
            "I0619 21:36:26.149245 139649090267008 learning.py:507] global step 882: loss = 0.2234 (5.337 sec/step)\n",
            "I0619 21:36:26.707034 139646017689344 supervisor.py:1050] Recording summary at step 882.\n",
            "I0619 21:36:29.288095 139649090267008 learning.py:507] global step 882: loss = 0.2960 (3.137 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:36:32.407578 139649090267008 learning.py:507] global step 882: loss = 0.2934 (3.118 sec/step)\n",
            "I0619 21:36:35.459679 139649090267008 learning.py:507] global step 882: loss = 0.2666 (3.050 sec/step)\n",
            "I0619 21:36:38.555503 139649090267008 learning.py:507] global step 882: loss = 0.1929 (3.094 sec/step)\n",
            "I0619 21:36:41.610785 139649090267008 learning.py:507] global step 883: loss = 0.3015 (3.053 sec/step)\n",
            "I0619 21:36:44.681795 139649090267008 learning.py:507] global step 883: loss = 0.2279 (3.069 sec/step)\n",
            "I0619 21:36:47.783167 139649090267008 learning.py:507] global step 883: loss = 0.2401 (3.099 sec/step)\n",
            "I0619 21:36:50.880129 139649090267008 learning.py:507] global step 883: loss = 0.2353 (3.095 sec/step)\n",
            "I0619 21:36:53.950199 139649090267008 learning.py:507] global step 883: loss = 0.2364 (3.068 sec/step)\n",
            "I0619 21:36:57.017164 139649090267008 learning.py:507] global step 883: loss = 0.2738 (3.065 sec/step)\n",
            "I0619 21:37:00.116925 139649090267008 learning.py:507] global step 883: loss = 0.2676 (3.098 sec/step)\n",
            "I0619 21:37:03.243847 139649090267008 learning.py:507] global step 883: loss = 0.4704 (3.125 sec/step)\n",
            "I0619 21:37:06.446007 139649090267008 learning.py:507] global step 884: loss = 0.3091 (3.199 sec/step)\n",
            "I0619 21:37:09.511636 139649090267008 learning.py:507] global step 884: loss = 0.3908 (3.064 sec/step)\n",
            "I0619 21:37:12.605389 139649090267008 learning.py:507] global step 884: loss = 0.3352 (3.092 sec/step)\n",
            "I0619 21:37:15.645956 139649090267008 learning.py:507] global step 884: loss = 0.2552 (3.039 sec/step)\n",
            "I0619 21:37:18.777481 139649090267008 learning.py:507] global step 884: loss = 0.2615 (3.130 sec/step)\n",
            "I0619 21:37:21.868618 139649090267008 learning.py:507] global step 884: loss = 0.2297 (3.089 sec/step)\n",
            "I0619 21:37:25.221783 139649090267008 learning.py:507] global step 884: loss = 0.3290 (3.351 sec/step)\n",
            "I0619 21:37:28.283654 139649090267008 learning.py:507] global step 884: loss = 0.2416 (3.060 sec/step)\n",
            "I0619 21:37:31.327576 139649090267008 learning.py:507] global step 885: loss = 0.2568 (3.041 sec/step)\n",
            "I0619 21:37:34.444787 139649090267008 learning.py:507] global step 885: loss = 0.3068 (3.115 sec/step)\n",
            "I0619 21:37:37.496223 139649090267008 learning.py:507] global step 885: loss = 0.3097 (3.050 sec/step)\n",
            "I0619 21:37:40.629471 139649090267008 learning.py:507] global step 885: loss = 0.2707 (3.132 sec/step)\n",
            "I0619 21:37:43.714240 139649090267008 learning.py:507] global step 885: loss = 0.2096 (3.083 sec/step)\n",
            "I0619 21:37:46.724206 139649090267008 learning.py:507] global step 885: loss = 0.2554 (3.008 sec/step)\n",
            "I0619 21:37:49.994635 139649090267008 learning.py:507] global step 885: loss = 0.2507 (3.269 sec/step)\n",
            "I0619 21:37:53.052778 139649090267008 learning.py:507] global step 885: loss = 0.2156 (3.057 sec/step)\n",
            "I0619 21:37:56.124072 139649090267008 learning.py:507] global step 886: loss = 0.2260 (3.069 sec/step)\n",
            "I0619 21:37:59.185035 139649090267008 learning.py:507] global step 886: loss = 0.3148 (3.059 sec/step)\n",
            "I0619 21:38:02.306044 139649090267008 learning.py:507] global step 886: loss = 0.2322 (3.119 sec/step)\n",
            "I0619 21:38:05.392126 139649090267008 learning.py:507] global step 886: loss = 0.2704 (3.084 sec/step)\n",
            "I0619 21:38:08.736828 139649090267008 learning.py:507] global step 886: loss = 0.2107 (3.343 sec/step)\n",
            "I0619 21:38:11.889880 139649090267008 learning.py:507] global step 886: loss = 0.3182 (3.151 sec/step)\n",
            "I0619 21:38:14.914013 139649090267008 learning.py:507] global step 886: loss = 0.2613 (3.022 sec/step)\n",
            "I0619 21:38:18.015925 139649090267008 learning.py:507] global step 886: loss = 0.2636 (3.100 sec/step)\n",
            "I0619 21:38:21.097206 139649090267008 learning.py:507] global step 887: loss = 0.2565 (3.078 sec/step)\n",
            "I0619 21:38:26.482254 139649090267008 learning.py:507] global step 887: loss = 0.3196 (5.383 sec/step)\n",
            "I0619 21:38:26.908158 139646017689344 supervisor.py:1050] Recording summary at step 887.\n",
            "I0619 21:38:29.621353 139649090267008 learning.py:507] global step 887: loss = 0.2582 (3.134 sec/step)\n",
            "I0619 21:38:32.709987 139649090267008 learning.py:507] global step 887: loss = 0.3902 (3.087 sec/step)\n",
            "I0619 21:38:35.832430 139649090267008 learning.py:507] global step 887: loss = 0.3199 (3.121 sec/step)\n",
            "I0619 21:38:38.961579 139649090267008 learning.py:507] global step 887: loss = 0.2796 (3.127 sec/step)\n",
            "I0619 21:38:42.020345 139649090267008 learning.py:507] global step 887: loss = 0.3066 (3.057 sec/step)\n",
            "I0619 21:38:45.095567 139649090267008 learning.py:507] global step 887: loss = 0.2652 (3.073 sec/step)\n",
            "I0619 21:38:48.175691 139649090267008 learning.py:507] global step 888: loss = 0.2572 (3.078 sec/step)\n",
            "I0619 21:38:51.386677 139649090267008 learning.py:507] global step 888: loss = 0.2388 (3.209 sec/step)\n",
            "I0619 21:38:54.496675 139649090267008 learning.py:507] global step 888: loss = 0.2344 (3.108 sec/step)\n",
            "I0619 21:38:57.567466 139649090267008 learning.py:507] global step 888: loss = 0.2294 (3.069 sec/step)\n",
            "I0619 21:39:00.646173 139649090267008 learning.py:507] global step 888: loss = 0.2750 (3.076 sec/step)\n",
            "I0619 21:39:03.696165 139649090267008 learning.py:507] global step 888: loss = 0.2543 (3.048 sec/step)\n",
            "I0619 21:39:06.771696 139649090267008 learning.py:507] global step 888: loss = 0.2141 (3.074 sec/step)\n",
            "I0619 21:39:10.054797 139649090267008 learning.py:507] global step 888: loss = 0.2441 (3.281 sec/step)\n",
            "I0619 21:39:13.110549 139649090267008 learning.py:507] global step 889: loss = 0.3107 (3.054 sec/step)\n",
            "I0619 21:39:16.194826 139649090267008 learning.py:507] global step 889: loss = 0.2386 (3.082 sec/step)\n",
            "I0619 21:39:19.276311 139649090267008 learning.py:507] global step 889: loss = 0.2942 (3.080 sec/step)\n",
            "I0619 21:39:22.291269 139649090267008 learning.py:507] global step 889: loss = 0.3502 (3.013 sec/step)\n",
            "I0619 21:39:25.476639 139649090267008 learning.py:507] global step 889: loss = 0.2273 (3.184 sec/step)\n",
            "I0619 21:39:28.492873 139649090267008 learning.py:507] global step 889: loss = 0.3003 (3.015 sec/step)\n",
            "I0619 21:39:31.577885 139649090267008 learning.py:507] global step 889: loss = 0.2597 (3.083 sec/step)\n",
            "I0619 21:39:34.662045 139649090267008 learning.py:507] global step 889: loss = 0.2397 (3.082 sec/step)\n",
            "I0619 21:39:37.730436 139649090267008 learning.py:507] global step 890: loss = 0.2759 (3.066 sec/step)\n",
            "I0619 21:39:40.774775 139649090267008 learning.py:507] global step 890: loss = 0.2756 (3.042 sec/step)\n",
            "I0619 21:39:44.052709 139649090267008 learning.py:507] global step 890: loss = 0.2218 (3.276 sec/step)\n",
            "I0619 21:39:47.162650 139649090267008 learning.py:507] global step 890: loss = 0.2516 (3.108 sec/step)\n",
            "I0619 21:39:50.206759 139649090267008 learning.py:507] global step 890: loss = 0.2957 (3.042 sec/step)\n",
            "I0619 21:39:53.276086 139649090267008 learning.py:507] global step 890: loss = 0.2464 (3.067 sec/step)\n",
            "I0619 21:39:56.308769 139649090267008 learning.py:507] global step 890: loss = 0.2393 (3.031 sec/step)\n",
            "I0619 21:39:59.446314 139649090267008 learning.py:507] global step 890: loss = 0.2767 (3.136 sec/step)\n",
            "I0619 21:40:02.527120 139649090267008 learning.py:507] global step 891: loss = 0.2593 (3.079 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:40:05.576590 139649090267008 learning.py:507] global step 891: loss = 0.2442 (3.047 sec/step)\n",
            "I0619 21:40:08.689668 139649090267008 learning.py:507] global step 891: loss = 0.2142 (3.111 sec/step)\n",
            "I0619 21:40:11.705071 139649090267008 learning.py:507] global step 891: loss = 0.2218 (3.013 sec/step)\n",
            "I0619 21:40:14.807279 139649090267008 learning.py:507] global step 891: loss = 0.2653 (3.101 sec/step)\n",
            "I0619 21:40:17.893712 139649090267008 learning.py:507] global step 891: loss = 0.2676 (3.085 sec/step)\n",
            "I0619 21:40:20.931594 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:40:21.023144 139649090267008 learning.py:507] global step 891: loss = 0.3131 (3.109 sec/step)\n",
            "I0619 21:40:27.370627 139646017689344 supervisor.py:1050] Recording summary at step 891.\n",
            "I0619 21:40:28.116958 139649090267008 learning.py:507] global step 891: loss = 0.2472 (6.806 sec/step)\n",
            "I0619 21:40:31.304956 139649090267008 learning.py:507] global step 892: loss = 0.2293 (3.185 sec/step)\n",
            "I0619 21:40:34.356161 139649090267008 learning.py:507] global step 892: loss = 0.2406 (3.050 sec/step)\n",
            "I0619 21:40:37.464370 139649090267008 learning.py:507] global step 892: loss = 0.3102 (3.106 sec/step)\n",
            "I0619 21:40:40.594314 139649090267008 learning.py:507] global step 892: loss = 0.2413 (3.128 sec/step)\n",
            "I0619 21:40:43.619057 139649090267008 learning.py:507] global step 892: loss = 0.2271 (3.022 sec/step)\n",
            "I0619 21:40:46.757683 139649090267008 learning.py:507] global step 892: loss = 0.2289 (3.137 sec/step)\n",
            "I0619 21:40:49.818455 139649090267008 learning.py:507] global step 892: loss = 0.2368 (3.059 sec/step)\n",
            "I0619 21:40:52.893146 139649090267008 learning.py:507] global step 892: loss = 0.2680 (3.073 sec/step)\n",
            "I0619 21:40:55.976830 139649090267008 learning.py:507] global step 893: loss = 0.3394 (3.081 sec/step)\n",
            "I0619 21:40:59.002408 139649090267008 learning.py:507] global step 893: loss = 0.2109 (3.024 sec/step)\n",
            "I0619 21:41:02.125093 139649090267008 learning.py:507] global step 893: loss = 0.2684 (3.121 sec/step)\n",
            "I0619 21:41:05.404211 139649090267008 learning.py:507] global step 893: loss = 0.2442 (3.277 sec/step)\n",
            "I0619 21:41:08.530320 139649090267008 learning.py:507] global step 893: loss = 0.2263 (3.124 sec/step)\n",
            "I0619 21:41:11.619244 139649090267008 learning.py:507] global step 893: loss = 0.2037 (3.087 sec/step)\n",
            "I0619 21:41:14.690407 139649090267008 learning.py:507] global step 893: loss = 0.2266 (3.069 sec/step)\n",
            "I0619 21:41:17.790198 139649090267008 learning.py:507] global step 893: loss = 0.2732 (3.098 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:41:20.918497 139649090267008 learning.py:507] global step 894: loss = 0.2267 (3.126 sec/step)\n",
            "I0619 21:41:24.002500 139649090267008 learning.py:507] global step 894: loss = 0.4162 (3.082 sec/step)\n",
            "I0619 21:41:27.107249 139649090267008 learning.py:507] global step 894: loss = 0.2396 (3.103 sec/step)\n",
            "I0619 21:41:30.142810 139649090267008 learning.py:507] global step 894: loss = 0.2872 (3.034 sec/step)\n",
            "I0619 21:41:33.181922 139649090267008 learning.py:507] global step 894: loss = 0.2358 (3.037 sec/step)\n",
            "I0619 21:41:36.533003 139649090267008 learning.py:507] global step 894: loss = 0.3472 (3.349 sec/step)\n",
            "I0619 21:41:39.565642 139649090267008 learning.py:507] global step 894: loss = 0.2253 (3.031 sec/step)\n",
            "I0619 21:41:42.640367 139649090267008 learning.py:507] global step 894: loss = 0.2341 (3.073 sec/step)\n",
            "I0619 21:41:45.698221 139649090267008 learning.py:507] global step 895: loss = 0.2826 (3.056 sec/step)\n",
            "I0619 21:41:48.769557 139649090267008 learning.py:507] global step 895: loss = 0.2613 (3.070 sec/step)\n",
            "I0619 21:41:51.782512 139649090267008 learning.py:507] global step 895: loss = 0.2551 (3.011 sec/step)\n",
            "I0619 21:41:55.263876 139649090267008 learning.py:507] global step 895: loss = 0.2949 (3.480 sec/step)\n",
            "I0619 21:41:58.300077 139649090267008 learning.py:507] global step 895: loss = 0.3012 (3.034 sec/step)\n",
            "I0619 21:42:01.384542 139649090267008 learning.py:507] global step 895: loss = 0.3050 (3.083 sec/step)\n",
            "I0619 21:42:04.425809 139649090267008 learning.py:507] global step 895: loss = 0.2803 (3.040 sec/step)\n",
            "I0619 21:42:07.600056 139649090267008 learning.py:507] global step 895: loss = 0.2698 (3.173 sec/step)\n",
            "I0619 21:42:10.865791 139649090267008 learning.py:507] global step 896: loss = 0.2106 (3.263 sec/step)\n",
            "I0619 21:42:14.065810 139649090267008 learning.py:507] global step 896: loss = 0.2530 (3.198 sec/step)\n",
            "I0619 21:42:17.205736 139649090267008 learning.py:507] global step 896: loss = 0.2684 (3.138 sec/step)\n",
            "I0619 21:42:20.231375 139649090267008 learning.py:507] global step 896: loss = 0.2251 (3.024 sec/step)\n",
            "I0619 21:42:25.547295 139649090267008 learning.py:507] global step 896: loss = 0.2324 (5.304 sec/step)\n",
            "I0619 21:42:26.608950 139646017689344 supervisor.py:1050] Recording summary at step 896.\n",
            "I0619 21:42:28.917644 139649090267008 learning.py:507] global step 896: loss = 0.2351 (3.368 sec/step)\n",
            "I0619 21:42:31.918380 139649090267008 learning.py:507] global step 896: loss = 0.1959 (2.999 sec/step)\n",
            "I0619 21:42:35.106804 139649090267008 learning.py:507] global step 896: loss = 0.2577 (3.186 sec/step)\n",
            "I0619 21:42:38.159770 139649090267008 learning.py:507] global step 897: loss = 0.3634 (3.049 sec/step)\n",
            "I0619 21:42:41.259879 139649090267008 learning.py:507] global step 897: loss = 0.2657 (3.098 sec/step)\n",
            "I0619 21:42:44.436196 139649090267008 learning.py:507] global step 897: loss = 0.2810 (3.174 sec/step)\n",
            "I0619 21:42:47.546183 139649090267008 learning.py:507] global step 897: loss = 0.3512 (3.108 sec/step)\n",
            "I0619 21:42:50.686417 139649090267008 learning.py:507] global step 897: loss = 0.2766 (3.138 sec/step)\n",
            "I0619 21:42:53.733870 139649090267008 learning.py:507] global step 897: loss = 0.3497 (3.045 sec/step)\n",
            "I0619 21:42:56.786599 139649090267008 learning.py:507] global step 897: loss = 0.2457 (3.050 sec/step)\n",
            "I0619 21:42:59.873634 139649090267008 learning.py:507] global step 897: loss = 0.2585 (3.085 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:43:02.961951 139649090267008 learning.py:507] global step 898: loss = 0.2647 (3.086 sec/step)\n",
            "I0619 21:43:06.097382 139649090267008 learning.py:507] global step 898: loss = 0.3225 (3.134 sec/step)\n",
            "I0619 21:43:09.235295 139649090267008 learning.py:507] global step 898: loss = 0.2316 (3.136 sec/step)\n",
            "I0619 21:43:12.285953 139649090267008 learning.py:507] global step 898: loss = 0.2710 (3.049 sec/step)\n",
            "I0619 21:43:15.329117 139649090267008 learning.py:507] global step 898: loss = 0.2874 (3.042 sec/step)\n",
            "I0619 21:43:18.396410 139649090267008 learning.py:507] global step 898: loss = 0.2550 (3.066 sec/step)\n",
            "I0619 21:43:21.547307 139649090267008 learning.py:507] global step 898: loss = 0.2850 (3.149 sec/step)\n",
            "I0619 21:43:24.705725 139649090267008 learning.py:507] global step 898: loss = 0.2862 (3.157 sec/step)\n",
            "I0619 21:43:27.780636 139649090267008 learning.py:507] global step 899: loss = 0.3062 (3.072 sec/step)\n",
            "I0619 21:43:30.810265 139649090267008 learning.py:507] global step 899: loss = 0.2294 (3.027 sec/step)\n",
            "I0619 21:43:33.874541 139649090267008 learning.py:507] global step 899: loss = 0.2309 (3.062 sec/step)\n",
            "I0619 21:43:37.249834 139649090267008 learning.py:507] global step 899: loss = 0.2403 (3.373 sec/step)\n",
            "I0619 21:43:40.390606 139649090267008 learning.py:507] global step 899: loss = 0.2597 (3.139 sec/step)\n",
            "I0619 21:43:43.559994 139649090267008 learning.py:507] global step 899: loss = 0.2441 (3.167 sec/step)\n",
            "I0619 21:43:46.746681 139649090267008 learning.py:507] global step 899: loss = 0.2776 (3.185 sec/step)\n",
            "I0619 21:43:49.897533 139649090267008 learning.py:507] global step 899: loss = 0.2103 (3.149 sec/step)\n",
            "I0619 21:43:53.031269 139649090267008 learning.py:507] global step 900: loss = 0.2594 (3.132 sec/step)\n",
            "I0619 21:43:56.441876 139649090267008 learning.py:507] global step 900: loss = 0.2167 (3.409 sec/step)\n",
            "I0619 21:43:59.644370 139649090267008 learning.py:507] global step 900: loss = 0.1829 (3.201 sec/step)\n",
            "I0619 21:44:02.953199 139649090267008 learning.py:507] global step 900: loss = 0.2193 (3.307 sec/step)\n",
            "I0619 21:44:06.167127 139649090267008 learning.py:507] global step 900: loss = 0.2563 (3.212 sec/step)\n",
            "I0619 21:44:09.417924 139649090267008 learning.py:507] global step 900: loss = 0.2721 (3.248 sec/step)\n",
            "I0619 21:44:12.565946 139649090267008 learning.py:507] global step 900: loss = 0.2304 (3.146 sec/step)\n",
            "I0619 21:44:15.713475 139649090267008 learning.py:507] global step 900: loss = 0.2247 (3.146 sec/step)\n",
            "I0619 21:44:18.761708 139649090267008 learning.py:507] global step 901: loss = 0.2698 (3.045 sec/step)\n",
            "I0619 21:44:23.078890 139649090267008 learning.py:507] global step 901: loss = 0.2964 (4.300 sec/step)\n",
            "I0619 21:44:26.187274 139646017689344 supervisor.py:1050] Recording summary at step 901.\n",
            "I0619 21:44:27.257194 139649090267008 learning.py:507] global step 901: loss = 0.2857 (4.176 sec/step)\n",
            "I0619 21:44:30.374135 139649090267008 learning.py:507] global step 901: loss = 0.2942 (3.115 sec/step)\n",
            "I0619 21:44:33.443408 139649090267008 learning.py:507] global step 901: loss = 0.2288 (3.068 sec/step)\n",
            "I0619 21:44:36.535866 139649090267008 learning.py:507] global step 901: loss = 0.2376 (3.091 sec/step)\n",
            "I0619 21:44:39.679495 139649090267008 learning.py:507] global step 901: loss = 0.2490 (3.142 sec/step)\n",
            "I0619 21:44:42.737574 139649090267008 learning.py:507] global step 901: loss = 0.2475 (3.056 sec/step)\n",
            "I0619 21:44:45.825251 139649090267008 learning.py:507] global step 902: loss = 0.2510 (3.086 sec/step)\n",
            "I0619 21:44:48.951096 139649090267008 learning.py:507] global step 902: loss = 0.2154 (3.124 sec/step)\n",
            "I0619 21:44:52.050075 139649090267008 learning.py:507] global step 902: loss = 0.2235 (3.097 sec/step)\n",
            "I0619 21:44:55.165224 139649090267008 learning.py:507] global step 902: loss = 0.3394 (3.113 sec/step)\n",
            "I0619 21:44:58.263489 139649090267008 learning.py:507] global step 902: loss = 0.2615 (3.097 sec/step)\n",
            "I0619 21:45:01.347714 139649090267008 learning.py:507] global step 902: loss = 0.2320 (3.082 sec/step)\n",
            "I0619 21:45:04.504168 139649090267008 learning.py:507] global step 902: loss = 0.3102 (3.155 sec/step)\n",
            "I0619 21:45:07.582122 139649090267008 learning.py:507] global step 902: loss = 0.2284 (3.076 sec/step)\n",
            "I0619 21:45:10.696134 139649090267008 learning.py:507] global step 903: loss = 0.2144 (3.112 sec/step)\n",
            "I0619 21:45:13.733054 139649090267008 learning.py:507] global step 903: loss = 0.2440 (3.035 sec/step)\n",
            "I0619 21:45:16.834353 139649090267008 learning.py:507] global step 903: loss = 0.2685 (3.100 sec/step)\n",
            "I0619 21:45:19.960169 139649090267008 learning.py:507] global step 903: loss = 0.3944 (3.124 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:45:23.185235 139649090267008 learning.py:507] global step 903: loss = 0.2689 (3.223 sec/step)\n",
            "I0619 21:45:26.265322 139649090267008 learning.py:507] global step 903: loss = 0.2402 (3.078 sec/step)\n",
            "I0619 21:45:29.424497 139649090267008 learning.py:507] global step 903: loss = 0.3586 (3.157 sec/step)\n",
            "I0619 21:45:32.600782 139649090267008 learning.py:507] global step 903: loss = 0.2903 (3.174 sec/step)\n",
            "I0619 21:45:35.723427 139649090267008 learning.py:507] global step 904: loss = 0.2156 (3.120 sec/step)\n",
            "I0619 21:45:38.888174 139649090267008 learning.py:507] global step 904: loss = 0.3577 (3.163 sec/step)\n",
            "I0619 21:45:42.072757 139649090267008 learning.py:507] global step 904: loss = 0.2843 (3.183 sec/step)\n",
            "I0619 21:45:45.178265 139649090267008 learning.py:507] global step 904: loss = 0.2590 (3.104 sec/step)\n",
            "I0619 21:45:48.248441 139649090267008 learning.py:507] global step 904: loss = 0.2878 (3.068 sec/step)\n",
            "I0619 21:45:51.401840 139649090267008 learning.py:507] global step 904: loss = 0.2797 (3.151 sec/step)\n",
            "I0619 21:45:54.501528 139649090267008 learning.py:507] global step 904: loss = 0.2613 (3.098 sec/step)\n",
            "I0619 21:45:57.709133 139649090267008 learning.py:507] global step 904: loss = 0.2733 (3.205 sec/step)\n",
            "I0619 21:46:00.877620 139649090267008 learning.py:507] global step 905: loss = 0.3052 (3.166 sec/step)\n",
            "I0619 21:46:04.219411 139649090267008 learning.py:507] global step 905: loss = 0.2397 (3.340 sec/step)\n",
            "I0619 21:46:07.286745 139649090267008 learning.py:507] global step 905: loss = 0.2318 (3.065 sec/step)\n",
            "I0619 21:46:10.484160 139649090267008 learning.py:507] global step 905: loss = 0.2387 (3.195 sec/step)\n",
            "I0619 21:46:13.715179 139649090267008 learning.py:507] global step 905: loss = 0.2926 (3.229 sec/step)\n",
            "I0619 21:46:16.761432 139649090267008 learning.py:507] global step 905: loss = 0.2562 (3.044 sec/step)\n",
            "I0619 21:46:19.848452 139649090267008 learning.py:507] global step 905: loss = 0.2492 (3.085 sec/step)\n",
            "I0619 21:46:25.006280 139649090267008 learning.py:507] global step 905: loss = 0.2843 (5.146 sec/step)\n",
            "I0619 21:46:26.314610 139646017689344 supervisor.py:1050] Recording summary at step 905.\n",
            "I0619 21:46:28.430206 139649090267008 learning.py:507] global step 906: loss = 0.2296 (3.419 sec/step)\n",
            "I0619 21:46:31.502729 139649090267008 learning.py:507] global step 906: loss = 0.3985 (3.070 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:46:34.580828 139649090267008 learning.py:507] global step 906: loss = 0.2973 (3.076 sec/step)\n",
            "I0619 21:46:37.623055 139649090267008 learning.py:507] global step 906: loss = 0.1844 (3.040 sec/step)\n",
            "I0619 21:46:40.713023 139649090267008 learning.py:507] global step 906: loss = 0.2882 (3.088 sec/step)\n",
            "I0619 21:46:43.796799 139649090267008 learning.py:507] global step 906: loss = 0.4051 (3.082 sec/step)\n",
            "I0619 21:46:47.042441 139649090267008 learning.py:507] global step 906: loss = 0.3315 (3.244 sec/step)\n",
            "I0619 21:46:50.191847 139649090267008 learning.py:507] global step 906: loss = 0.2233 (3.148 sec/step)\n",
            "I0619 21:46:53.243273 139649090267008 learning.py:507] global step 907: loss = 0.2966 (3.049 sec/step)\n",
            "I0619 21:46:56.279738 139649090267008 learning.py:507] global step 907: loss = 0.3277 (3.035 sec/step)\n",
            "I0619 21:46:59.322030 139649090267008 learning.py:507] global step 907: loss = 0.2090 (3.040 sec/step)\n",
            "I0619 21:47:02.382924 139649090267008 learning.py:507] global step 907: loss = 0.2480 (3.059 sec/step)\n",
            "I0619 21:47:05.506619 139649090267008 learning.py:507] global step 907: loss = 0.2458 (3.122 sec/step)\n",
            "I0619 21:47:08.602773 139649090267008 learning.py:507] global step 907: loss = 0.2302 (3.094 sec/step)\n",
            "I0619 21:47:11.694182 139649090267008 learning.py:507] global step 907: loss = 0.2647 (3.087 sec/step)\n",
            "I0619 21:47:14.747905 139649090267008 learning.py:507] global step 907: loss = 0.2319 (3.051 sec/step)\n",
            "I0619 21:47:17.833905 139649090267008 learning.py:507] global step 908: loss = 0.3552 (3.084 sec/step)\n",
            "I0619 21:47:20.948833 139649090267008 learning.py:507] global step 908: loss = 0.2140 (3.113 sec/step)\n",
            "I0619 21:47:24.110932 139649090267008 learning.py:507] global step 908: loss = 0.2923 (3.160 sec/step)\n",
            "I0619 21:47:27.168130 139649090267008 learning.py:507] global step 908: loss = 0.2605 (3.055 sec/step)\n",
            "I0619 21:47:30.268917 139649090267008 learning.py:507] global step 908: loss = 0.3203 (3.099 sec/step)\n",
            "I0619 21:47:33.373771 139649090267008 learning.py:507] global step 908: loss = 0.2315 (3.103 sec/step)\n",
            "I0619 21:47:36.428508 139649090267008 learning.py:507] global step 908: loss = 0.2222 (3.053 sec/step)\n",
            "I0619 21:47:39.529719 139649090267008 learning.py:507] global step 908: loss = 0.2473 (3.099 sec/step)\n",
            "I0619 21:47:42.573281 139649090267008 learning.py:507] global step 909: loss = 0.3276 (3.042 sec/step)\n",
            "I0619 21:47:45.606323 139649090267008 learning.py:507] global step 909: loss = 0.2623 (3.031 sec/step)\n",
            "I0619 21:47:48.746951 139649090267008 learning.py:507] global step 909: loss = 0.2450 (3.139 sec/step)\n",
            "I0619 21:47:51.828441 139649090267008 learning.py:507] global step 909: loss = 0.2529 (3.080 sec/step)\n",
            "I0619 21:47:55.087778 139649090267008 learning.py:507] global step 909: loss = 0.2184 (3.258 sec/step)\n",
            "I0619 21:47:58.159557 139649090267008 learning.py:507] global step 909: loss = 0.3392 (3.070 sec/step)\n",
            "I0619 21:48:01.233820 139649090267008 learning.py:507] global step 909: loss = 0.3179 (3.072 sec/step)\n",
            "I0619 21:48:04.384109 139649090267008 learning.py:507] global step 909: loss = 0.2559 (3.148 sec/step)\n",
            "I0619 21:48:07.538440 139649090267008 learning.py:507] global step 910: loss = 0.2448 (3.152 sec/step)\n",
            "I0619 21:48:10.653585 139649090267008 learning.py:507] global step 910: loss = 0.2339 (3.113 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:48:14.041624 139649090267008 learning.py:507] global step 910: loss = 0.2672 (3.386 sec/step)\n",
            "I0619 21:48:17.159177 139649090267008 learning.py:507] global step 910: loss = 0.2379 (3.116 sec/step)\n",
            "I0619 21:48:20.369471 139649090267008 learning.py:507] global step 910: loss = 0.3320 (3.208 sec/step)\n",
            "I0619 21:48:25.620088 139649090267008 learning.py:507] global step 910: loss = 0.2534 (5.243 sec/step)\n",
            "I0619 21:48:26.551725 139646017689344 supervisor.py:1050] Recording summary at step 910.\n",
            "I0619 21:48:28.844216 139649090267008 learning.py:507] global step 910: loss = 0.2293 (3.222 sec/step)\n",
            "I0619 21:48:32.882728 139649090267008 learning.py:507] global step 910: loss = 0.4259 (4.037 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:48:35.968922 139649090267008 learning.py:507] global step 911: loss = 0.3293 (3.084 sec/step)\n",
            "I0619 21:48:39.206928 139649090267008 learning.py:507] global step 911: loss = 0.2939 (3.236 sec/step)\n",
            "I0619 21:48:42.251566 139649090267008 learning.py:507] global step 911: loss = 0.2574 (3.043 sec/step)\n",
            "I0619 21:48:45.284629 139649090267008 learning.py:507] global step 911: loss = 0.2664 (3.031 sec/step)\n",
            "I0619 21:48:48.328918 139649090267008 learning.py:507] global step 911: loss = 0.2321 (3.041 sec/step)\n",
            "I0619 21:48:52.096123 139649090267008 learning.py:507] global step 911: loss = 0.2505 (3.765 sec/step)\n",
            "I0619 21:48:55.262602 139649090267008 learning.py:507] global step 911: loss = 0.2231 (3.165 sec/step)\n",
            "I0619 21:48:58.326331 139649090267008 learning.py:507] global step 911: loss = 0.3348 (3.062 sec/step)\n",
            "I0619 21:49:01.429882 139649090267008 learning.py:507] global step 912: loss = 0.3218 (3.101 sec/step)\n",
            "I0619 21:49:04.507150 139649090267008 learning.py:507] global step 912: loss = 0.2671 (3.076 sec/step)\n",
            "I0619 21:49:07.632691 139649090267008 learning.py:507] global step 912: loss = 0.1971 (3.124 sec/step)\n",
            "I0619 21:49:10.740426 139649090267008 learning.py:507] global step 912: loss = 0.2281 (3.106 sec/step)\n",
            "I0619 21:49:13.820594 139649090267008 learning.py:507] global step 912: loss = 0.2422 (3.078 sec/step)\n",
            "I0619 21:49:16.913450 139649090267008 learning.py:507] global step 912: loss = 0.2597 (3.091 sec/step)\n",
            "I0619 21:49:19.997035 139649090267008 learning.py:507] global step 912: loss = 0.2569 (3.082 sec/step)\n",
            "I0619 21:49:23.120105 139649090267008 learning.py:507] global step 912: loss = 0.2458 (3.121 sec/step)\n",
            "I0619 21:49:26.258568 139649090267008 learning.py:507] global step 913: loss = 0.2751 (3.136 sec/step)\n",
            "I0619 21:49:29.335948 139649090267008 learning.py:507] global step 913: loss = 0.2459 (3.075 sec/step)\n",
            "I0619 21:49:32.441492 139649090267008 learning.py:507] global step 913: loss = 0.4386 (3.104 sec/step)\n",
            "I0619 21:49:35.600037 139649090267008 learning.py:507] global step 913: loss = 0.2471 (3.157 sec/step)\n",
            "I0619 21:49:38.650901 139649090267008 learning.py:507] global step 913: loss = 0.3270 (3.049 sec/step)\n",
            "I0619 21:49:41.804748 139649090267008 learning.py:507] global step 913: loss = 0.3346 (3.152 sec/step)\n",
            "I0619 21:49:44.991650 139649090267008 learning.py:507] global step 913: loss = 0.2255 (3.185 sec/step)\n",
            "I0619 21:49:48.177801 139649090267008 learning.py:507] global step 913: loss = 0.2657 (3.184 sec/step)\n",
            "I0619 21:49:51.245676 139649090267008 learning.py:507] global step 914: loss = 0.2587 (3.066 sec/step)\n",
            "I0619 21:49:54.353012 139649090267008 learning.py:507] global step 914: loss = 0.2165 (3.106 sec/step)\n",
            "I0619 21:49:57.377716 139649090267008 learning.py:507] global step 914: loss = 0.2909 (3.023 sec/step)\n",
            "I0619 21:50:00.447100 139649090267008 learning.py:507] global step 914: loss = 0.2655 (3.067 sec/step)\n",
            "I0619 21:50:03.529310 139649090267008 learning.py:507] global step 914: loss = 0.2553 (3.080 sec/step)\n",
            "I0619 21:50:06.763669 139649090267008 learning.py:507] global step 914: loss = 0.2497 (3.233 sec/step)\n",
            "I0619 21:50:09.791169 139649090267008 learning.py:507] global step 914: loss = 0.2648 (3.025 sec/step)\n",
            "I0619 21:50:12.852513 139649090267008 learning.py:507] global step 914: loss = 0.2444 (3.059 sec/step)\n",
            "I0619 21:50:15.954434 139649090267008 learning.py:507] global step 915: loss = 0.2290 (3.099 sec/step)\n",
            "I0619 21:50:19.006060 139649090267008 learning.py:507] global step 915: loss = 0.2544 (3.050 sec/step)\n",
            "I0619 21:50:20.931299 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 21:50:24.019230 139649090267008 learning.py:507] global step 915: loss = 0.3259 (4.996 sec/step)\n",
            "I0619 21:50:28.291035 139646017689344 supervisor.py:1050] Recording summary at step 915.\n",
            "I0619 21:50:29.374593 139649090267008 learning.py:507] global step 915: loss = 0.2146 (5.343 sec/step)\n",
            "I0619 21:50:32.380556 139649090267008 learning.py:507] global step 915: loss = 0.2952 (3.004 sec/step)\n",
            "I0619 21:50:35.442929 139649090267008 learning.py:507] global step 915: loss = 0.2037 (3.060 sec/step)\n",
            "I0619 21:50:38.476552 139649090267008 learning.py:507] global step 915: loss = 0.2327 (3.031 sec/step)\n",
            "I0619 21:50:41.759438 139649090267008 learning.py:507] global step 915: loss = 0.2493 (3.278 sec/step)\n",
            "I0619 21:50:44.831013 139649090267008 learning.py:507] global step 916: loss = 0.3408 (3.069 sec/step)\n",
            "I0619 21:50:48.040129 139649090267008 learning.py:507] global step 916: loss = 0.4185 (3.207 sec/step)\n",
            "I0619 21:50:51.183783 139649090267008 learning.py:507] global step 916: loss = 0.2014 (3.142 sec/step)\n",
            "I0619 21:50:54.245324 139649090267008 learning.py:507] global step 916: loss = 0.2490 (3.060 sec/step)\n",
            "I0619 21:50:57.344137 139649090267008 learning.py:507] global step 916: loss = 0.2581 (3.097 sec/step)\n",
            "I0619 21:51:00.438040 139649090267008 learning.py:507] global step 916: loss = 0.3013 (3.092 sec/step)\n",
            "I0619 21:51:03.621850 139649090267008 learning.py:507] global step 916: loss = 0.2854 (3.182 sec/step)\n",
            "I0619 21:51:06.740239 139649090267008 learning.py:507] global step 916: loss = 0.2493 (3.117 sec/step)\n",
            "I0619 21:51:09.847622 139649090267008 learning.py:507] global step 917: loss = 0.3072 (3.105 sec/step)\n",
            "I0619 21:51:12.898232 139649090267008 learning.py:507] global step 917: loss = 0.2726 (3.049 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:51:16.016493 139649090267008 learning.py:507] global step 917: loss = 0.3178 (3.116 sec/step)\n",
            "I0619 21:51:19.100453 139649090267008 learning.py:507] global step 917: loss = 0.3246 (3.082 sec/step)\n",
            "I0619 21:51:22.330576 139649090267008 learning.py:507] global step 917: loss = 0.2507 (3.228 sec/step)\n",
            "I0619 21:51:25.539563 139649090267008 learning.py:507] global step 917: loss = 0.2600 (3.207 sec/step)\n",
            "I0619 21:51:28.618225 139649090267008 learning.py:507] global step 917: loss = 0.2782 (3.077 sec/step)\n",
            "I0619 21:51:31.705822 139649090267008 learning.py:507] global step 917: loss = 0.2499 (3.086 sec/step)\n",
            "I0619 21:51:34.762485 139649090267008 learning.py:507] global step 918: loss = 0.4020 (3.054 sec/step)\n",
            "I0619 21:51:37.830948 139649090267008 learning.py:507] global step 918: loss = 0.2119 (3.067 sec/step)\n",
            "I0619 21:51:40.909107 139649090267008 learning.py:507] global step 918: loss = 0.3182 (3.076 sec/step)\n",
            "I0619 21:51:43.996350 139649090267008 learning.py:507] global step 918: loss = 0.2897 (3.085 sec/step)\n",
            "I0619 21:51:47.086775 139649090267008 learning.py:507] global step 918: loss = 0.2702 (3.089 sec/step)\n",
            "I0619 21:51:50.132827 139649090267008 learning.py:507] global step 918: loss = 0.2660 (3.044 sec/step)\n",
            "I0619 21:51:53.492311 139649090267008 learning.py:507] global step 918: loss = 0.2369 (3.357 sec/step)\n",
            "I0619 21:51:56.558989 139649090267008 learning.py:507] global step 918: loss = 0.2741 (3.065 sec/step)\n",
            "I0619 21:51:59.581957 139649090267008 learning.py:507] global step 919: loss = 0.2562 (3.020 sec/step)\n",
            "I0619 21:52:02.669527 139649090267008 learning.py:507] global step 919: loss = 0.3202 (3.086 sec/step)\n",
            "I0619 21:52:05.774898 139649090267008 learning.py:507] global step 919: loss = 0.2778 (3.103 sec/step)\n",
            "I0619 21:52:08.833978 139649090267008 learning.py:507] global step 919: loss = 0.2523 (3.057 sec/step)\n",
            "I0619 21:52:12.299895 139649090267008 learning.py:507] global step 919: loss = 0.2593 (3.462 sec/step)\n",
            "I0619 21:52:15.428722 139649090267008 learning.py:507] global step 919: loss = 0.2466 (3.127 sec/step)\n",
            "I0619 21:52:18.476643 139649090267008 learning.py:507] global step 919: loss = 0.2253 (3.046 sec/step)\n",
            "I0619 21:52:22.227474 139649090267008 learning.py:507] global step 919: loss = 0.2600 (3.736 sec/step)\n",
            "I0619 21:52:26.171620 139646017689344 supervisor.py:1050] Recording summary at step 919.\n",
            "I0619 21:52:27.020815 139649090267008 learning.py:507] global step 920: loss = 0.2277 (4.791 sec/step)\n",
            "I0619 21:52:30.123442 139649090267008 learning.py:507] global step 920: loss = 0.2632 (3.101 sec/step)\n",
            "I0619 21:52:33.167139 139649090267008 learning.py:507] global step 920: loss = 0.2543 (3.042 sec/step)\n",
            "I0619 21:52:36.227507 139649090267008 learning.py:507] global step 920: loss = 0.2220 (3.058 sec/step)\n",
            "I0619 21:52:39.303498 139649090267008 learning.py:507] global step 920: loss = 0.2558 (3.074 sec/step)\n",
            "I0619 21:52:42.307197 139649090267008 learning.py:507] global step 920: loss = 0.2930 (3.002 sec/step)\n",
            "I0619 21:52:45.545762 139649090267008 learning.py:507] global step 920: loss = 0.3139 (3.237 sec/step)\n",
            "I0619 21:52:48.622076 139649090267008 learning.py:507] global step 920: loss = 0.3370 (3.074 sec/step)\n",
            "I0619 21:52:51.675806 139649090267008 learning.py:507] global step 921: loss = 0.2923 (3.051 sec/step)\n",
            "I0619 21:52:54.713439 139649090267008 learning.py:507] global step 921: loss = 0.2743 (3.035 sec/step)\n",
            "I0619 21:52:57.807005 139649090267008 learning.py:507] global step 921: loss = 0.2201 (3.092 sec/step)\n",
            "I0619 21:53:00.833766 139649090267008 learning.py:507] global step 921: loss = 0.3164 (3.024 sec/step)\n",
            "I0619 21:53:04.131134 139649090267008 learning.py:507] global step 921: loss = 0.2674 (3.296 sec/step)\n",
            "I0619 21:53:07.213527 139649090267008 learning.py:507] global step 921: loss = 0.2317 (3.080 sec/step)\n",
            "I0619 21:53:10.313897 139649090267008 learning.py:507] global step 921: loss = 0.2517 (3.098 sec/step)\n",
            "I0619 21:53:13.370399 139649090267008 learning.py:507] global step 921: loss = 0.2409 (3.055 sec/step)\n",
            "I0619 21:53:16.454516 139649090267008 learning.py:507] global step 922: loss = 0.2787 (3.082 sec/step)\n",
            "I0619 21:53:19.639284 139649090267008 learning.py:507] global step 922: loss = 0.2137 (3.183 sec/step)\n",
            "I0619 21:53:22.721933 139649090267008 learning.py:507] global step 922: loss = 0.3025 (3.080 sec/step)\n",
            "I0619 21:53:25.824659 139649090267008 learning.py:507] global step 922: loss = 0.3453 (3.101 sec/step)\n",
            "I0619 21:53:28.969431 139649090267008 learning.py:507] global step 922: loss = 0.2795 (3.143 sec/step)\n",
            "I0619 21:53:32.071270 139649090267008 learning.py:507] global step 922: loss = 0.3483 (3.100 sec/step)\n",
            "I0619 21:53:35.104633 139649090267008 learning.py:507] global step 922: loss = 0.2097 (3.031 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:53:38.370326 139649090267008 learning.py:507] global step 922: loss = 0.2891 (3.264 sec/step)\n",
            "I0619 21:53:41.399588 139649090267008 learning.py:507] global step 923: loss = 0.2713 (3.027 sec/step)\n",
            "I0619 21:53:44.456111 139649090267008 learning.py:507] global step 923: loss = 0.2506 (3.055 sec/step)\n",
            "I0619 21:53:47.500342 139649090267008 learning.py:507] global step 923: loss = 0.2064 (3.043 sec/step)\n",
            "I0619 21:53:50.560671 139649090267008 learning.py:507] global step 923: loss = 0.2253 (3.058 sec/step)\n",
            "I0619 21:53:53.626314 139649090267008 learning.py:507] global step 923: loss = 0.2528 (3.064 sec/step)\n",
            "I0619 21:53:56.759955 139649090267008 learning.py:507] global step 923: loss = 0.2154 (3.132 sec/step)\n",
            "I0619 21:53:59.795446 139649090267008 learning.py:507] global step 923: loss = 0.3488 (3.034 sec/step)\n",
            "I0619 21:54:02.923588 139649090267008 learning.py:507] global step 923: loss = 0.3013 (3.126 sec/step)\n",
            "I0619 21:54:06.003117 139649090267008 learning.py:507] global step 924: loss = 0.3026 (3.077 sec/step)\n",
            "I0619 21:54:09.207895 139649090267008 learning.py:507] global step 924: loss = 0.2243 (3.203 sec/step)\n",
            "I0619 21:54:12.332214 139649090267008 learning.py:507] global step 924: loss = 0.2542 (3.123 sec/step)\n",
            "I0619 21:54:15.434113 139649090267008 learning.py:507] global step 924: loss = 0.2603 (3.100 sec/step)\n",
            "I0619 21:54:18.507612 139649090267008 learning.py:507] global step 924: loss = 0.2483 (3.072 sec/step)\n",
            "I0619 21:54:22.515798 139649090267008 learning.py:507] global step 924: loss = 0.2402 (3.964 sec/step)\n",
            "I0619 21:54:26.369487 139646017689344 supervisor.py:1050] Recording summary at step 924.\n",
            "I0619 21:54:27.212856 139649090267008 learning.py:507] global step 924: loss = 0.4944 (4.695 sec/step)\n",
            "I0619 21:54:30.276100 139649090267008 learning.py:507] global step 924: loss = 0.2560 (3.061 sec/step)\n",
            "I0619 21:54:33.314870 139649090267008 learning.py:507] global step 925: loss = 0.2212 (3.036 sec/step)\n",
            "I0619 21:54:36.472503 139649090267008 learning.py:507] global step 925: loss = 0.2220 (3.156 sec/step)\n",
            "I0619 21:54:39.545418 139649090267008 learning.py:507] global step 925: loss = 0.3073 (3.071 sec/step)\n",
            "I0619 21:54:42.618362 139649090267008 learning.py:507] global step 925: loss = 0.3000 (3.071 sec/step)\n",
            "I0619 21:54:45.751708 139649090267008 learning.py:507] global step 925: loss = 0.3178 (3.128 sec/step)\n",
            "I0619 21:54:48.933634 139649090267008 learning.py:507] global step 925: loss = 0.3346 (3.180 sec/step)\n",
            "I0619 21:54:52.017551 139649090267008 learning.py:507] global step 925: loss = 0.2718 (3.082 sec/step)\n",
            "I0619 21:54:55.205513 139649090267008 learning.py:507] global step 925: loss = 0.2506 (3.186 sec/step)\n",
            "I0619 21:54:58.290370 139649090267008 learning.py:507] global step 926: loss = 0.2329 (3.083 sec/step)\n",
            "I0619 21:55:01.412327 139649090267008 learning.py:507] global step 926: loss = 0.2508 (3.120 sec/step)\n",
            "I0619 21:55:04.434419 139649090267008 learning.py:507] global step 926: loss = 0.3204 (3.020 sec/step)\n",
            "I0619 21:55:07.483113 139649090267008 learning.py:507] global step 926: loss = 0.2599 (3.047 sec/step)\n",
            "I0619 21:55:10.577754 139649090267008 learning.py:507] global step 926: loss = 0.2467 (3.093 sec/step)\n",
            "I0619 21:55:13.700190 139649090267008 learning.py:507] global step 926: loss = 0.2217 (3.121 sec/step)\n",
            "I0619 21:55:16.796656 139649090267008 learning.py:507] global step 926: loss = 0.2188 (3.095 sec/step)\n",
            "I0619 21:55:19.941300 139649090267008 learning.py:507] global step 926: loss = 0.2946 (3.143 sec/step)\n",
            "I0619 21:55:23.038467 139649090267008 learning.py:507] global step 927: loss = 0.2670 (3.095 sec/step)\n",
            "I0619 21:55:26.084329 139649090267008 learning.py:507] global step 927: loss = 0.3013 (3.044 sec/step)\n",
            "I0619 21:55:29.179696 139649090267008 learning.py:507] global step 927: loss = 0.2720 (3.094 sec/step)\n",
            "I0619 21:55:32.346211 139649090267008 learning.py:507] global step 927: loss = 0.3297 (3.165 sec/step)\n",
            "I0619 21:55:35.417683 139649090267008 learning.py:507] global step 927: loss = 0.3243 (3.070 sec/step)\n",
            "I0619 21:55:38.481131 139649090267008 learning.py:507] global step 927: loss = 0.2448 (3.062 sec/step)\n",
            "I0619 21:55:41.565730 139649090267008 learning.py:507] global step 927: loss = 0.2029 (3.083 sec/step)\n",
            "I0619 21:55:44.632822 139649090267008 learning.py:507] global step 927: loss = 0.2505 (3.065 sec/step)\n",
            "I0619 21:55:47.729926 139649090267008 learning.py:507] global step 928: loss = 0.2585 (3.095 sec/step)\n",
            "I0619 21:55:50.926106 139649090267008 learning.py:507] global step 928: loss = 0.3241 (3.194 sec/step)\n",
            "I0619 21:55:54.096547 139649090267008 learning.py:507] global step 928: loss = 0.2558 (3.169 sec/step)\n",
            "I0619 21:55:57.198127 139649090267008 learning.py:507] global step 928: loss = 0.2309 (3.099 sec/step)\n",
            "I0619 21:56:00.333485 139649090267008 learning.py:507] global step 928: loss = 0.4463 (3.133 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 21:56:03.501650 139649090267008 learning.py:507] global step 928: loss = 0.2577 (3.166 sec/step)\n",
            "I0619 21:56:06.622708 139649090267008 learning.py:507] global step 928: loss = 0.3081 (3.119 sec/step)\n",
            "I0619 21:56:09.720648 139649090267008 learning.py:507] global step 928: loss = 0.2606 (3.096 sec/step)\n",
            "I0619 21:56:12.828232 139649090267008 learning.py:507] global step 929: loss = 0.2886 (3.105 sec/step)\n",
            "I0619 21:56:15.942229 139649090267008 learning.py:507] global step 929: loss = 0.2807 (3.112 sec/step)\n",
            "I0619 21:56:19.353655 139649090267008 learning.py:507] global step 929: loss = 0.2396 (3.409 sec/step)\n",
            "I0619 21:56:24.400130 139649090267008 learning.py:507] global step 929: loss = 0.2382 (5.043 sec/step)\n",
            "I0619 21:56:26.323139 139646017689344 supervisor.py:1050] Recording summary at step 929.\n",
            "I0619 21:56:27.964741 139649090267008 learning.py:507] global step 929: loss = 0.2621 (3.560 sec/step)\n",
            "I0619 21:56:31.088764 139649090267008 learning.py:507] global step 929: loss = 0.2778 (3.122 sec/step)\n",
            "I0619 21:56:34.155911 139649090267008 learning.py:507] global step 929: loss = 0.2354 (3.065 sec/step)\n",
            "I0619 21:56:37.558978 139649090267008 learning.py:507] global step 929: loss = 0.2569 (3.401 sec/step)\n",
            "I0619 21:56:40.658070 139649090267008 learning.py:507] global step 930: loss = 0.3702 (3.097 sec/step)\n",
            "I0619 21:56:43.722514 139649090267008 learning.py:507] global step 930: loss = 0.2604 (3.063 sec/step)\n",
            "I0619 21:56:46.759865 139649090267008 learning.py:507] global step 930: loss = 0.2406 (3.036 sec/step)\n",
            "I0619 21:56:49.980541 139649090267008 learning.py:507] global step 930: loss = 0.2233 (3.219 sec/step)\n",
            "I0619 21:56:53.035588 139649090267008 learning.py:507] global step 930: loss = 0.2378 (3.053 sec/step)\n",
            "I0619 21:56:56.077002 139649090267008 learning.py:507] global step 930: loss = 0.2130 (3.040 sec/step)\n",
            "I0619 21:56:59.154094 139649090267008 learning.py:507] global step 930: loss = 0.2485 (3.075 sec/step)\n",
            "I0619 21:57:02.253400 139649090267008 learning.py:507] global step 930: loss = 0.2208 (3.097 sec/step)\n",
            "I0619 21:57:05.313546 139649090267008 learning.py:507] global step 931: loss = 0.2738 (3.057 sec/step)\n",
            "I0619 21:57:08.591913 139649090267008 learning.py:507] global step 931: loss = 0.2910 (3.276 sec/step)\n",
            "I0619 21:57:11.659560 139649090267008 learning.py:507] global step 931: loss = 0.2796 (3.066 sec/step)\n",
            "I0619 21:57:14.731882 139649090267008 learning.py:507] global step 931: loss = 0.2617 (3.071 sec/step)\n",
            "I0619 21:57:17.788800 139649090267008 learning.py:507] global step 931: loss = 0.2418 (3.055 sec/step)\n",
            "I0619 21:57:20.852769 139649090267008 learning.py:507] global step 931: loss = 0.2639 (3.062 sec/step)\n",
            "I0619 21:57:23.943272 139649090267008 learning.py:507] global step 931: loss = 0.2277 (3.089 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:57:27.048527 139649090267008 learning.py:507] global step 931: loss = 0.3238 (3.103 sec/step)\n",
            "I0619 21:57:30.182280 139649090267008 learning.py:507] global step 932: loss = 0.2247 (3.131 sec/step)\n",
            "I0619 21:57:33.273896 139649090267008 learning.py:507] global step 932: loss = 0.2609 (3.090 sec/step)\n",
            "I0619 21:57:36.368211 139649090267008 learning.py:507] global step 932: loss = 0.2937 (3.093 sec/step)\n",
            "I0619 21:57:39.419483 139649090267008 learning.py:507] global step 932: loss = 0.2453 (3.049 sec/step)\n",
            "I0619 21:57:42.493155 139649090267008 learning.py:507] global step 932: loss = 0.2414 (3.072 sec/step)\n",
            "I0619 21:57:45.684320 139649090267008 learning.py:507] global step 932: loss = 0.2365 (3.189 sec/step)\n",
            "I0619 21:57:48.813298 139649090267008 learning.py:507] global step 932: loss = 0.3010 (3.127 sec/step)\n",
            "I0619 21:57:51.858225 139649090267008 learning.py:507] global step 932: loss = 0.2020 (3.043 sec/step)\n",
            "I0619 21:57:55.001150 139649090267008 learning.py:507] global step 933: loss = 0.3584 (3.141 sec/step)\n",
            "I0619 21:57:58.112988 139649090267008 learning.py:507] global step 933: loss = 0.3208 (3.110 sec/step)\n",
            "I0619 21:58:01.183377 139649090267008 learning.py:507] global step 933: loss = 0.2102 (3.068 sec/step)\n",
            "I0619 21:58:04.231736 139649090267008 learning.py:507] global step 933: loss = 0.2403 (3.046 sec/step)\n",
            "I0619 21:58:07.339200 139649090267008 learning.py:507] global step 933: loss = 0.2481 (3.106 sec/step)\n",
            "I0619 21:58:11.038648 139649090267008 learning.py:507] global step 933: loss = 0.2326 (3.698 sec/step)\n",
            "I0619 21:58:14.127143 139649090267008 learning.py:507] global step 933: loss = 0.2436 (3.087 sec/step)\n",
            "I0619 21:58:17.258749 139649090267008 learning.py:507] global step 933: loss = 0.2746 (3.130 sec/step)\n",
            "I0619 21:58:20.356321 139649090267008 learning.py:507] global step 934: loss = 0.2472 (3.095 sec/step)\n",
            "I0619 21:58:25.600529 139649090267008 learning.py:507] global step 934: loss = 0.2239 (5.242 sec/step)\n",
            "I0619 21:58:27.300723 139646017689344 supervisor.py:1050] Recording summary at step 934.\n",
            "I0619 21:58:29.405617 139649090267008 learning.py:507] global step 934: loss = 0.2581 (3.802 sec/step)\n",
            "I0619 21:58:32.499850 139649090267008 learning.py:507] global step 934: loss = 0.2204 (3.092 sec/step)\n",
            "I0619 21:58:35.540813 139649090267008 learning.py:507] global step 934: loss = 0.2062 (3.039 sec/step)\n",
            "I0619 21:58:38.642905 139649090267008 learning.py:507] global step 934: loss = 0.2370 (3.100 sec/step)\n",
            "I0619 21:58:41.690684 139649090267008 learning.py:507] global step 934: loss = 0.2544 (3.046 sec/step)\n",
            "I0619 21:58:44.801775 139649090267008 learning.py:507] global step 934: loss = 0.2240 (3.109 sec/step)\n",
            "I0619 21:58:47.906412 139649090267008 learning.py:507] global step 935: loss = 0.2516 (3.102 sec/step)\n",
            "I0619 21:58:51.003290 139649090267008 learning.py:507] global step 935: loss = 0.3342 (3.095 sec/step)\n",
            "I0619 21:58:54.088911 139649090267008 learning.py:507] global step 935: loss = 0.2196 (3.084 sec/step)\n",
            "I0619 21:58:57.171982 139649090267008 learning.py:507] global step 935: loss = 0.2602 (3.081 sec/step)\n",
            "I0619 21:59:00.291134 139649090267008 learning.py:507] global step 935: loss = 0.2214 (3.115 sec/step)\n",
            "I0619 21:59:03.323761 139649090267008 learning.py:507] global step 935: loss = 0.2924 (3.031 sec/step)\n",
            "I0619 21:59:06.350680 139649090267008 learning.py:507] global step 935: loss = 0.2358 (3.025 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 21:59:09.442914 139649090267008 learning.py:507] global step 935: loss = 0.2537 (3.090 sec/step)\n",
            "I0619 21:59:12.460591 139649090267008 learning.py:507] global step 936: loss = 0.2197 (3.015 sec/step)\n",
            "I0619 21:59:15.532487 139649090267008 learning.py:507] global step 936: loss = 0.2163 (3.070 sec/step)\n",
            "I0619 21:59:18.619081 139649090267008 learning.py:507] global step 936: loss = 0.3482 (3.085 sec/step)\n",
            "I0619 21:59:21.706031 139649090267008 learning.py:507] global step 936: loss = 0.2866 (3.085 sec/step)\n",
            "I0619 21:59:24.841378 139649090267008 learning.py:507] global step 936: loss = 0.2100 (3.134 sec/step)\n",
            "I0619 21:59:27.907314 139649090267008 learning.py:507] global step 936: loss = 0.3394 (3.064 sec/step)\n",
            "I0619 21:59:31.002792 139649090267008 learning.py:507] global step 936: loss = 0.2437 (3.094 sec/step)\n",
            "I0619 21:59:34.052550 139649090267008 learning.py:507] global step 936: loss = 0.2801 (3.048 sec/step)\n",
            "I0619 21:59:37.129603 139649090267008 learning.py:507] global step 937: loss = 0.2622 (3.075 sec/step)\n",
            "I0619 21:59:40.251212 139649090267008 learning.py:507] global step 937: loss = 0.2218 (3.120 sec/step)\n",
            "I0619 21:59:43.279623 139649090267008 learning.py:507] global step 937: loss = 0.2676 (3.027 sec/step)\n",
            "I0619 21:59:46.321583 139649090267008 learning.py:507] global step 937: loss = 0.2764 (3.040 sec/step)\n",
            "I0619 21:59:49.373100 139649090267008 learning.py:507] global step 937: loss = 0.3079 (3.050 sec/step)\n",
            "I0619 21:59:52.441890 139649090267008 learning.py:507] global step 937: loss = 0.2827 (3.067 sec/step)\n",
            "I0619 21:59:55.535475 139649090267008 learning.py:507] global step 937: loss = 0.2265 (3.092 sec/step)\n",
            "I0619 21:59:58.571145 139649090267008 learning.py:507] global step 937: loss = 0.3120 (3.034 sec/step)\n",
            "I0619 22:00:01.694741 139649090267008 learning.py:507] global step 938: loss = 0.2702 (3.121 sec/step)\n",
            "I0619 22:00:04.784339 139649090267008 learning.py:507] global step 938: loss = 0.3755 (3.088 sec/step)\n",
            "I0619 22:00:07.833653 139649090267008 learning.py:507] global step 938: loss = 0.2699 (3.047 sec/step)\n",
            "I0619 22:00:10.921417 139649090267008 learning.py:507] global step 938: loss = 0.2334 (3.086 sec/step)\n",
            "I0619 22:00:14.059954 139649090267008 learning.py:507] global step 938: loss = 0.2530 (3.133 sec/step)\n",
            "I0619 22:00:17.104225 139649090267008 learning.py:507] global step 938: loss = 0.2465 (3.042 sec/step)\n",
            "I0619 22:00:20.215738 139649090267008 learning.py:507] global step 938: loss = 0.2008 (3.110 sec/step)\n",
            "I0619 22:00:20.931319 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 22:00:27.255850 139646017689344 supervisor.py:1050] Recording summary at step 938.\n",
            "I0619 22:00:27.256232 139649090267008 learning.py:507] global step 938: loss = 0.2380 (6.869 sec/step)\n",
            "I0619 22:00:30.342314 139649090267008 learning.py:507] global step 939: loss = 0.3224 (3.077 sec/step)\n",
            "I0619 22:00:33.485446 139649090267008 learning.py:507] global step 939: loss = 0.2578 (3.141 sec/step)\n",
            "I0619 22:00:36.570446 139649090267008 learning.py:507] global step 939: loss = 0.2314 (3.083 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:00:39.680253 139649090267008 learning.py:507] global step 939: loss = 0.2571 (3.108 sec/step)\n",
            "I0619 22:00:42.794140 139649090267008 learning.py:507] global step 939: loss = 0.2615 (3.112 sec/step)\n",
            "I0619 22:00:45.859415 139649090267008 learning.py:507] global step 939: loss = 0.2112 (3.063 sec/step)\n",
            "I0619 22:00:48.916668 139649090267008 learning.py:507] global step 939: loss = 0.3112 (3.055 sec/step)\n",
            "I0619 22:00:52.001357 139649090267008 learning.py:507] global step 939: loss = 0.3600 (3.083 sec/step)\n",
            "I0619 22:00:55.051094 139649090267008 learning.py:507] global step 940: loss = 0.2319 (3.047 sec/step)\n",
            "I0619 22:00:58.112696 139649090267008 learning.py:507] global step 940: loss = 0.2895 (3.060 sec/step)\n",
            "I0619 22:01:01.229597 139649090267008 learning.py:507] global step 940: loss = 0.2315 (3.115 sec/step)\n",
            "I0619 22:01:04.394178 139649090267008 learning.py:507] global step 940: loss = 0.3147 (3.163 sec/step)\n",
            "I0619 22:01:07.500274 139649090267008 learning.py:507] global step 940: loss = 0.2125 (3.104 sec/step)\n",
            "I0619 22:01:10.630781 139649090267008 learning.py:507] global step 940: loss = 0.2511 (3.129 sec/step)\n",
            "I0619 22:01:13.775568 139649090267008 learning.py:507] global step 940: loss = 0.2663 (3.143 sec/step)\n",
            "I0619 22:01:16.817941 139649090267008 learning.py:507] global step 940: loss = 0.2442 (3.040 sec/step)\n",
            "I0619 22:01:19.871543 139649090267008 learning.py:507] global step 941: loss = 0.2833 (3.051 sec/step)\n",
            "I0619 22:01:22.968776 139649090267008 learning.py:507] global step 941: loss = 0.2896 (3.095 sec/step)\n",
            "I0619 22:01:26.072524 139649090267008 learning.py:507] global step 941: loss = 0.2501 (3.102 sec/step)\n",
            "I0619 22:01:29.169749 139649090267008 learning.py:507] global step 941: loss = 0.2379 (3.095 sec/step)\n",
            "I0619 22:01:32.265458 139649090267008 learning.py:507] global step 941: loss = 0.3069 (3.094 sec/step)\n",
            "I0619 22:01:35.383934 139649090267008 learning.py:507] global step 941: loss = 0.2648 (3.116 sec/step)\n",
            "I0619 22:01:38.427681 139649090267008 learning.py:507] global step 941: loss = 0.3306 (3.042 sec/step)\n",
            "I0619 22:01:41.491068 139649090267008 learning.py:507] global step 941: loss = 0.2111 (3.061 sec/step)\n",
            "I0619 22:01:44.653165 139649090267008 learning.py:507] global step 942: loss = 0.2140 (3.160 sec/step)\n",
            "I0619 22:01:47.747720 139649090267008 learning.py:507] global step 942: loss = 0.2314 (3.093 sec/step)\n",
            "I0619 22:01:50.840330 139649090267008 learning.py:507] global step 942: loss = 0.2127 (3.091 sec/step)\n",
            "I0619 22:01:53.899278 139649090267008 learning.py:507] global step 942: loss = 0.2958 (3.057 sec/step)\n",
            "I0619 22:01:56.918892 139649090267008 learning.py:507] global step 942: loss = 0.3532 (3.018 sec/step)\n",
            "I0619 22:01:59.970554 139649090267008 learning.py:507] global step 942: loss = 0.2894 (3.050 sec/step)\n",
            "I0619 22:02:03.013216 139649090267008 learning.py:507] global step 942: loss = 0.2949 (3.041 sec/step)\n",
            "I0619 22:02:06.234634 139649090267008 learning.py:507] global step 942: loss = 0.2858 (3.219 sec/step)\n",
            "I0619 22:02:09.277496 139649090267008 learning.py:507] global step 943: loss = 0.2296 (3.041 sec/step)\n",
            "I0619 22:02:12.299529 139649090267008 learning.py:507] global step 943: loss = 0.2969 (3.020 sec/step)\n",
            "I0619 22:02:15.440694 139649090267008 learning.py:507] global step 943: loss = 0.2448 (3.139 sec/step)\n",
            "I0619 22:02:18.522554 139649090267008 learning.py:507] global step 943: loss = 0.2264 (3.080 sec/step)\n",
            "I0619 22:02:22.713829 139649090267008 learning.py:507] global step 943: loss = 0.2381 (4.181 sec/step)\n",
            "I0619 22:02:26.155411 139646017689344 supervisor.py:1050] Recording summary at step 943.\n",
            "I0619 22:02:27.197632 139649090267008 learning.py:507] global step 943: loss = 0.2085 (4.482 sec/step)\n",
            "I0619 22:02:30.298624 139649090267008 learning.py:507] global step 943: loss = 0.3281 (3.099 sec/step)\n",
            "I0619 22:02:33.363208 139649090267008 learning.py:507] global step 943: loss = 0.2156 (3.063 sec/step)\n",
            "I0619 22:02:36.433064 139649090267008 learning.py:507] global step 944: loss = 0.2424 (3.068 sec/step)\n",
            "I0619 22:02:39.681408 139649090267008 learning.py:507] global step 944: loss = 0.2335 (3.246 sec/step)\n",
            "I0619 22:02:42.720005 139649090267008 learning.py:507] global step 944: loss = 0.2790 (3.037 sec/step)\n",
            "I0619 22:02:45.868072 139649090267008 learning.py:507] global step 944: loss = 0.2533 (3.146 sec/step)\n",
            "I0619 22:02:48.953612 139649090267008 learning.py:507] global step 944: loss = 0.2752 (3.084 sec/step)\n",
            "I0619 22:02:52.076256 139649090267008 learning.py:507] global step 944: loss = 0.2495 (3.121 sec/step)\n",
            "I0619 22:02:55.257061 139649090267008 learning.py:507] global step 944: loss = 0.2537 (3.179 sec/step)\n",
            "I0619 22:02:58.402070 139649090267008 learning.py:507] global step 944: loss = 0.2936 (3.143 sec/step)\n",
            "I0619 22:03:01.447297 139649090267008 learning.py:507] global step 945: loss = 0.3229 (3.043 sec/step)\n",
            "I0619 22:03:04.552533 139649090267008 learning.py:507] global step 945: loss = 0.2703 (3.103 sec/step)\n",
            "I0619 22:03:07.632925 139649090267008 learning.py:507] global step 945: loss = 0.2731 (3.079 sec/step)\n",
            "I0619 22:03:10.863721 139649090267008 learning.py:507] global step 945: loss = 0.2633 (3.229 sec/step)\n",
            "I0619 22:03:13.901771 139649090267008 learning.py:507] global step 945: loss = 0.2237 (3.036 sec/step)\n",
            "I0619 22:03:17.027446 139649090267008 learning.py:507] global step 945: loss = 0.2261 (3.124 sec/step)\n",
            "I0619 22:03:20.134252 139649090267008 learning.py:507] global step 945: loss = 0.2442 (3.105 sec/step)\n",
            "I0619 22:03:23.204325 139649090267008 learning.py:507] global step 945: loss = 0.3001 (3.068 sec/step)\n",
            "I0619 22:03:26.305080 139649090267008 learning.py:507] global step 946: loss = 0.2546 (3.098 sec/step)\n",
            "I0619 22:03:29.607703 139649090267008 learning.py:507] global step 946: loss = 0.2239 (3.301 sec/step)\n",
            "I0619 22:03:32.693348 139649090267008 learning.py:507] global step 946: loss = 0.3137 (3.084 sec/step)\n",
            "I0619 22:03:35.824871 139649090267008 learning.py:507] global step 946: loss = 0.2352 (3.130 sec/step)\n",
            "I0619 22:03:38.895735 139649090267008 learning.py:507] global step 946: loss = 0.1993 (3.069 sec/step)\n",
            "I0619 22:03:42.087119 139649090267008 learning.py:507] global step 946: loss = 0.2335 (3.190 sec/step)\n",
            "I0619 22:03:45.180085 139649090267008 learning.py:507] global step 946: loss = 0.2277 (3.091 sec/step)\n",
            "I0619 22:03:48.279640 139649090267008 learning.py:507] global step 946: loss = 0.2135 (3.098 sec/step)\n",
            "I0619 22:03:51.358367 139649090267008 learning.py:507] global step 947: loss = 0.2276 (3.077 sec/step)\n",
            "I0619 22:03:54.449026 139649090267008 learning.py:507] global step 947: loss = 0.2083 (3.089 sec/step)\n",
            "I0619 22:03:57.610054 139649090267008 learning.py:507] global step 947: loss = 0.3667 (3.159 sec/step)\n",
            "I0619 22:04:00.851781 139649090267008 learning.py:507] global step 947: loss = 0.2433 (3.240 sec/step)\n",
            "I0619 22:04:03.929188 139649090267008 learning.py:507] global step 947: loss = 0.2383 (3.076 sec/step)\n",
            "I0619 22:04:07.012211 139649090267008 learning.py:507] global step 947: loss = 0.2340 (3.081 sec/step)\n",
            "I0619 22:04:10.126133 139649090267008 learning.py:507] global step 947: loss = 0.2926 (3.112 sec/step)\n",
            "I0619 22:04:13.293084 139649090267008 learning.py:507] global step 947: loss = 0.2691 (3.165 sec/step)\n",
            "I0619 22:04:16.434311 139649090267008 learning.py:507] global step 948: loss = 0.2651 (3.139 sec/step)\n",
            "I0619 22:04:19.499441 139649090267008 learning.py:507] global step 948: loss = 0.2712 (3.063 sec/step)\n",
            "I0619 22:04:24.423978 139649090267008 learning.py:507] global step 948: loss = 0.2166 (4.919 sec/step)\n",
            "I0619 22:04:26.386389 139646017689344 supervisor.py:1050] Recording summary at step 948.\n",
            "I0619 22:04:28.014705 139649090267008 learning.py:507] global step 948: loss = 0.2529 (3.580 sec/step)\n",
            "I0619 22:04:31.200821 139649090267008 learning.py:507] global step 948: loss = 0.2705 (3.184 sec/step)\n",
            "I0619 22:04:34.299790 139649090267008 learning.py:507] global step 948: loss = 0.2292 (3.097 sec/step)\n",
            "I0619 22:04:37.360307 139649090267008 learning.py:507] global step 948: loss = 0.2113 (3.059 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:04:40.503098 139649090267008 learning.py:507] global step 948: loss = 0.2326 (3.141 sec/step)\n",
            "I0619 22:04:43.574618 139649090267008 learning.py:507] global step 949: loss = 0.2652 (3.069 sec/step)\n",
            "I0619 22:04:46.637279 139649090267008 learning.py:507] global step 949: loss = 0.2782 (3.060 sec/step)\n",
            "I0619 22:04:49.772357 139649090267008 learning.py:507] global step 949: loss = 0.2166 (3.133 sec/step)\n",
            "I0619 22:04:52.998630 139649090267008 learning.py:507] global step 949: loss = 0.2799 (3.224 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:04:56.090472 139649090267008 learning.py:507] global step 949: loss = 0.2991 (3.090 sec/step)\n",
            "I0619 22:04:59.264095 139649090267008 learning.py:507] global step 949: loss = 0.3285 (3.172 sec/step)\n",
            "I0619 22:05:02.456878 139649090267008 learning.py:507] global step 949: loss = 0.2523 (3.191 sec/step)\n",
            "I0619 22:05:05.526082 139649090267008 learning.py:507] global step 949: loss = 0.2827 (3.067 sec/step)\n",
            "I0619 22:05:08.657632 139649090267008 learning.py:507] global step 950: loss = 0.2911 (3.129 sec/step)\n",
            "I0619 22:05:11.972931 139649090267008 learning.py:507] global step 950: loss = 0.2562 (3.313 sec/step)\n",
            "I0619 22:05:15.061826 139649090267008 learning.py:507] global step 950: loss = 0.2085 (3.087 sec/step)\n",
            "I0619 22:05:18.075632 139649090267008 learning.py:507] global step 950: loss = 0.2287 (3.012 sec/step)\n",
            "I0619 22:05:21.119000 139649090267008 learning.py:507] global step 950: loss = 0.2305 (3.041 sec/step)\n",
            "I0619 22:05:24.262010 139649090267008 learning.py:507] global step 950: loss = 0.2461 (3.141 sec/step)\n",
            "I0619 22:05:27.370285 139649090267008 learning.py:507] global step 950: loss = 0.2893 (3.106 sec/step)\n",
            "I0619 22:05:30.485419 139649090267008 learning.py:507] global step 950: loss = 0.2751 (3.113 sec/step)\n",
            "I0619 22:05:33.616622 139649090267008 learning.py:507] global step 951: loss = 0.2380 (3.128 sec/step)\n",
            "I0619 22:05:36.669870 139649090267008 learning.py:507] global step 951: loss = 0.2578 (3.052 sec/step)\n",
            "I0619 22:05:39.750729 139649090267008 learning.py:507] global step 951: loss = 0.3114 (3.079 sec/step)\n",
            "I0619 22:05:42.813481 139649090267008 learning.py:507] global step 951: loss = 0.2251 (3.061 sec/step)\n",
            "I0619 22:05:45.922786 139649090267008 learning.py:507] global step 951: loss = 0.2913 (3.108 sec/step)\n",
            "I0619 22:05:48.983226 139649090267008 learning.py:507] global step 951: loss = 0.1938 (3.059 sec/step)\n",
            "I0619 22:05:52.051789 139649090267008 learning.py:507] global step 951: loss = 0.2864 (3.066 sec/step)\n",
            "I0619 22:05:55.151527 139649090267008 learning.py:507] global step 951: loss = 0.2287 (3.098 sec/step)\n",
            "I0619 22:05:58.249259 139649090267008 learning.py:507] global step 952: loss = 0.2339 (3.095 sec/step)\n",
            "I0619 22:06:01.489295 139649090267008 learning.py:507] global step 952: loss = 0.2364 (3.238 sec/step)\n",
            "I0619 22:06:04.574145 139649090267008 learning.py:507] global step 952: loss = 0.2937 (3.083 sec/step)\n",
            "I0619 22:06:07.701073 139649090267008 learning.py:507] global step 952: loss = 0.3037 (3.125 sec/step)\n",
            "I0619 22:06:10.723947 139649090267008 learning.py:507] global step 952: loss = 0.2581 (3.021 sec/step)\n",
            "I0619 22:06:13.849143 139649090267008 learning.py:507] global step 952: loss = 0.2533 (3.123 sec/step)\n",
            "I0619 22:06:16.983646 139649090267008 learning.py:507] global step 952: loss = 0.2619 (3.133 sec/step)\n",
            "I0619 22:06:20.360268 139649090267008 learning.py:507] global step 952: loss = 0.2338 (3.375 sec/step)\n",
            "I0619 22:06:25.591807 139649090267008 learning.py:507] global step 953: loss = 0.2439 (5.229 sec/step)\n",
            "I0619 22:06:26.774529 139646017689344 supervisor.py:1050] Recording summary at step 953.\n",
            "I0619 22:06:28.916641 139649090267008 learning.py:507] global step 953: loss = 0.3788 (3.323 sec/step)\n",
            "I0619 22:06:32.052059 139649090267008 learning.py:507] global step 953: loss = 0.2854 (3.134 sec/step)\n",
            "I0619 22:06:35.167456 139649090267008 learning.py:507] global step 953: loss = 0.2595 (3.114 sec/step)\n",
            "I0619 22:06:38.244197 139649090267008 learning.py:507] global step 953: loss = 0.2731 (3.075 sec/step)\n",
            "I0619 22:06:41.354880 139649090267008 learning.py:507] global step 953: loss = 0.2317 (3.109 sec/step)\n",
            "I0619 22:06:44.410311 139649090267008 learning.py:507] global step 953: loss = 0.2557 (3.054 sec/step)\n",
            "I0619 22:06:47.468995 139649090267008 learning.py:507] global step 953: loss = 0.2490 (3.057 sec/step)\n",
            "I0619 22:06:50.587606 139649090267008 learning.py:507] global step 954: loss = 0.3018 (3.116 sec/step)\n",
            "I0619 22:06:53.637766 139649090267008 learning.py:507] global step 954: loss = 0.2165 (3.048 sec/step)\n",
            "I0619 22:06:56.724785 139649090267008 learning.py:507] global step 954: loss = 0.2361 (3.085 sec/step)\n",
            "I0619 22:06:59.739585 139649090267008 learning.py:507] global step 954: loss = 0.2227 (3.013 sec/step)\n",
            "I0619 22:07:02.765043 139649090267008 learning.py:507] global step 954: loss = 0.2133 (3.024 sec/step)\n",
            "I0619 22:07:05.877789 139649090267008 learning.py:507] global step 954: loss = 0.2083 (3.111 sec/step)\n",
            "I0619 22:07:08.983819 139649090267008 learning.py:507] global step 954: loss = 0.2663 (3.104 sec/step)\n",
            "I0619 22:07:12.024722 139649090267008 learning.py:507] global step 954: loss = 0.2223 (3.039 sec/step)\n",
            "I0619 22:07:15.074493 139649090267008 learning.py:507] global step 955: loss = 0.2248 (3.047 sec/step)\n",
            "I0619 22:07:18.223141 139649090267008 learning.py:507] global step 955: loss = 0.2256 (3.146 sec/step)\n",
            "I0619 22:07:21.266593 139649090267008 learning.py:507] global step 955: loss = 0.2265 (3.042 sec/step)\n",
            "I0619 22:07:24.380011 139649090267008 learning.py:507] global step 955: loss = 0.2012 (3.112 sec/step)\n",
            "I0619 22:07:27.441392 139649090267008 learning.py:507] global step 955: loss = 0.3156 (3.060 sec/step)\n",
            "I0619 22:07:30.540803 139649090267008 learning.py:507] global step 955: loss = 0.2234 (3.098 sec/step)\n",
            "I0619 22:07:33.639126 139649090267008 learning.py:507] global step 955: loss = 0.2891 (3.096 sec/step)\n",
            "I0619 22:07:36.919762 139649090267008 learning.py:507] global step 955: loss = 0.3654 (3.276 sec/step)\n",
            "I0619 22:07:39.986846 139649090267008 learning.py:507] global step 956: loss = 0.2463 (3.064 sec/step)\n",
            "I0619 22:07:43.075447 139649090267008 learning.py:507] global step 956: loss = 0.2179 (3.087 sec/step)\n",
            "I0619 22:07:46.159732 139649090267008 learning.py:507] global step 956: loss = 0.2657 (3.082 sec/step)\n",
            "I0619 22:07:49.273266 139649090267008 learning.py:507] global step 956: loss = 0.2754 (3.112 sec/step)\n",
            "I0619 22:07:52.297645 139649090267008 learning.py:507] global step 956: loss = 0.2061 (3.023 sec/step)\n",
            "I0619 22:07:55.641990 139649090267008 learning.py:507] global step 956: loss = 0.2611 (3.343 sec/step)\n",
            "I0619 22:07:58.671854 139649090267008 learning.py:507] global step 956: loss = 0.2538 (3.028 sec/step)\n",
            "I0619 22:08:01.791998 139649090267008 learning.py:507] global step 956: loss = 0.2460 (3.118 sec/step)\n",
            "I0619 22:08:04.877867 139649090267008 learning.py:507] global step 957: loss = 0.2570 (3.084 sec/step)\n",
            "I0619 22:08:08.010903 139649090267008 learning.py:507] global step 957: loss = 0.2286 (3.131 sec/step)\n",
            "I0619 22:08:11.132822 139649090267008 learning.py:507] global step 957: loss = 0.2723 (3.120 sec/step)\n",
            "I0619 22:08:14.213611 139649090267008 learning.py:507] global step 957: loss = 0.2256 (3.079 sec/step)\n",
            "I0619 22:08:17.458382 139649090267008 learning.py:507] global step 957: loss = 0.1979 (3.243 sec/step)\n",
            "I0619 22:08:20.543808 139649090267008 learning.py:507] global step 957: loss = 0.2573 (3.084 sec/step)\n",
            "I0619 22:08:25.916981 139649090267008 learning.py:507] global step 957: loss = 0.2410 (5.371 sec/step)\n",
            "I0619 22:08:26.239222 139646017689344 supervisor.py:1050] Recording summary at step 957.\n",
            "I0619 22:08:29.040954 139649090267008 learning.py:507] global step 957: loss = 0.3708 (3.122 sec/step)\n",
            "I0619 22:08:32.078850 139649090267008 learning.py:507] global step 958: loss = 0.2667 (3.036 sec/step)\n",
            "I0619 22:08:35.326372 139649090267008 learning.py:507] global step 958: loss = 0.2055 (3.246 sec/step)\n",
            "I0619 22:08:38.378726 139649090267008 learning.py:507] global step 958: loss = 0.3309 (3.050 sec/step)\n",
            "I0619 22:08:41.429692 139649090267008 learning.py:507] global step 958: loss = 0.4430 (3.049 sec/step)\n",
            "I0619 22:08:44.530614 139649090267008 learning.py:507] global step 958: loss = 0.2457 (3.099 sec/step)\n",
            "I0619 22:08:47.612779 139649090267008 learning.py:507] global step 958: loss = 0.2508 (3.080 sec/step)\n",
            "I0619 22:08:50.781846 139649090267008 learning.py:507] global step 958: loss = 0.2606 (3.167 sec/step)\n",
            "I0619 22:08:53.868596 139649090267008 learning.py:507] global step 958: loss = 0.2431 (3.085 sec/step)\n",
            "I0619 22:08:57.033914 139649090267008 learning.py:507] global step 959: loss = 0.2516 (3.163 sec/step)\n",
            "I0619 22:09:00.171631 139649090267008 learning.py:507] global step 959: loss = 0.1907 (3.135 sec/step)\n",
            "I0619 22:09:03.212305 139649090267008 learning.py:507] global step 959: loss = 0.2516 (3.039 sec/step)\n",
            "I0619 22:09:06.337783 139649090267008 learning.py:507] global step 959: loss = 0.2378 (3.124 sec/step)\n",
            "I0619 22:09:09.447924 139649090267008 learning.py:507] global step 959: loss = 0.2954 (3.108 sec/step)\n",
            "I0619 22:09:12.764744 139649090267008 learning.py:507] global step 959: loss = 0.2686 (3.315 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:09:16.424423 139649090267008 learning.py:507] global step 959: loss = 0.1993 (3.658 sec/step)\n",
            "I0619 22:09:19.570139 139649090267008 learning.py:507] global step 959: loss = 0.3398 (3.144 sec/step)\n",
            "I0619 22:09:22.673359 139649090267008 learning.py:507] global step 960: loss = 0.2377 (3.101 sec/step)\n",
            "I0619 22:09:25.856484 139649090267008 learning.py:507] global step 960: loss = 0.1959 (3.181 sec/step)\n",
            "I0619 22:09:28.937629 139649090267008 learning.py:507] global step 960: loss = 0.3379 (3.079 sec/step)\n",
            "I0619 22:09:32.351264 139649090267008 learning.py:507] global step 960: loss = 0.3809 (3.412 sec/step)\n",
            "I0619 22:09:35.940612 139649090267008 learning.py:507] global step 960: loss = 0.3008 (3.587 sec/step)\n",
            "I0619 22:09:39.029659 139649090267008 learning.py:507] global step 960: loss = 0.3163 (3.087 sec/step)\n",
            "I0619 22:09:42.284386 139649090267008 learning.py:507] global step 960: loss = 0.2730 (3.253 sec/step)\n",
            "I0619 22:09:45.347341 139649090267008 learning.py:507] global step 960: loss = 0.2305 (3.061 sec/step)\n",
            "I0619 22:09:48.449639 139649090267008 learning.py:507] global step 961: loss = 0.2947 (3.100 sec/step)\n",
            "I0619 22:09:51.564233 139649090267008 learning.py:507] global step 961: loss = 0.2859 (3.113 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:09:54.986943 139649090267008 learning.py:507] global step 961: loss = 0.2856 (3.421 sec/step)\n",
            "I0619 22:09:58.167569 139649090267008 learning.py:507] global step 961: loss = 0.2583 (3.179 sec/step)\n",
            "I0619 22:10:01.736149 139649090267008 learning.py:507] global step 961: loss = 0.2385 (3.567 sec/step)\n",
            "I0619 22:10:04.946022 139649090267008 learning.py:507] global step 961: loss = 0.2281 (3.208 sec/step)\n",
            "I0619 22:10:08.131453 139649090267008 learning.py:507] global step 961: loss = 0.2394 (3.183 sec/step)\n",
            "I0619 22:10:11.312134 139649090267008 learning.py:507] global step 961: loss = 0.2652 (3.179 sec/step)\n",
            "I0619 22:10:14.496616 139649090267008 learning.py:507] global step 962: loss = 0.3223 (3.182 sec/step)\n",
            "I0619 22:10:17.665440 139649090267008 learning.py:507] global step 962: loss = 0.1872 (3.167 sec/step)\n",
            "I0619 22:10:20.885579 139649090267008 learning.py:507] global step 962: loss = 0.2527 (3.218 sec/step)\n",
            "I0619 22:10:20.931311 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 22:10:28.126997 139646017689344 supervisor.py:1050] Recording summary at step 962.\n",
            "I0619 22:10:28.131721 139649090267008 learning.py:507] global step 962: loss = 0.1980 (7.070 sec/step)\n",
            "I0619 22:10:31.326108 139649090267008 learning.py:507] global step 962: loss = 0.2569 (3.191 sec/step)\n",
            "I0619 22:10:34.710333 139649090267008 learning.py:507] global step 962: loss = 0.2608 (3.382 sec/step)\n",
            "I0619 22:10:37.915475 139649090267008 learning.py:507] global step 962: loss = 0.2354 (3.203 sec/step)\n",
            "I0619 22:10:41.062403 139649090267008 learning.py:507] global step 962: loss = 0.2914 (3.145 sec/step)\n",
            "I0619 22:10:44.279934 139649090267008 learning.py:507] global step 963: loss = 0.2463 (3.215 sec/step)\n",
            "I0619 22:10:47.546621 139649090267008 learning.py:507] global step 963: loss = 0.3104 (3.265 sec/step)\n",
            "I0619 22:10:50.740319 139649090267008 learning.py:507] global step 963: loss = 0.2988 (3.192 sec/step)\n",
            "I0619 22:10:53.900792 139649090267008 learning.py:507] global step 963: loss = 0.2268 (3.159 sec/step)\n",
            "I0619 22:10:57.096151 139649090267008 learning.py:507] global step 963: loss = 0.2537 (3.193 sec/step)\n",
            "I0619 22:11:00.298053 139649090267008 learning.py:507] global step 963: loss = 0.2288 (3.200 sec/step)\n",
            "I0619 22:11:03.421180 139649090267008 learning.py:507] global step 963: loss = 0.2984 (3.121 sec/step)\n",
            "I0619 22:11:06.619281 139649090267008 learning.py:507] global step 963: loss = 0.1930 (3.196 sec/step)\n",
            "I0619 22:11:09.853375 139649090267008 learning.py:507] global step 964: loss = 0.2428 (3.231 sec/step)\n",
            "I0619 22:11:12.952424 139649090267008 learning.py:507] global step 964: loss = 0.1951 (3.097 sec/step)\n",
            "I0619 22:11:16.076159 139649090267008 learning.py:507] global step 964: loss = 0.2841 (3.122 sec/step)\n",
            "I0619 22:11:19.147528 139649090267008 learning.py:507] global step 964: loss = 0.2450 (3.069 sec/step)\n",
            "I0619 22:11:22.245922 139649090267008 learning.py:507] global step 964: loss = 0.2134 (3.097 sec/step)\n",
            "I0619 22:11:25.405311 139649090267008 learning.py:507] global step 964: loss = 0.2547 (3.157 sec/step)\n",
            "I0619 22:11:28.568642 139649090267008 learning.py:507] global step 964: loss = 0.2204 (3.161 sec/step)\n",
            "I0619 22:11:31.701459 139649090267008 learning.py:507] global step 964: loss = 0.2420 (3.131 sec/step)\n",
            "I0619 22:11:34.829934 139649090267008 learning.py:507] global step 965: loss = 0.2327 (3.127 sec/step)\n",
            "I0619 22:11:37.889555 139649090267008 learning.py:507] global step 965: loss = 0.2585 (3.058 sec/step)\n",
            "I0619 22:11:40.931457 139649090267008 learning.py:507] global step 965: loss = 0.2800 (3.040 sec/step)\n",
            "I0619 22:11:43.983591 139649090267008 learning.py:507] global step 965: loss = 0.2601 (3.050 sec/step)\n",
            "I0619 22:11:47.123091 139649090267008 learning.py:507] global step 965: loss = 0.2207 (3.138 sec/step)\n",
            "I0619 22:11:50.200924 139649090267008 learning.py:507] global step 965: loss = 0.2494 (3.076 sec/step)\n",
            "I0619 22:11:53.267578 139649090267008 learning.py:507] global step 965: loss = 0.2928 (3.065 sec/step)\n",
            "I0619 22:11:56.366586 139649090267008 learning.py:507] global step 965: loss = 0.2991 (3.097 sec/step)\n",
            "I0619 22:11:59.483980 139649090267008 learning.py:507] global step 966: loss = 0.2472 (3.115 sec/step)\n",
            "I0619 22:12:02.554391 139649090267008 learning.py:507] global step 966: loss = 0.2671 (3.069 sec/step)\n",
            "I0619 22:12:05.629014 139649090267008 learning.py:507] global step 966: loss = 0.2367 (3.073 sec/step)\n",
            "I0619 22:12:08.709624 139649090267008 learning.py:507] global step 966: loss = 0.2066 (3.079 sec/step)\n",
            "I0619 22:12:11.839406 139649090267008 learning.py:507] global step 966: loss = 0.3293 (3.128 sec/step)\n",
            "I0619 22:12:14.897902 139649090267008 learning.py:507] global step 966: loss = 0.2233 (3.057 sec/step)\n",
            "I0619 22:12:18.131658 139649090267008 learning.py:507] global step 966: loss = 0.2139 (3.232 sec/step)\n",
            "I0619 22:12:21.396402 139649090267008 learning.py:507] global step 966: loss = 0.2059 (3.110 sec/step)\n",
            "I0619 22:12:25.805078 139646017689344 supervisor.py:1050] Recording summary at step 966.\n",
            "I0619 22:12:26.510345 139649090267008 learning.py:507] global step 967: loss = 0.2345 (4.980 sec/step)\n",
            "I0619 22:12:29.572831 139649090267008 learning.py:507] global step 967: loss = 0.3249 (3.060 sec/step)\n",
            "I0619 22:12:32.654440 139649090267008 learning.py:507] global step 967: loss = 0.2825 (3.080 sec/step)\n",
            "I0619 22:12:35.663028 139649090267008 learning.py:507] global step 967: loss = 0.2580 (3.007 sec/step)\n",
            "I0619 22:12:38.727913 139649090267008 learning.py:507] global step 967: loss = 0.2181 (3.063 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:12:41.811076 139649090267008 learning.py:507] global step 967: loss = 0.2114 (3.081 sec/step)\n",
            "I0619 22:12:44.868291 139649090267008 learning.py:507] global step 967: loss = 0.2709 (3.055 sec/step)\n",
            "I0619 22:12:48.149816 139649090267008 learning.py:507] global step 967: loss = 0.2202 (3.280 sec/step)\n",
            "I0619 22:12:51.182920 139649090267008 learning.py:507] global step 968: loss = 0.2216 (3.030 sec/step)\n",
            "I0619 22:12:54.193887 139649090267008 learning.py:507] global step 968: loss = 0.2611 (3.009 sec/step)\n",
            "I0619 22:12:57.246278 139649090267008 learning.py:507] global step 968: loss = 0.3957 (3.050 sec/step)\n",
            "I0619 22:13:00.374595 139649090267008 learning.py:507] global step 968: loss = 0.2087 (3.126 sec/step)\n",
            "I0619 22:13:03.507367 139649090267008 learning.py:507] global step 968: loss = 0.2263 (3.131 sec/step)\n",
            "I0619 22:13:06.852717 139649090267008 learning.py:507] global step 968: loss = 0.2114 (3.343 sec/step)\n",
            "I0619 22:13:10.289743 139649090267008 learning.py:507] global step 968: loss = 0.2833 (3.435 sec/step)\n",
            "I0619 22:13:13.376674 139649090267008 learning.py:507] global step 968: loss = 0.2467 (3.085 sec/step)\n",
            "I0619 22:13:16.538535 139649090267008 learning.py:507] global step 969: loss = 0.2140 (3.160 sec/step)\n",
            "I0619 22:13:19.762990 139649090267008 learning.py:507] global step 969: loss = 0.2062 (3.222 sec/step)\n",
            "I0619 22:13:22.828270 139649090267008 learning.py:507] global step 969: loss = 0.2880 (3.064 sec/step)\n",
            "I0619 22:13:25.867830 139649090267008 learning.py:507] global step 969: loss = 0.2718 (3.038 sec/step)\n",
            "I0619 22:13:29.300717 139649090267008 learning.py:507] global step 969: loss = 0.2577 (3.431 sec/step)\n",
            "I0619 22:13:32.443328 139649090267008 learning.py:507] global step 969: loss = 0.2121 (3.141 sec/step)\n",
            "I0619 22:13:35.572985 139649090267008 learning.py:507] global step 969: loss = 0.2142 (3.128 sec/step)\n",
            "I0619 22:13:38.710917 139649090267008 learning.py:507] global step 969: loss = 0.1966 (3.136 sec/step)\n",
            "I0619 22:13:41.823826 139649090267008 learning.py:507] global step 970: loss = 0.2693 (3.110 sec/step)\n",
            "I0619 22:13:44.939773 139649090267008 learning.py:507] global step 970: loss = 0.2308 (3.114 sec/step)\n",
            "I0619 22:13:48.026408 139649090267008 learning.py:507] global step 970: loss = 0.2117 (3.085 sec/step)\n",
            "I0619 22:13:51.111232 139649090267008 learning.py:507] global step 970: loss = 0.2593 (3.083 sec/step)\n",
            "I0619 22:13:54.217299 139649090267008 learning.py:507] global step 970: loss = 0.2215 (3.104 sec/step)\n",
            "I0619 22:13:57.241526 139649090267008 learning.py:507] global step 970: loss = 0.2398 (3.023 sec/step)\n",
            "I0619 22:14:00.263729 139649090267008 learning.py:507] global step 970: loss = 0.2828 (3.020 sec/step)\n",
            "I0619 22:14:03.316173 139649090267008 learning.py:507] global step 970: loss = 0.2173 (3.051 sec/step)\n",
            "I0619 22:14:06.430724 139649090267008 learning.py:507] global step 971: loss = 0.2280 (3.112 sec/step)\n",
            "I0619 22:14:09.553840 139649090267008 learning.py:507] global step 971: loss = 0.2103 (3.121 sec/step)\n",
            "I0619 22:14:12.664456 139649090267008 learning.py:507] global step 971: loss = 0.2348 (3.109 sec/step)\n",
            "I0619 22:14:15.762262 139649090267008 learning.py:507] global step 971: loss = 0.2475 (3.096 sec/step)\n",
            "I0619 22:14:18.851845 139649090267008 learning.py:507] global step 971: loss = 0.3313 (3.088 sec/step)\n",
            "I0619 22:14:23.356241 139649090267008 learning.py:507] global step 971: loss = 0.2254 (4.480 sec/step)\n",
            "I0619 22:14:26.374985 139646017689344 supervisor.py:1050] Recording summary at step 971.\n",
            "I0619 22:14:27.442128 139649090267008 learning.py:507] global step 971: loss = 0.2697 (4.084 sec/step)\n",
            "I0619 22:14:30.534929 139649090267008 learning.py:507] global step 971: loss = 0.2971 (3.091 sec/step)\n",
            "I0619 22:14:33.618167 139649090267008 learning.py:507] global step 972: loss = 0.2277 (3.081 sec/step)\n",
            "I0619 22:14:36.704832 139649090267008 learning.py:507] global step 972: loss = 0.2599 (3.085 sec/step)\n",
            "I0619 22:14:39.799182 139649090267008 learning.py:507] global step 972: loss = 0.2738 (3.093 sec/step)\n",
            "I0619 22:14:42.922420 139649090267008 learning.py:507] global step 972: loss = 0.3080 (3.121 sec/step)\n",
            "I0619 22:14:45.998227 139649090267008 learning.py:507] global step 972: loss = 0.2433 (3.074 sec/step)\n",
            "I0619 22:14:49.027954 139649090267008 learning.py:507] global step 972: loss = 0.2190 (3.028 sec/step)\n",
            "I0619 22:14:52.129882 139649090267008 learning.py:507] global step 972: loss = 0.3232 (3.100 sec/step)\n",
            "I0619 22:14:55.235650 139649090267008 learning.py:507] global step 972: loss = 0.2280 (3.104 sec/step)\n",
            "I0619 22:14:58.357986 139649090267008 learning.py:507] global step 973: loss = 0.2879 (3.120 sec/step)\n",
            "I0619 22:15:01.444009 139649090267008 learning.py:507] global step 973: loss = 0.2143 (3.084 sec/step)\n",
            "I0619 22:15:04.565222 139649090267008 learning.py:507] global step 973: loss = 0.3291 (3.119 sec/step)\n",
            "I0619 22:15:07.687808 139649090267008 learning.py:507] global step 973: loss = 0.2595 (3.121 sec/step)\n",
            "I0619 22:15:10.838467 139649090267008 learning.py:507] global step 973: loss = 0.2624 (3.149 sec/step)\n",
            "I0619 22:15:13.987346 139649090267008 learning.py:507] global step 973: loss = 0.2345 (3.147 sec/step)\n",
            "I0619 22:15:17.099644 139649090267008 learning.py:507] global step 973: loss = 0.2776 (3.110 sec/step)\n",
            "I0619 22:15:20.205545 139649090267008 learning.py:507] global step 973: loss = 0.3014 (3.104 sec/step)\n",
            "I0619 22:15:23.245587 139649090267008 learning.py:507] global step 974: loss = 0.3071 (3.038 sec/step)\n",
            "I0619 22:15:26.288039 139649090267008 learning.py:507] global step 974: loss = 0.2136 (3.040 sec/step)\n",
            "I0619 22:15:29.307594 139649090267008 learning.py:507] global step 974: loss = 0.3871 (3.018 sec/step)\n",
            "I0619 22:15:32.383184 139649090267008 learning.py:507] global step 974: loss = 0.3261 (3.074 sec/step)\n",
            "I0619 22:15:35.510891 139649090267008 learning.py:507] global step 974: loss = 0.2126 (3.126 sec/step)\n",
            "I0619 22:15:38.552871 139649090267008 learning.py:507] global step 974: loss = 0.2431 (3.040 sec/step)\n",
            "I0619 22:15:41.647903 139649090267008 learning.py:507] global step 974: loss = 0.2394 (3.093 sec/step)\n",
            "I0619 22:15:44.722980 139649090267008 learning.py:507] global step 974: loss = 0.2978 (3.073 sec/step)\n",
            "I0619 22:15:47.837658 139649090267008 learning.py:507] global step 975: loss = 0.2476 (3.112 sec/step)\n",
            "I0619 22:15:50.873803 139649090267008 learning.py:507] global step 975: loss = 0.2343 (3.034 sec/step)\n",
            "I0619 22:15:53.981562 139649090267008 learning.py:507] global step 975: loss = 0.1872 (3.106 sec/step)\n",
            "I0619 22:15:57.090415 139649090267008 learning.py:507] global step 975: loss = 0.2516 (3.106 sec/step)\n",
            "I0619 22:16:00.163344 139649090267008 learning.py:507] global step 975: loss = 0.2740 (3.071 sec/step)\n",
            "I0619 22:16:03.250949 139649090267008 learning.py:507] global step 975: loss = 0.2349 (3.086 sec/step)\n",
            "I0619 22:16:06.356920 139649090267008 learning.py:507] global step 975: loss = 0.2908 (3.104 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:16:09.549471 139649090267008 learning.py:507] global step 975: loss = 0.2041 (3.191 sec/step)\n",
            "I0619 22:16:12.679879 139649090267008 learning.py:507] global step 976: loss = 0.2978 (3.128 sec/step)\n",
            "I0619 22:16:15.811206 139649090267008 learning.py:507] global step 976: loss = 0.2450 (3.129 sec/step)\n",
            "I0619 22:16:18.928975 139649090267008 learning.py:507] global step 976: loss = 0.2850 (3.116 sec/step)\n",
            "I0619 22:16:23.388755 139649090267008 learning.py:507] global step 976: loss = 0.2217 (4.446 sec/step)\n",
            "I0619 22:16:26.469287 139646017689344 supervisor.py:1050] Recording summary at step 976.\n",
            "I0619 22:16:27.480429 139649090267008 learning.py:507] global step 976: loss = 0.2487 (4.082 sec/step)\n",
            "I0619 22:16:30.697297 139649090267008 learning.py:507] global step 976: loss = 0.2562 (3.215 sec/step)\n",
            "I0619 22:16:33.748718 139649090267008 learning.py:507] global step 976: loss = 0.2905 (3.050 sec/step)\n",
            "I0619 22:16:36.989112 139649090267008 learning.py:507] global step 976: loss = 0.2940 (3.239 sec/step)\n",
            "I0619 22:16:40.126065 139649090267008 learning.py:507] global step 977: loss = 0.2033 (3.134 sec/step)\n",
            "I0619 22:16:43.233980 139649090267008 learning.py:507] global step 977: loss = 0.3405 (3.106 sec/step)\n",
            "I0619 22:16:46.372296 139649090267008 learning.py:507] global step 977: loss = 0.2255 (3.136 sec/step)\n",
            "I0619 22:16:49.476103 139649090267008 learning.py:507] global step 977: loss = 0.2515 (3.102 sec/step)\n",
            "I0619 22:16:52.574718 139649090267008 learning.py:507] global step 977: loss = 0.2981 (3.097 sec/step)\n",
            "I0619 22:16:55.689097 139649090267008 learning.py:507] global step 977: loss = 0.2853 (3.113 sec/step)\n",
            "I0619 22:16:58.886665 139649090267008 learning.py:507] global step 977: loss = 0.2270 (3.196 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:17:01.934709 139649090267008 learning.py:507] global step 977: loss = 0.2274 (3.046 sec/step)\n",
            "I0619 22:17:04.996552 139649090267008 learning.py:507] global step 978: loss = 0.2566 (3.060 sec/step)\n",
            "I0619 22:17:08.116861 139649090267008 learning.py:507] global step 978: loss = 0.2270 (3.118 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:17:11.240221 139649090267008 learning.py:507] global step 978: loss = 0.2544 (3.121 sec/step)\n",
            "I0619 22:17:14.317689 139649090267008 learning.py:507] global step 978: loss = 0.2420 (3.076 sec/step)\n",
            "I0619 22:17:17.444878 139649090267008 learning.py:507] global step 978: loss = 0.3451 (3.125 sec/step)\n",
            "I0619 22:17:20.532922 139649090267008 learning.py:507] global step 978: loss = 0.2022 (3.086 sec/step)\n",
            "I0619 22:17:23.549381 139649090267008 learning.py:507] global step 978: loss = 0.2011 (3.015 sec/step)\n",
            "I0619 22:17:26.649876 139649090267008 learning.py:507] global step 978: loss = 0.2680 (3.099 sec/step)\n",
            "I0619 22:17:29.677187 139649090267008 learning.py:507] global step 979: loss = 0.2357 (3.025 sec/step)\n",
            "I0619 22:17:32.752404 139649090267008 learning.py:507] global step 979: loss = 0.2190 (3.073 sec/step)\n",
            "I0619 22:17:35.844607 139649090267008 learning.py:507] global step 979: loss = 0.2179 (3.090 sec/step)\n",
            "I0619 22:17:38.950670 139649090267008 learning.py:507] global step 979: loss = 0.2268 (3.104 sec/step)\n",
            "I0619 22:17:42.021398 139649090267008 learning.py:507] global step 979: loss = 0.2308 (3.069 sec/step)\n",
            "I0619 22:17:45.041525 139649090267008 learning.py:507] global step 979: loss = 0.2185 (3.018 sec/step)\n",
            "I0619 22:17:48.162215 139649090267008 learning.py:507] global step 979: loss = 0.2635 (3.119 sec/step)\n",
            "I0619 22:17:51.282355 139649090267008 learning.py:507] global step 979: loss = 0.2252 (3.118 sec/step)\n",
            "I0619 22:17:54.339354 139649090267008 learning.py:507] global step 980: loss = 0.3535 (3.055 sec/step)\n",
            "I0619 22:17:57.464045 139649090267008 learning.py:507] global step 980: loss = 0.2844 (3.123 sec/step)\n",
            "I0619 22:18:00.533879 139649090267008 learning.py:507] global step 980: loss = 0.3207 (3.068 sec/step)\n",
            "I0619 22:18:03.605741 139649090267008 learning.py:507] global step 980: loss = 0.3124 (3.070 sec/step)\n",
            "I0619 22:18:06.637116 139649090267008 learning.py:507] global step 980: loss = 0.2383 (3.030 sec/step)\n",
            "I0619 22:18:09.721265 139649090267008 learning.py:507] global step 980: loss = 0.3130 (3.082 sec/step)\n",
            "I0619 22:18:12.816253 139649090267008 learning.py:507] global step 980: loss = 0.3066 (3.093 sec/step)\n",
            "I0619 22:18:16.014948 139649090267008 learning.py:507] global step 980: loss = 0.2414 (3.196 sec/step)\n",
            "I0619 22:18:19.105026 139649090267008 learning.py:507] global step 981: loss = 0.2424 (3.088 sec/step)\n",
            "I0619 22:18:24.141789 139649090267008 learning.py:507] global step 981: loss = 0.2451 (5.035 sec/step)\n",
            "I0619 22:18:26.302489 139646017689344 supervisor.py:1050] Recording summary at step 981.\n",
            "I0619 22:18:27.829396 139649090267008 learning.py:507] global step 981: loss = 0.2603 (3.680 sec/step)\n",
            "I0619 22:18:30.973586 139649090267008 learning.py:507] global step 981: loss = 0.2172 (3.142 sec/step)\n",
            "I0619 22:18:34.057702 139649090267008 learning.py:507] global step 981: loss = 0.2198 (3.082 sec/step)\n",
            "I0619 22:18:37.162436 139649090267008 learning.py:507] global step 981: loss = 0.2584 (3.103 sec/step)\n",
            "I0619 22:18:40.437684 139649090267008 learning.py:507] global step 981: loss = 0.2827 (3.273 sec/step)\n",
            "I0619 22:18:43.536596 139649090267008 learning.py:507] global step 981: loss = 0.2423 (3.097 sec/step)\n",
            "I0619 22:18:46.627393 139649090267008 learning.py:507] global step 982: loss = 0.2380 (3.088 sec/step)\n",
            "I0619 22:18:49.693101 139649090267008 learning.py:507] global step 982: loss = 0.2586 (3.064 sec/step)\n",
            "I0619 22:18:52.796201 139649090267008 learning.py:507] global step 982: loss = 0.2191 (3.101 sec/step)\n",
            "I0619 22:18:55.892929 139649090267008 learning.py:507] global step 982: loss = 0.2710 (3.095 sec/step)\n",
            "I0619 22:18:58.955647 139649090267008 learning.py:507] global step 982: loss = 0.2597 (3.061 sec/step)\n",
            "I0619 22:19:02.032272 139649090267008 learning.py:507] global step 982: loss = 0.2388 (3.074 sec/step)\n",
            "I0619 22:19:05.150681 139649090267008 learning.py:507] global step 982: loss = 0.2338 (3.116 sec/step)\n",
            "I0619 22:19:08.241070 139649090267008 learning.py:507] global step 982: loss = 0.2145 (3.089 sec/step)\n",
            "I0619 22:19:11.245099 139649090267008 learning.py:507] global step 983: loss = 0.2707 (3.002 sec/step)\n",
            "I0619 22:19:14.356812 139649090267008 learning.py:507] global step 983: loss = 0.2536 (3.110 sec/step)\n",
            "I0619 22:19:17.378075 139649090267008 learning.py:507] global step 983: loss = 0.2922 (3.019 sec/step)\n",
            "I0619 22:19:20.445061 139649090267008 learning.py:507] global step 983: loss = 0.2891 (3.065 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:19:23.543806 139649090267008 learning.py:507] global step 983: loss = 0.2539 (3.095 sec/step)\n",
            "I0619 22:19:26.585813 139649090267008 learning.py:507] global step 983: loss = 0.2070 (3.040 sec/step)\n",
            "I0619 22:19:29.645231 139649090267008 learning.py:507] global step 983: loss = 0.2348 (3.058 sec/step)\n",
            "I0619 22:19:32.670898 139649090267008 learning.py:507] global step 983: loss = 0.2643 (3.024 sec/step)\n",
            "I0619 22:19:35.755617 139649090267008 learning.py:507] global step 984: loss = 0.2184 (3.082 sec/step)\n",
            "I0619 22:19:38.868522 139649090267008 learning.py:507] global step 984: loss = 0.1984 (3.111 sec/step)\n",
            "I0619 22:19:41.944071 139649090267008 learning.py:507] global step 984: loss = 0.2478 (3.074 sec/step)\n",
            "I0619 22:19:45.071437 139649090267008 learning.py:507] global step 984: loss = 0.2584 (3.126 sec/step)\n",
            "I0619 22:19:48.177361 139649090267008 learning.py:507] global step 984: loss = 0.2188 (3.104 sec/step)\n",
            "I0619 22:19:51.294249 139649090267008 learning.py:507] global step 984: loss = 0.2897 (3.115 sec/step)\n",
            "I0619 22:19:54.428940 139649090267008 learning.py:507] global step 984: loss = 0.2278 (3.133 sec/step)\n",
            "I0619 22:19:57.522058 139649090267008 learning.py:507] global step 984: loss = 0.2975 (3.091 sec/step)\n",
            "I0619 22:20:00.610346 139649090267008 learning.py:507] global step 985: loss = 0.2487 (3.086 sec/step)\n",
            "I0619 22:20:03.698715 139649090267008 learning.py:507] global step 985: loss = 0.2578 (3.087 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:20:06.813740 139649090267008 learning.py:507] global step 985: loss = 0.2701 (3.113 sec/step)\n",
            "I0619 22:20:09.922304 139649090267008 learning.py:507] global step 985: loss = 0.2922 (3.107 sec/step)\n",
            "I0619 22:20:13.044188 139649090267008 learning.py:507] global step 985: loss = 0.2184 (3.120 sec/step)\n",
            "I0619 22:20:16.225932 139649090267008 learning.py:507] global step 985: loss = 0.2196 (3.180 sec/step)\n",
            "I0619 22:20:19.419018 139649090267008 learning.py:507] global step 985: loss = 0.2439 (3.191 sec/step)\n",
            "I0619 22:20:20.931298 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 22:20:25.673201 139649090267008 learning.py:507] global step 985: loss = 0.2802 (6.246 sec/step)\n",
            "I0619 22:20:27.578772 139646017689344 supervisor.py:1050] Recording summary at step 985.\n",
            "I0619 22:20:29.433249 139649090267008 learning.py:507] global step 986: loss = 0.2099 (3.757 sec/step)\n",
            "I0619 22:20:32.472292 139649090267008 learning.py:507] global step 986: loss = 0.1892 (3.037 sec/step)\n",
            "I0619 22:20:35.541794 139649090267008 learning.py:507] global step 986: loss = 0.2013 (3.068 sec/step)\n",
            "I0619 22:20:38.802305 139649090267008 learning.py:507] global step 986: loss = 0.2956 (3.258 sec/step)\n",
            "I0619 22:20:41.907336 139649090267008 learning.py:507] global step 986: loss = 0.2716 (3.103 sec/step)\n",
            "I0619 22:20:44.970337 139649090267008 learning.py:507] global step 986: loss = 0.2736 (3.061 sec/step)\n",
            "I0619 22:20:48.027200 139649090267008 learning.py:507] global step 986: loss = 0.2780 (3.055 sec/step)\n",
            "I0619 22:20:51.110737 139649090267008 learning.py:507] global step 986: loss = 0.3041 (3.082 sec/step)\n",
            "I0619 22:20:54.669499 139649090267008 learning.py:507] global step 987: loss = 0.2539 (3.556 sec/step)\n",
            "I0619 22:20:57.804124 139649090267008 learning.py:507] global step 987: loss = 0.2521 (3.133 sec/step)\n",
            "I0619 22:21:00.882299 139649090267008 learning.py:507] global step 987: loss = 0.2903 (3.076 sec/step)\n",
            "I0619 22:21:04.026145 139649090267008 learning.py:507] global step 987: loss = 0.3044 (3.142 sec/step)\n",
            "I0619 22:21:07.079412 139649090267008 learning.py:507] global step 987: loss = 0.3368 (3.052 sec/step)\n",
            "I0619 22:21:10.176595 139649090267008 learning.py:507] global step 987: loss = 0.2610 (3.095 sec/step)\n",
            "I0619 22:21:13.882933 139649090267008 learning.py:507] global step 987: loss = 0.2220 (3.705 sec/step)\n",
            "I0619 22:21:16.964896 139649090267008 learning.py:507] global step 987: loss = 0.2404 (3.080 sec/step)\n",
            "I0619 22:21:20.449708 139649090267008 learning.py:507] global step 988: loss = 0.2058 (3.482 sec/step)\n",
            "I0619 22:21:23.554240 139649090267008 learning.py:507] global step 988: loss = 0.2084 (3.103 sec/step)\n",
            "I0619 22:21:26.592391 139649090267008 learning.py:507] global step 988: loss = 0.2749 (3.036 sec/step)\n",
            "I0619 22:21:29.640986 139649090267008 learning.py:507] global step 988: loss = 0.2580 (3.047 sec/step)\n",
            "I0619 22:21:32.664415 139649090267008 learning.py:507] global step 988: loss = 0.2308 (3.022 sec/step)\n",
            "I0619 22:21:35.761668 139649090267008 learning.py:507] global step 988: loss = 0.1906 (3.095 sec/step)\n",
            "I0619 22:21:39.499650 139649090267008 learning.py:507] global step 988: loss = 0.2083 (3.736 sec/step)\n",
            "I0619 22:21:42.521986 139649090267008 learning.py:507] global step 988: loss = 0.2305 (3.020 sec/step)\n",
            "I0619 22:21:45.708281 139649090267008 learning.py:507] global step 989: loss = 0.2222 (3.184 sec/step)\n",
            "I0619 22:21:48.814237 139649090267008 learning.py:507] global step 989: loss = 0.2609 (3.104 sec/step)\n",
            "I0619 22:21:52.036835 139649090267008 learning.py:507] global step 989: loss = 0.2563 (3.221 sec/step)\n",
            "I0619 22:21:55.194366 139649090267008 learning.py:507] global step 989: loss = 0.2226 (3.156 sec/step)\n",
            "I0619 22:21:58.627158 139649090267008 learning.py:507] global step 989: loss = 0.2406 (3.431 sec/step)\n",
            "I0619 22:22:01.872286 139649090267008 learning.py:507] global step 989: loss = 0.2442 (3.243 sec/step)\n",
            "I0619 22:22:04.932627 139649090267008 learning.py:507] global step 989: loss = 0.2518 (3.059 sec/step)\n",
            "I0619 22:22:07.940140 139649090267008 learning.py:507] global step 989: loss = 0.2997 (3.006 sec/step)\n",
            "I0619 22:22:11.037270 139649090267008 learning.py:507] global step 990: loss = 0.2330 (3.095 sec/step)\n",
            "I0619 22:22:14.408278 139649090267008 learning.py:507] global step 990: loss = 0.2674 (3.369 sec/step)\n",
            "I0619 22:22:17.474110 139649090267008 learning.py:507] global step 990: loss = 0.2370 (3.064 sec/step)\n",
            "I0619 22:22:20.870901 139649090267008 learning.py:507] global step 990: loss = 0.2183 (3.395 sec/step)\n",
            "I0619 22:22:26.297167 139649090267008 learning.py:507] global step 990: loss = 0.2430 (5.423 sec/step)\n",
            "I0619 22:22:26.722167 139646017689344 supervisor.py:1050] Recording summary at step 990.\n",
            "I0619 22:22:29.488408 139649090267008 learning.py:507] global step 990: loss = 0.2371 (3.189 sec/step)\n",
            "I0619 22:22:32.901372 139649090267008 learning.py:507] global step 990: loss = 0.3855 (3.411 sec/step)\n",
            "I0619 22:22:36.139912 139649090267008 learning.py:507] global step 990: loss = 0.2119 (3.237 sec/step)\n",
            "I0619 22:22:39.199233 139649090267008 learning.py:507] global step 991: loss = 0.2444 (3.057 sec/step)\n",
            "I0619 22:22:42.304011 139649090267008 learning.py:507] global step 991: loss = 0.2774 (3.103 sec/step)\n",
            "I0619 22:22:45.349220 139649090267008 learning.py:507] global step 991: loss = 0.2175 (3.043 sec/step)\n",
            "I0619 22:22:48.412736 139649090267008 learning.py:507] global step 991: loss = 0.2471 (3.061 sec/step)\n",
            "I0619 22:22:51.560155 139649090267008 learning.py:507] global step 991: loss = 0.3009 (3.145 sec/step)\n",
            "I0619 22:22:54.894411 139649090267008 learning.py:507] global step 991: loss = 0.2807 (3.332 sec/step)\n",
            "I0619 22:22:58.014340 139649090267008 learning.py:507] global step 991: loss = 0.2182 (3.118 sec/step)\n",
            "I0619 22:23:01.111538 139649090267008 learning.py:507] global step 991: loss = 0.2923 (3.095 sec/step)\n",
            "I0619 22:23:04.190371 139649090267008 learning.py:507] global step 992: loss = 0.3837 (3.076 sec/step)\n",
            "I0619 22:23:07.292400 139649090267008 learning.py:507] global step 992: loss = 0.2246 (3.100 sec/step)\n",
            "I0619 22:23:10.376597 139649090267008 learning.py:507] global step 992: loss = 0.3289 (3.082 sec/step)\n",
            "I0619 22:23:13.410896 139649090267008 learning.py:507] global step 992: loss = 0.1989 (3.032 sec/step)\n",
            "I0619 22:23:16.751276 139649090267008 learning.py:507] global step 992: loss = 0.2256 (3.339 sec/step)\n",
            "I0619 22:23:19.816925 139649090267008 learning.py:507] global step 992: loss = 0.2499 (3.064 sec/step)\n",
            "I0619 22:23:22.868444 139649090267008 learning.py:507] global step 992: loss = 0.2272 (3.049 sec/step)\n",
            "I0619 22:23:25.891014 139649090267008 learning.py:507] global step 992: loss = 0.2475 (3.021 sec/step)\n",
            "I0619 22:23:28.967979 139649090267008 learning.py:507] global step 993: loss = 0.2512 (3.075 sec/step)\n",
            "I0619 22:23:32.083230 139649090267008 learning.py:507] global step 993: loss = 0.2259 (3.113 sec/step)\n",
            "I0619 22:23:35.609489 139649090267008 learning.py:507] global step 993: loss = 0.2059 (3.524 sec/step)\n",
            "I0619 22:23:38.770585 139649090267008 learning.py:507] global step 993: loss = 0.2188 (3.159 sec/step)\n",
            "I0619 22:23:41.875190 139649090267008 learning.py:507] global step 993: loss = 0.2365 (3.103 sec/step)\n",
            "I0619 22:23:44.965420 139649090267008 learning.py:507] global step 993: loss = 0.2325 (3.088 sec/step)\n",
            "I0619 22:23:48.019990 139649090267008 learning.py:507] global step 993: loss = 0.2393 (3.053 sec/step)\n",
            "I0619 22:23:51.107359 139649090267008 learning.py:507] global step 993: loss = 0.2702 (3.085 sec/step)\n",
            "I0619 22:23:54.285020 139649090267008 learning.py:507] global step 994: loss = 0.3370 (3.175 sec/step)\n",
            "I0619 22:23:57.508118 139649090267008 learning.py:507] global step 994: loss = 0.2404 (3.221 sec/step)\n",
            "I0619 22:24:00.527328 139649090267008 learning.py:507] global step 994: loss = 0.3057 (3.018 sec/step)\n",
            "I0619 22:24:03.594392 139649090267008 learning.py:507] global step 994: loss = 0.2285 (3.065 sec/step)\n",
            "I0619 22:24:06.674768 139649090267008 learning.py:507] global step 994: loss = 0.2098 (3.079 sec/step)\n",
            "I0619 22:24:09.792302 139649090267008 learning.py:507] global step 994: loss = 0.3016 (3.116 sec/step)\n",
            "I0619 22:24:12.984277 139649090267008 learning.py:507] global step 994: loss = 0.2835 (3.190 sec/step)\n",
            "I0619 22:24:16.084633 139649090267008 learning.py:507] global step 994: loss = 0.2625 (3.099 sec/step)\n",
            "I0619 22:24:19.202944 139649090267008 learning.py:507] global step 995: loss = 0.3963 (3.116 sec/step)\n",
            "I0619 22:24:24.179072 139649090267008 learning.py:507] global step 995: loss = 0.1997 (4.963 sec/step)\n",
            "I0619 22:24:26.377434 139646017689344 supervisor.py:1050] Recording summary at step 995.\n",
            "I0619 22:24:27.871491 139649090267008 learning.py:507] global step 995: loss = 0.2263 (3.687 sec/step)\n",
            "I0619 22:24:31.087818 139649090267008 learning.py:507] global step 995: loss = 0.2232 (3.214 sec/step)\n",
            "I0619 22:24:34.196492 139649090267008 learning.py:507] global step 995: loss = 0.2242 (3.107 sec/step)\n",
            "I0619 22:24:37.300225 139649090267008 learning.py:507] global step 995: loss = 0.2176 (3.101 sec/step)\n",
            "I0619 22:24:40.363795 139649090267008 learning.py:507] global step 995: loss = 0.2494 (3.061 sec/step)\n",
            "I0619 22:24:43.435508 139649090267008 learning.py:507] global step 995: loss = 0.2624 (3.070 sec/step)\n",
            "I0619 22:24:46.554525 139649090267008 learning.py:507] global step 996: loss = 0.2588 (3.116 sec/step)\n",
            "I0619 22:24:49.634911 139649090267008 learning.py:507] global step 996: loss = 0.2382 (3.079 sec/step)\n",
            "I0619 22:24:52.680279 139649090267008 learning.py:507] global step 996: loss = 0.2329 (3.044 sec/step)\n",
            "I0619 22:24:55.798214 139649090267008 learning.py:507] global step 996: loss = 0.2355 (3.116 sec/step)\n",
            "I0619 22:24:58.889210 139649090267008 learning.py:507] global step 996: loss = 0.2265 (3.089 sec/step)\n",
            "I0619 22:25:01.922357 139649090267008 learning.py:507] global step 996: loss = 0.2369 (3.032 sec/step)\n",
            "I0619 22:25:05.124924 139649090267008 learning.py:507] global step 996: loss = 0.2391 (3.201 sec/step)\n",
            "I0619 22:25:08.202425 139649090267008 learning.py:507] global step 996: loss = 0.2595 (3.076 sec/step)\n",
            "I0619 22:25:11.226477 139649090267008 learning.py:507] global step 997: loss = 0.3782 (3.022 sec/step)\n",
            "I0619 22:25:14.305471 139649090267008 learning.py:507] global step 997: loss = 0.2556 (3.077 sec/step)\n",
            "I0619 22:25:17.429752 139649090267008 learning.py:507] global step 997: loss = 0.3049 (3.122 sec/step)\n",
            "I0619 22:25:20.554132 139649090267008 learning.py:507] global step 997: loss = 0.2225 (3.122 sec/step)\n",
            "I0619 22:25:23.626886 139649090267008 learning.py:507] global step 997: loss = 0.2275 (3.071 sec/step)\n",
            "I0619 22:25:26.770307 139649090267008 learning.py:507] global step 997: loss = 0.2001 (3.142 sec/step)\n",
            "I0619 22:25:29.820306 139649090267008 learning.py:507] global step 997: loss = 0.2875 (3.048 sec/step)\n",
            "I0619 22:25:32.987832 139649090267008 learning.py:507] global step 997: loss = 0.2796 (3.166 sec/step)\n",
            "I0619 22:25:36.046883 139649090267008 learning.py:507] global step 998: loss = 0.2208 (3.057 sec/step)\n",
            "I0619 22:25:39.183933 139649090267008 learning.py:507] global step 998: loss = 0.1952 (3.135 sec/step)\n",
            "I0619 22:25:42.327827 139649090267008 learning.py:507] global step 998: loss = 0.2171 (3.142 sec/step)\n",
            "I0619 22:25:45.419570 139649090267008 learning.py:507] global step 998: loss = 0.2917 (3.090 sec/step)\n",
            "I0619 22:25:48.637205 139649090267008 learning.py:507] global step 998: loss = 0.2017 (3.216 sec/step)\n",
            "I0619 22:25:51.779936 139649090267008 learning.py:507] global step 998: loss = 0.2374 (3.141 sec/step)\n",
            "I0619 22:25:54.913534 139649090267008 learning.py:507] global step 998: loss = 0.2215 (3.132 sec/step)\n",
            "I0619 22:25:58.090576 139649090267008 learning.py:507] global step 998: loss = 0.2932 (3.175 sec/step)\n",
            "I0619 22:26:01.189952 139649090267008 learning.py:507] global step 999: loss = 0.2660 (3.097 sec/step)\n",
            "I0619 22:26:04.248092 139649090267008 learning.py:507] global step 999: loss = 0.1936 (3.056 sec/step)\n",
            "I0619 22:26:07.398323 139649090267008 learning.py:507] global step 999: loss = 0.3332 (3.148 sec/step)\n",
            "I0619 22:26:10.476628 139649090267008 learning.py:507] global step 999: loss = 0.3322 (3.076 sec/step)\n",
            "I0619 22:26:13.549683 139649090267008 learning.py:507] global step 999: loss = 0.2381 (3.071 sec/step)\n",
            "I0619 22:26:16.736855 139649090267008 learning.py:507] global step 999: loss = 0.2660 (3.185 sec/step)\n",
            "I0619 22:26:19.885670 139649090267008 learning.py:507] global step 999: loss = 0.2472 (3.147 sec/step)\n",
            "I0619 22:26:25.062536 139649090267008 learning.py:507] global step 999: loss = 0.2843 (5.175 sec/step)\n",
            "I0619 22:26:26.461983 139646017689344 supervisor.py:1050] Recording summary at step 999.\n",
            "I0619 22:26:28.528434 139649090267008 learning.py:507] global step 1000: loss = 0.2165 (3.460 sec/step)\n",
            "I0619 22:26:31.639043 139649090267008 learning.py:507] global step 1000: loss = 0.2692 (3.109 sec/step)\n",
            "I0619 22:26:34.711171 139649090267008 learning.py:507] global step 1000: loss = 0.2802 (3.071 sec/step)\n",
            "I0619 22:26:37.810767 139649090267008 learning.py:507] global step 1000: loss = 0.2254 (3.098 sec/step)\n",
            "I0619 22:26:40.869756 139649090267008 learning.py:507] global step 1000: loss = 0.2403 (3.056 sec/step)\n",
            "I0619 22:26:43.892892 139649090267008 learning.py:507] global step 1000: loss = 0.3062 (3.021 sec/step)\n",
            "I0619 22:26:47.082670 139649090267008 learning.py:507] global step 1000: loss = 0.2134 (3.188 sec/step)\n",
            "I0619 22:26:50.204308 139649090267008 learning.py:507] global step 1000: loss = 0.2207 (3.120 sec/step)\n",
            "I0619 22:26:53.327751 139649090267008 learning.py:507] global step 1001: loss = 0.1931 (3.122 sec/step)\n",
            "I0619 22:26:56.535389 139649090267008 learning.py:507] global step 1001: loss = 0.2082 (3.206 sec/step)\n",
            "I0619 22:26:59.668534 139649090267008 learning.py:507] global step 1001: loss = 0.2513 (3.128 sec/step)\n",
            "I0619 22:27:02.788058 139649090267008 learning.py:507] global step 1001: loss = 0.3056 (3.117 sec/step)\n",
            "I0619 22:27:06.082769 139649090267008 learning.py:507] global step 1001: loss = 0.2838 (3.293 sec/step)\n",
            "I0619 22:27:09.089604 139649090267008 learning.py:507] global step 1001: loss = 0.2139 (3.005 sec/step)\n",
            "I0619 22:27:12.404553 139649090267008 learning.py:507] global step 1001: loss = 0.2428 (3.313 sec/step)\n",
            "I0619 22:27:15.466430 139649090267008 learning.py:507] global step 1001: loss = 0.3326 (3.060 sec/step)\n",
            "I0619 22:27:18.538370 139649090267008 learning.py:507] global step 1002: loss = 0.2503 (3.070 sec/step)\n",
            "I0619 22:27:21.640578 139649090267008 learning.py:507] global step 1002: loss = 0.2213 (3.100 sec/step)\n",
            "I0619 22:27:24.666604 139649090267008 learning.py:507] global step 1002: loss = 0.3171 (3.024 sec/step)\n",
            "I0619 22:27:27.723781 139649090267008 learning.py:507] global step 1002: loss = 0.2243 (3.055 sec/step)\n",
            "I0619 22:27:31.213596 139649090267008 learning.py:507] global step 1002: loss = 0.2891 (3.488 sec/step)\n",
            "I0619 22:27:34.283090 139649090267008 learning.py:507] global step 1002: loss = 0.2240 (3.067 sec/step)\n",
            "I0619 22:27:37.331473 139649090267008 learning.py:507] global step 1002: loss = 0.2823 (3.046 sec/step)\n",
            "I0619 22:27:40.435069 139649090267008 learning.py:507] global step 1002: loss = 0.1972 (3.102 sec/step)\n",
            "I0619 22:27:43.513109 139649090267008 learning.py:507] global step 1003: loss = 0.4304 (3.075 sec/step)\n",
            "I0619 22:27:46.618168 139649090267008 learning.py:507] global step 1003: loss = 0.2380 (3.103 sec/step)\n",
            "I0619 22:27:49.708544 139649090267008 learning.py:507] global step 1003: loss = 0.2212 (3.088 sec/step)\n",
            "I0619 22:27:52.781922 139649090267008 learning.py:507] global step 1003: loss = 0.3259 (3.071 sec/step)\n",
            "I0619 22:27:55.826179 139649090267008 learning.py:507] global step 1003: loss = 0.2308 (3.042 sec/step)\n",
            "I0619 22:27:58.922905 139649090267008 learning.py:507] global step 1003: loss = 0.2382 (3.095 sec/step)\n",
            "I0619 22:28:02.023451 139649090267008 learning.py:507] global step 1003: loss = 0.2223 (3.099 sec/step)\n",
            "I0619 22:28:05.119763 139649090267008 learning.py:507] global step 1003: loss = 0.2363 (3.094 sec/step)\n",
            "I0619 22:28:08.134913 139649090267008 learning.py:507] global step 1004: loss = 0.2340 (3.013 sec/step)\n",
            "I0619 22:28:11.192073 139649090267008 learning.py:507] global step 1004: loss = 0.4149 (3.055 sec/step)\n",
            "I0619 22:28:14.331552 139649090267008 learning.py:507] global step 1004: loss = 0.3750 (3.138 sec/step)\n",
            "I0619 22:28:17.679718 139649090267008 learning.py:507] global step 1004: loss = 0.2176 (3.346 sec/step)\n",
            "I0619 22:28:20.822514 139649090267008 learning.py:507] global step 1004: loss = 0.2151 (3.141 sec/step)\n",
            "I0619 22:28:26.173044 139649090267008 learning.py:507] global step 1004: loss = 0.2369 (5.347 sec/step)\n",
            "I0619 22:28:26.931458 139646017689344 supervisor.py:1050] Recording summary at step 1004.\n",
            "I0619 22:28:29.386006 139649090267008 learning.py:507] global step 1004: loss = 0.2262 (3.210 sec/step)\n",
            "I0619 22:28:32.455230 139649090267008 learning.py:507] global step 1004: loss = 0.2926 (3.067 sec/step)\n",
            "I0619 22:28:35.741572 139649090267008 learning.py:507] global step 1005: loss = 0.2253 (3.285 sec/step)\n",
            "I0619 22:28:38.838406 139649090267008 learning.py:507] global step 1005: loss = 0.1968 (3.095 sec/step)\n",
            "I0619 22:28:41.937232 139649090267008 learning.py:507] global step 1005: loss = 0.2462 (3.097 sec/step)\n",
            "I0619 22:28:45.045089 139649090267008 learning.py:507] global step 1005: loss = 0.2442 (3.106 sec/step)\n",
            "I0619 22:28:48.080058 139649090267008 learning.py:507] global step 1005: loss = 0.2230 (3.033 sec/step)\n",
            "I0619 22:28:51.179467 139649090267008 learning.py:507] global step 1005: loss = 0.2697 (3.098 sec/step)\n",
            "I0619 22:28:54.241420 139649090267008 learning.py:507] global step 1005: loss = 0.2202 (3.060 sec/step)\n",
            "I0619 22:28:57.280758 139649090267008 learning.py:507] global step 1005: loss = 0.3659 (3.038 sec/step)\n",
            "I0619 22:29:00.495694 139649090267008 learning.py:507] global step 1006: loss = 0.2174 (3.213 sec/step)\n",
            "I0619 22:29:03.564594 139649090267008 learning.py:507] global step 1006: loss = 0.2231 (3.067 sec/step)\n",
            "I0619 22:29:06.633929 139649090267008 learning.py:507] global step 1006: loss = 0.2037 (3.067 sec/step)\n",
            "I0619 22:29:09.704482 139649090267008 learning.py:507] global step 1006: loss = 0.2551 (3.068 sec/step)\n",
            "I0619 22:29:12.787668 139649090267008 learning.py:507] global step 1006: loss = 0.1822 (3.081 sec/step)\n",
            "I0619 22:29:15.885606 139649090267008 learning.py:507] global step 1006: loss = 0.2241 (3.096 sec/step)\n",
            "I0619 22:29:18.998549 139649090267008 learning.py:507] global step 1006: loss = 0.2044 (3.111 sec/step)\n",
            "I0619 22:29:22.051455 139649090267008 learning.py:507] global step 1006: loss = 0.2439 (3.051 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:29:25.139662 139649090267008 learning.py:507] global step 1007: loss = 0.2504 (3.086 sec/step)\n",
            "I0619 22:29:28.169783 139649090267008 learning.py:507] global step 1007: loss = 0.2185 (3.026 sec/step)\n",
            "I0619 22:29:31.386244 139649090267008 learning.py:507] global step 1007: loss = 0.1998 (3.215 sec/step)\n",
            "I0619 22:29:34.482113 139649090267008 learning.py:507] global step 1007: loss = 0.2257 (3.094 sec/step)\n",
            "I0619 22:29:37.576253 139649090267008 learning.py:507] global step 1007: loss = 0.2715 (3.092 sec/step)\n",
            "I0619 22:29:40.732693 139649090267008 learning.py:507] global step 1007: loss = 0.2277 (3.154 sec/step)\n",
            "I0619 22:29:43.795700 139649090267008 learning.py:507] global step 1007: loss = 0.2887 (3.061 sec/step)\n",
            "I0619 22:29:46.867938 139649090267008 learning.py:507] global step 1007: loss = 0.2177 (3.071 sec/step)\n",
            "I0619 22:29:50.113436 139649090267008 learning.py:507] global step 1008: loss = 0.2333 (3.243 sec/step)\n",
            "I0619 22:29:53.170509 139649090267008 learning.py:507] global step 1008: loss = 0.2049 (3.055 sec/step)\n",
            "I0619 22:29:56.288886 139649090267008 learning.py:507] global step 1008: loss = 0.2097 (3.116 sec/step)\n",
            "I0619 22:29:59.329108 139649090267008 learning.py:507] global step 1008: loss = 0.2716 (3.039 sec/step)\n",
            "I0619 22:30:02.420341 139649090267008 learning.py:507] global step 1008: loss = 0.2861 (3.089 sec/step)\n",
            "I0619 22:30:05.480686 139649090267008 learning.py:507] global step 1008: loss = 0.2861 (3.059 sec/step)\n",
            "I0619 22:30:08.531185 139649090267008 learning.py:507] global step 1008: loss = 0.3280 (3.049 sec/step)\n",
            "I0619 22:30:11.593226 139649090267008 learning.py:507] global step 1008: loss = 0.2205 (3.060 sec/step)\n",
            "I0619 22:30:14.663349 139649090267008 learning.py:507] global step 1009: loss = 0.3431 (3.068 sec/step)\n",
            "I0619 22:30:17.795625 139649090267008 learning.py:507] global step 1009: loss = 0.2552 (3.130 sec/step)\n",
            "I0619 22:30:20.920320 139649090267008 learning.py:507] global step 1009: loss = 0.2090 (3.123 sec/step)\n",
            "I0619 22:30:20.931262 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 22:30:28.161584 139649090267008 learning.py:507] global step 1009: loss = 0.3153 (7.239 sec/step)\n",
            "I0619 22:30:28.176800 139646017689344 supervisor.py:1050] Recording summary at step 1009.\n",
            "I0619 22:30:31.274131 139649090267008 learning.py:507] global step 1009: loss = 0.2260 (3.111 sec/step)\n",
            "I0619 22:30:34.292217 139649090267008 learning.py:507] global step 1009: loss = 0.2196 (3.016 sec/step)\n",
            "I0619 22:30:37.597896 139649090267008 learning.py:507] global step 1009: loss = 0.2600 (3.304 sec/step)\n",
            "I0619 22:30:40.697366 139649090267008 learning.py:507] global step 1009: loss = 0.2014 (3.098 sec/step)\n",
            "I0619 22:30:43.847399 139649090267008 learning.py:507] global step 1010: loss = 0.2784 (3.148 sec/step)\n",
            "I0619 22:30:46.934258 139649090267008 learning.py:507] global step 1010: loss = 0.1924 (3.085 sec/step)\n",
            "I0619 22:30:50.083456 139649090267008 learning.py:507] global step 1010: loss = 0.3277 (3.147 sec/step)\n",
            "I0619 22:30:53.149923 139649090267008 learning.py:507] global step 1010: loss = 0.2616 (3.064 sec/step)\n",
            "I0619 22:30:56.280672 139649090267008 learning.py:507] global step 1010: loss = 0.2405 (3.129 sec/step)\n",
            "I0619 22:30:59.675350 139649090267008 learning.py:507] global step 1010: loss = 0.2961 (3.393 sec/step)\n",
            "I0619 22:31:02.858907 139649090267008 learning.py:507] global step 1010: loss = 0.2290 (3.182 sec/step)\n",
            "I0619 22:31:05.960241 139649090267008 learning.py:507] global step 1010: loss = 0.3827 (3.099 sec/step)\n",
            "I0619 22:31:09.061251 139649090267008 learning.py:507] global step 1011: loss = 0.2084 (3.099 sec/step)\n",
            "I0619 22:31:12.143166 139649090267008 learning.py:507] global step 1011: loss = 0.2697 (3.080 sec/step)\n",
            "I0619 22:31:15.248629 139649090267008 learning.py:507] global step 1011: loss = 0.2926 (3.104 sec/step)\n",
            "I0619 22:31:18.818388 139649090267008 learning.py:507] global step 1011: loss = 0.2765 (3.568 sec/step)\n",
            "I0619 22:31:21.894730 139649090267008 learning.py:507] global step 1011: loss = 0.2547 (3.074 sec/step)\n",
            "I0619 22:31:24.983605 139649090267008 learning.py:507] global step 1011: loss = 0.2242 (3.087 sec/step)\n",
            "I0619 22:31:27.976075 139649090267008 learning.py:507] global step 1011: loss = 0.2835 (2.991 sec/step)\n",
            "I0619 22:31:31.098355 139649090267008 learning.py:507] global step 1011: loss = 0.2373 (3.120 sec/step)\n",
            "I0619 22:31:34.186481 139649090267008 learning.py:507] global step 1012: loss = 0.2090 (3.086 sec/step)\n",
            "I0619 22:31:37.172463 139649090267008 learning.py:507] global step 1012: loss = 0.2325 (2.984 sec/step)\n",
            "I0619 22:31:40.258783 139649090267008 learning.py:507] global step 1012: loss = 0.2857 (3.085 sec/step)\n",
            "I0619 22:31:43.329850 139649090267008 learning.py:507] global step 1012: loss = 0.2395 (3.069 sec/step)\n",
            "I0619 22:31:46.366012 139649090267008 learning.py:507] global step 1012: loss = 0.3052 (3.034 sec/step)\n",
            "I0619 22:31:49.439583 139649090267008 learning.py:507] global step 1012: loss = 0.2514 (3.072 sec/step)\n",
            "I0619 22:31:52.454012 139649090267008 learning.py:507] global step 1012: loss = 0.2253 (3.013 sec/step)\n",
            "I0619 22:31:55.564285 139649090267008 learning.py:507] global step 1012: loss = 0.3395 (3.108 sec/step)\n",
            "I0619 22:31:58.618192 139649090267008 learning.py:507] global step 1013: loss = 0.2075 (3.051 sec/step)\n",
            "I0619 22:32:01.720226 139649090267008 learning.py:507] global step 1013: loss = 0.2816 (3.100 sec/step)\n",
            "I0619 22:32:04.789226 139649090267008 learning.py:507] global step 1013: loss = 0.2876 (3.067 sec/step)\n",
            "I0619 22:32:07.902931 139649090267008 learning.py:507] global step 1013: loss = 0.2589 (3.112 sec/step)\n",
            "I0619 22:32:11.185661 139649090267008 learning.py:507] global step 1013: loss = 0.2754 (3.281 sec/step)\n",
            "I0619 22:32:14.339180 139649090267008 learning.py:507] global step 1013: loss = 0.2409 (3.151 sec/step)\n",
            "I0619 22:32:17.475501 139649090267008 learning.py:507] global step 1013: loss = 0.1910 (3.134 sec/step)\n",
            "I0619 22:32:20.609199 139649090267008 learning.py:507] global step 1013: loss = 0.2271 (3.132 sec/step)\n",
            "I0619 22:32:26.100688 139649090267008 learning.py:507] global step 1014: loss = 0.2535 (5.488 sec/step)\n",
            "I0619 22:32:26.104342 139646017689344 supervisor.py:1050] Recording summary at step 1014.\n",
            "I0619 22:32:29.300892 139649090267008 learning.py:507] global step 1014: loss = 0.2484 (3.198 sec/step)\n",
            "I0619 22:32:32.369312 139649090267008 learning.py:507] global step 1014: loss = 0.2229 (3.067 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:32:35.423471 139649090267008 learning.py:507] global step 1014: loss = 0.3070 (3.052 sec/step)\n",
            "I0619 22:32:38.532423 139649090267008 learning.py:507] global step 1014: loss = 0.2304 (3.107 sec/step)\n",
            "I0619 22:32:41.591092 139649090267008 learning.py:507] global step 1014: loss = 0.2148 (3.057 sec/step)\n",
            "I0619 22:32:44.713537 139649090267008 learning.py:507] global step 1014: loss = 0.2215 (3.121 sec/step)\n",
            "I0619 22:32:47.800752 139649090267008 learning.py:507] global step 1014: loss = 0.3531 (3.085 sec/step)\n",
            "I0619 22:32:50.878339 139649090267008 learning.py:507] global step 1015: loss = 0.3396 (3.075 sec/step)\n",
            "I0619 22:32:54.066896 139649090267008 learning.py:507] global step 1015: loss = 0.2180 (3.187 sec/step)\n",
            "I0619 22:32:57.120817 139649090267008 learning.py:507] global step 1015: loss = 0.2721 (3.052 sec/step)\n",
            "I0619 22:33:00.207235 139649090267008 learning.py:507] global step 1015: loss = 0.2629 (3.084 sec/step)\n",
            "I0619 22:33:03.319024 139649090267008 learning.py:507] global step 1015: loss = 0.1892 (3.110 sec/step)\n",
            "I0619 22:33:06.394716 139649090267008 learning.py:507] global step 1015: loss = 0.3062 (3.074 sec/step)\n",
            "I0619 22:33:09.464327 139649090267008 learning.py:507] global step 1015: loss = 0.2165 (3.068 sec/step)\n",
            "I0619 22:33:12.576613 139649090267008 learning.py:507] global step 1015: loss = 0.2393 (3.110 sec/step)\n",
            "I0619 22:33:15.649204 139649090267008 learning.py:507] global step 1016: loss = 0.2432 (3.070 sec/step)\n",
            "I0619 22:33:18.812478 139649090267008 learning.py:507] global step 1016: loss = 0.2143 (3.161 sec/step)\n",
            "I0619 22:33:21.878673 139649090267008 learning.py:507] global step 1016: loss = 0.3111 (3.064 sec/step)\n",
            "I0619 22:33:25.008835 139649090267008 learning.py:507] global step 1016: loss = 0.2794 (3.128 sec/step)\n",
            "I0619 22:33:28.052649 139649090267008 learning.py:507] global step 1016: loss = 0.2290 (3.042 sec/step)\n",
            "I0619 22:33:31.171324 139649090267008 learning.py:507] global step 1016: loss = 0.2498 (3.117 sec/step)\n",
            "I0619 22:33:34.394098 139649090267008 learning.py:507] global step 1016: loss = 0.2550 (3.221 sec/step)\n",
            "I0619 22:33:37.485001 139649090267008 learning.py:507] global step 1016: loss = 0.2552 (3.089 sec/step)\n",
            "I0619 22:33:40.622146 139649090267008 learning.py:507] global step 1017: loss = 0.2338 (3.131 sec/step)\n",
            "I0619 22:33:43.738591 139649090267008 learning.py:507] global step 1017: loss = 0.2178 (3.114 sec/step)\n",
            "I0619 22:33:46.843342 139649090267008 learning.py:507] global step 1017: loss = 0.2129 (3.103 sec/step)\n",
            "I0619 22:33:49.893044 139649090267008 learning.py:507] global step 1017: loss = 0.2526 (3.048 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:33:53.058944 139649090267008 learning.py:507] global step 1017: loss = 0.1987 (3.161 sec/step)\n",
            "I0619 22:33:56.139877 139649090267008 learning.py:507] global step 1017: loss = 0.2340 (3.079 sec/step)\n",
            "I0619 22:33:59.224140 139649090267008 learning.py:507] global step 1017: loss = 0.1888 (3.082 sec/step)\n",
            "I0619 22:34:02.337919 139649090267008 learning.py:507] global step 1017: loss = 0.2112 (3.112 sec/step)\n",
            "I0619 22:34:05.452691 139649090267008 learning.py:507] global step 1018: loss = 0.2771 (3.112 sec/step)\n",
            "I0619 22:34:08.578893 139649090267008 learning.py:507] global step 1018: loss = 0.2705 (3.124 sec/step)\n",
            "I0619 22:34:11.657390 139649090267008 learning.py:507] global step 1018: loss = 0.2535 (3.077 sec/step)\n",
            "I0619 22:34:14.724993 139649090267008 learning.py:507] global step 1018: loss = 0.2133 (3.066 sec/step)\n",
            "I0619 22:34:17.800380 139649090267008 learning.py:507] global step 1018: loss = 0.2154 (3.074 sec/step)\n",
            "I0619 22:34:20.873793 139649090267008 learning.py:507] global step 1018: loss = 0.3449 (3.072 sec/step)\n",
            "I0619 22:34:26.311666 139649090267008 learning.py:507] global step 1018: loss = 0.2248 (5.434 sec/step)\n",
            "I0619 22:34:26.968510 139646017689344 supervisor.py:1050] Recording summary at step 1018.\n",
            "I0619 22:34:29.576215 139649090267008 learning.py:507] global step 1018: loss = 0.2180 (3.262 sec/step)\n",
            "I0619 22:34:32.647115 139649090267008 learning.py:507] global step 1019: loss = 0.2558 (3.069 sec/step)\n",
            "I0619 22:34:35.744599 139649090267008 learning.py:507] global step 1019: loss = 0.2537 (3.095 sec/step)\n",
            "I0619 22:34:38.873463 139649090267008 learning.py:507] global step 1019: loss = 0.2513 (3.124 sec/step)\n",
            "I0619 22:34:41.971116 139649090267008 learning.py:507] global step 1019: loss = 0.2122 (3.096 sec/step)\n",
            "I0619 22:34:45.094345 139649090267008 learning.py:507] global step 1019: loss = 0.2149 (3.121 sec/step)\n",
            "I0619 22:34:48.122099 139649090267008 learning.py:507] global step 1019: loss = 0.2776 (3.026 sec/step)\n",
            "I0619 22:34:51.151470 139649090267008 learning.py:507] global step 1019: loss = 0.2040 (3.027 sec/step)\n",
            "I0619 22:34:54.360123 139649090267008 learning.py:507] global step 1019: loss = 0.2721 (3.207 sec/step)\n",
            "I0619 22:34:57.485563 139649090267008 learning.py:507] global step 1020: loss = 0.2127 (3.123 sec/step)\n",
            "I0619 22:35:00.696303 139649090267008 learning.py:507] global step 1020: loss = 0.2264 (3.209 sec/step)\n",
            "I0619 22:35:03.790366 139649090267008 learning.py:507] global step 1020: loss = 0.3655 (3.092 sec/step)\n",
            "I0619 22:35:07.007329 139649090267008 learning.py:507] global step 1020: loss = 0.3038 (3.215 sec/step)\n",
            "I0619 22:35:10.087367 139649090267008 learning.py:507] global step 1020: loss = 0.2799 (3.077 sec/step)\n",
            "I0619 22:35:13.128721 139649090267008 learning.py:507] global step 1020: loss = 0.2461 (3.039 sec/step)\n",
            "I0619 22:35:16.285399 139649090267008 learning.py:507] global step 1020: loss = 0.2866 (3.155 sec/step)\n",
            "I0619 22:35:19.532155 139649090267008 learning.py:507] global step 1020: loss = 0.2103 (3.244 sec/step)\n",
            "I0619 22:35:22.650912 139649090267008 learning.py:507] global step 1021: loss = 0.2678 (3.116 sec/step)\n",
            "I0619 22:35:25.795633 139649090267008 learning.py:507] global step 1021: loss = 0.2259 (3.143 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:35:28.928627 139649090267008 learning.py:507] global step 1021: loss = 0.2571 (3.131 sec/step)\n",
            "I0619 22:35:32.083423 139649090267008 learning.py:507] global step 1021: loss = 0.2168 (3.153 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:35:35.271981 139649090267008 learning.py:507] global step 1021: loss = 0.2888 (3.187 sec/step)\n",
            "I0619 22:35:38.386989 139649090267008 learning.py:507] global step 1021: loss = 0.3093 (3.113 sec/step)\n",
            "I0619 22:35:41.481032 139649090267008 learning.py:507] global step 1021: loss = 0.3238 (3.092 sec/step)\n",
            "I0619 22:35:44.563937 139649090267008 learning.py:507] global step 1021: loss = 0.2777 (3.081 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:35:48.004423 139649090267008 learning.py:507] global step 1022: loss = 0.2660 (3.438 sec/step)\n",
            "I0619 22:35:51.018349 139649090267008 learning.py:507] global step 1022: loss = 0.2271 (3.012 sec/step)\n",
            "I0619 22:35:54.061318 139649090267008 learning.py:507] global step 1022: loss = 0.2136 (3.041 sec/step)\n",
            "I0619 22:35:57.137492 139649090267008 learning.py:507] global step 1022: loss = 0.2062 (3.074 sec/step)\n",
            "I0619 22:36:00.225142 139649090267008 learning.py:507] global step 1022: loss = 0.2459 (3.086 sec/step)\n",
            "I0619 22:36:03.245275 139649090267008 learning.py:507] global step 1022: loss = 0.2250 (3.018 sec/step)\n",
            "I0619 22:36:06.763040 139649090267008 learning.py:507] global step 1022: loss = 0.1940 (3.516 sec/step)\n",
            "I0619 22:36:09.910823 139649090267008 learning.py:507] global step 1022: loss = 0.2907 (3.146 sec/step)\n",
            "I0619 22:36:12.965837 139649090267008 learning.py:507] global step 1023: loss = 0.2828 (3.052 sec/step)\n",
            "I0619 22:36:16.079942 139649090267008 learning.py:507] global step 1023: loss = 0.2278 (3.112 sec/step)\n",
            "I0619 22:36:19.213445 139649090267008 learning.py:507] global step 1023: loss = 0.2144 (3.131 sec/step)\n",
            "I0619 22:36:24.210261 139649090267008 learning.py:507] global step 1023: loss = 0.2397 (4.992 sec/step)\n",
            "I0619 22:36:26.392444 139646017689344 supervisor.py:1050] Recording summary at step 1023.\n",
            "I0619 22:36:27.928101 139649090267008 learning.py:507] global step 1023: loss = 0.2481 (3.715 sec/step)\n",
            "I0619 22:36:31.063489 139649090267008 learning.py:507] global step 1023: loss = 0.2013 (3.133 sec/step)\n",
            "I0619 22:36:34.116887 139649090267008 learning.py:507] global step 1023: loss = 0.3033 (3.052 sec/step)\n",
            "I0619 22:36:37.159982 139649090267008 learning.py:507] global step 1023: loss = 0.2294 (3.041 sec/step)\n",
            "I0619 22:36:40.518767 139649090267008 learning.py:507] global step 1024: loss = 0.2314 (3.357 sec/step)\n",
            "I0619 22:36:43.554892 139649090267008 learning.py:507] global step 1024: loss = 0.2312 (3.034 sec/step)\n",
            "I0619 22:36:46.624470 139649090267008 learning.py:507] global step 1024: loss = 0.2092 (3.068 sec/step)\n",
            "I0619 22:36:49.686634 139649090267008 learning.py:507] global step 1024: loss = 0.3016 (3.060 sec/step)\n",
            "I0619 22:36:52.823293 139649090267008 learning.py:507] global step 1024: loss = 0.2392 (3.135 sec/step)\n",
            "I0619 22:36:55.935133 139649090267008 learning.py:507] global step 1024: loss = 0.2825 (3.110 sec/step)\n",
            "I0619 22:36:59.037775 139649090267008 learning.py:507] global step 1024: loss = 0.2128 (3.101 sec/step)\n",
            "I0619 22:37:02.275325 139649090267008 learning.py:507] global step 1024: loss = 0.2234 (3.236 sec/step)\n",
            "I0619 22:37:05.333153 139649090267008 learning.py:507] global step 1025: loss = 0.2311 (3.056 sec/step)\n",
            "I0619 22:37:08.449954 139649090267008 learning.py:507] global step 1025: loss = 0.3074 (3.115 sec/step)\n",
            "I0619 22:37:11.549319 139649090267008 learning.py:507] global step 1025: loss = 0.2845 (3.098 sec/step)\n",
            "I0619 22:37:14.636493 139649090267008 learning.py:507] global step 1025: loss = 0.2380 (3.085 sec/step)\n",
            "I0619 22:37:17.784696 139649090267008 learning.py:507] global step 1025: loss = 0.2691 (3.146 sec/step)\n",
            "I0619 22:37:21.192990 139649090267008 learning.py:507] global step 1025: loss = 0.3066 (3.406 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:37:24.311080 139649090267008 learning.py:507] global step 1025: loss = 0.2327 (3.116 sec/step)\n",
            "I0619 22:37:27.456268 139649090267008 learning.py:507] global step 1025: loss = 0.2735 (3.143 sec/step)\n",
            "I0619 22:37:30.456392 139649090267008 learning.py:507] global step 1026: loss = 0.2495 (2.997 sec/step)\n",
            "I0619 22:37:33.546948 139649090267008 learning.py:507] global step 1026: loss = 0.2466 (3.089 sec/step)\n",
            "I0619 22:37:36.645763 139649090267008 learning.py:507] global step 1026: loss = 0.2807 (3.097 sec/step)\n",
            "I0619 22:37:39.727117 139649090267008 learning.py:507] global step 1026: loss = 0.2097 (3.080 sec/step)\n",
            "I0619 22:37:42.871062 139649090267008 learning.py:507] global step 1026: loss = 0.2166 (3.142 sec/step)\n",
            "I0619 22:37:45.965822 139649090267008 learning.py:507] global step 1026: loss = 0.2160 (3.093 sec/step)\n",
            "I0619 22:37:48.993731 139649090267008 learning.py:507] global step 1026: loss = 0.1957 (3.026 sec/step)\n",
            "I0619 22:37:52.097733 139649090267008 learning.py:507] global step 1026: loss = 0.2529 (3.102 sec/step)\n",
            "Corrupt JPEG data: premature end of data segment\n",
            "I0619 22:37:55.230131 139649090267008 learning.py:507] global step 1027: loss = 0.2062 (3.130 sec/step)\n",
            "I0619 22:37:58.246024 139649090267008 learning.py:507] global step 1027: loss = 0.2691 (3.014 sec/step)\n",
            "I0619 22:38:01.319648 139649090267008 learning.py:507] global step 1027: loss = 0.2324 (3.072 sec/step)\n",
            "I0619 22:38:04.468445 139649090267008 learning.py:507] global step 1027: loss = 0.2133 (3.147 sec/step)\n",
            "I0619 22:38:07.488292 139649090267008 learning.py:507] global step 1027: loss = 0.2051 (3.018 sec/step)\n",
            "I0619 22:38:10.725530 139649090267008 learning.py:507] global step 1027: loss = 0.2064 (3.236 sec/step)\n",
            "I0619 22:38:13.913428 139649090267008 learning.py:507] global step 1027: loss = 0.2872 (3.186 sec/step)\n",
            "I0619 22:38:17.019119 139649090267008 learning.py:507] global step 1027: loss = 0.2586 (3.104 sec/step)\n",
            "I0619 22:38:20.177843 139649090267008 learning.py:507] global step 1028: loss = 0.3167 (3.156 sec/step)\n",
            "I0619 22:38:25.534180 139649090267008 learning.py:507] global step 1028: loss = 0.2142 (5.349 sec/step)\n",
            "I0619 22:38:26.660162 139646017689344 supervisor.py:1050] Recording summary at step 1028.\n",
            "I0619 22:38:28.791350 139649090267008 learning.py:507] global step 1028: loss = 0.2781 (3.253 sec/step)\n",
            "I0619 22:38:31.891254 139649090267008 learning.py:507] global step 1028: loss = 0.2293 (3.098 sec/step)\n",
            "I0619 22:38:35.117738 139649090267008 learning.py:507] global step 1028: loss = 0.2418 (3.224 sec/step)\n",
            "I0619 22:38:38.189257 139649090267008 learning.py:507] global step 1028: loss = 0.2218 (3.070 sec/step)\n",
            "I0619 22:38:41.306395 139649090267008 learning.py:507] global step 1028: loss = 0.2530 (3.115 sec/step)\n",
            "I0619 22:38:44.474003 139649090267008 learning.py:507] global step 1028: loss = 0.2197 (3.166 sec/step)\n",
            "I0619 22:38:47.503578 139649090267008 learning.py:507] global step 1029: loss = 0.2634 (3.027 sec/step)\n",
            "I0619 22:38:50.603919 139649090267008 learning.py:507] global step 1029: loss = 0.2628 (3.098 sec/step)\n",
            "I0619 22:38:53.750088 139649090267008 learning.py:507] global step 1029: loss = 0.2450 (3.144 sec/step)\n",
            "I0619 22:38:56.871989 139649090267008 learning.py:507] global step 1029: loss = 0.2449 (3.120 sec/step)\n",
            "I0619 22:38:59.958473 139649090267008 learning.py:507] global step 1029: loss = 0.2450 (3.084 sec/step)\n",
            "I0619 22:39:03.238381 139649090267008 learning.py:507] global step 1029: loss = 0.2891 (3.278 sec/step)\n",
            "I0619 22:39:06.415998 139649090267008 learning.py:507] global step 1029: loss = 0.3177 (3.176 sec/step)\n",
            "I0619 22:39:09.467843 139649090267008 learning.py:507] global step 1029: loss = 0.2721 (3.050 sec/step)\n",
            "I0619 22:39:12.572621 139649090267008 learning.py:507] global step 1030: loss = 0.2340 (3.102 sec/step)\n",
            "I0619 22:39:15.652735 139649090267008 learning.py:507] global step 1030: loss = 0.2416 (3.078 sec/step)\n",
            "I0619 22:39:18.739553 139649090267008 learning.py:507] global step 1030: loss = 0.2353 (3.085 sec/step)\n",
            "I0619 22:39:21.912996 139649090267008 learning.py:507] global step 1030: loss = 0.3467 (3.171 sec/step)\n",
            "I0619 22:39:24.940538 139649090267008 learning.py:507] global step 1030: loss = 0.2350 (3.026 sec/step)\n",
            "I0619 22:39:28.029391 139649090267008 learning.py:507] global step 1030: loss = 0.2309 (3.087 sec/step)\n",
            "I0619 22:39:31.095696 139649090267008 learning.py:507] global step 1030: loss = 0.2357 (3.065 sec/step)\n",
            "I0619 22:39:34.170434 139649090267008 learning.py:507] global step 1030: loss = 0.2545 (3.073 sec/step)\n",
            "I0619 22:39:37.225581 139649090267008 learning.py:507] global step 1031: loss = 0.2591 (3.053 sec/step)\n",
            "I0619 22:39:40.250262 139649090267008 learning.py:507] global step 1031: loss = 0.2382 (3.023 sec/step)\n",
            "I0619 22:39:43.427384 139649090267008 learning.py:507] global step 1031: loss = 0.2729 (3.175 sec/step)\n",
            "I0619 22:39:46.501218 139649090267008 learning.py:507] global step 1031: loss = 0.2122 (3.071 sec/step)\n",
            "I0619 22:39:49.629013 139649090267008 learning.py:507] global step 1031: loss = 0.2084 (3.126 sec/step)\n",
            "I0619 22:39:52.665081 139649090267008 learning.py:507] global step 1031: loss = 0.2355 (3.034 sec/step)\n",
            "I0619 22:39:55.765077 139649090267008 learning.py:507] global step 1031: loss = 0.2033 (3.098 sec/step)\n",
            "I0619 22:39:58.832224 139649090267008 learning.py:507] global step 1031: loss = 0.2328 (3.065 sec/step)\n",
            "I0619 22:40:02.061708 139649090267008 learning.py:507] global step 1032: loss = 0.2393 (3.227 sec/step)\n",
            "I0619 22:40:05.116921 139649090267008 learning.py:507] global step 1032: loss = 0.3109 (3.053 sec/step)\n",
            "I0619 22:40:08.155925 139649090267008 learning.py:507] global step 1032: loss = 0.2305 (3.037 sec/step)\n",
            "I0619 22:40:11.256070 139649090267008 learning.py:507] global step 1032: loss = 0.3775 (3.098 sec/step)\n",
            "I0619 22:40:14.347151 139649090267008 learning.py:507] global step 1032: loss = 0.3237 (3.089 sec/step)\n",
            "I0619 22:40:17.460894 139649090267008 learning.py:507] global step 1032: loss = 0.2496 (3.112 sec/step)\n",
            "I0619 22:40:20.564189 139649090267008 learning.py:507] global step 1032: loss = 0.2351 (3.101 sec/step)\n",
            "I0619 22:40:20.931661 139646034474752 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0619 22:40:27.532694 139649090267008 learning.py:507] global step 1032: loss = 0.2357 (6.966 sec/step)\n",
            "I0619 22:40:28.355943 139646017689344 supervisor.py:1050] Recording summary at step 1032.\n",
            "I0619 22:40:30.784699 139649090267008 learning.py:507] global step 1033: loss = 0.3163 (3.249 sec/step)\n",
            "I0619 22:40:33.960389 139649090267008 learning.py:507] global step 1033: loss = 0.2034 (3.174 sec/step)\n",
            "I0619 22:40:37.146406 139649090267008 learning.py:507] global step 1033: loss = 0.2926 (3.184 sec/step)\n",
            "I0619 22:40:40.391166 139649090267008 learning.py:507] global step 1033: loss = 0.3332 (3.243 sec/step)\n",
            "I0619 22:40:43.511388 139649090267008 learning.py:507] global step 1033: loss = 0.2803 (3.118 sec/step)\n",
            "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
            "I0619 22:40:46.670715 139649090267008 learning.py:507] global step 1033: loss = 0.2976 (3.157 sec/step)\n",
            "I0619 22:40:49.759578 139649090267008 learning.py:507] global step 1033: loss = 0.2156 (3.087 sec/step)\n",
            "I0619 22:40:52.843713 139649090267008 learning.py:507] global step 1033: loss = 0.2100 (3.082 sec/step)\n",
            "I0619 22:40:55.956590 139649090267008 learning.py:507] global step 1034: loss = 0.2695 (3.111 sec/step)\n",
            "I0619 22:40:59.241733 139649090267008 learning.py:507] global step 1034: loss = 0.2491 (3.283 sec/step)\n",
            "I0619 22:41:02.446295 139649090267008 learning.py:507] global step 1034: loss = 0.3230 (3.202 sec/step)\n",
            "I0619 22:41:05.495280 139649090267008 learning.py:507] global step 1034: loss = 0.2319 (3.047 sec/step)\n",
            "I0619 22:41:08.601914 139649090267008 learning.py:507] global step 1034: loss = 0.2450 (3.105 sec/step)\n",
            "I0619 22:41:11.720715 139649090267008 learning.py:507] global step 1034: loss = 0.2149 (3.117 sec/step)\n",
            "I0619 22:41:14.831505 139649090267008 learning.py:507] global step 1034: loss = 0.2226 (3.109 sec/step)\n",
            "I0619 22:41:18.202729 139649090267008 learning.py:507] global step 1034: loss = 0.2164 (3.369 sec/step)\n",
            "I0619 22:41:21.340752 139649090267008 learning.py:507] global step 1035: loss = 0.3201 (3.136 sec/step)\n",
            "I0619 22:41:24.400866 139649090267008 learning.py:507] global step 1035: loss = 0.2234 (3.058 sec/step)\n",
            "I0619 22:41:27.463701 139649090267008 learning.py:507] global step 1035: loss = 0.2265 (3.061 sec/step)\n",
            "I0619 22:41:30.519394 139649090267008 learning.py:507] global step 1035: loss = 0.2011 (3.054 sec/step)\n",
            "I0619 22:41:33.597984 139649090267008 learning.py:507] global step 1035: loss = 0.2784 (3.077 sec/step)\n",
            "I0619 22:41:36.813138 139649090267008 learning.py:507] global step 1035: loss = 0.2305 (3.213 sec/step)\n",
            "I0619 22:41:39.893325 139649090267008 learning.py:507] global step 1035: loss = 0.2404 (3.078 sec/step)\n",
            "I0619 22:41:42.938189 139649090267008 learning.py:507] global step 1035: loss = 0.2582 (3.042 sec/step)\n",
            "I0619 22:41:46.032515 139649090267008 learning.py:507] global step 1036: loss = 0.2341 (3.091 sec/step)\n",
            "I0619 22:41:49.131915 139649090267008 learning.py:507] global step 1036: loss = 0.3337 (3.098 sec/step)\n",
            "I0619 22:41:52.222879 139649090267008 learning.py:507] global step 1036: loss = 0.2282 (3.089 sec/step)\n",
            "I0619 22:41:55.350198 139649090267008 learning.py:507] global step 1036: loss = 0.2786 (3.126 sec/step)\n",
            "I0619 22:41:58.449607 139649090267008 learning.py:507] global step 1036: loss = 0.2555 (3.098 sec/step)\n",
            "I0619 22:42:01.590511 139649090267008 learning.py:507] global step 1036: loss = 0.2108 (3.139 sec/step)\n",
            "I0619 22:42:04.586598 139649090267008 learning.py:507] global step 1036: loss = 0.2204 (2.994 sec/step)\n",
            "I0619 22:42:07.645711 139649090267008 learning.py:507] global step 1036: loss = 0.2015 (3.057 sec/step)\n",
            "I0619 22:42:10.740139 139649090267008 learning.py:507] global step 1037: loss = 0.2497 (3.092 sec/step)\n",
            "I0619 22:42:13.823898 139649090267008 learning.py:507] global step 1037: loss = 0.2319 (3.082 sec/step)\n",
            "I0619 22:42:16.883672 139649090267008 learning.py:507] global step 1037: loss = 0.2304 (3.058 sec/step)\n",
            "I0619 22:42:20.016931 139649090267008 learning.py:507] global step 1037: loss = 0.3354 (3.131 sec/step)\n",
            "I0619 22:42:25.279692 139649090267008 learning.py:507] global step 1037: loss = 0.2065 (5.255 sec/step)\n",
            "I0619 22:42:26.271212 139646017689344 supervisor.py:1050] Recording summary at step 1037.\n",
            "I0619 22:42:28.630050 139649090267008 learning.py:507] global step 1037: loss = 0.2368 (3.348 sec/step)\n",
            "I0619 22:42:31.862494 139649090267008 learning.py:507] global step 1037: loss = 0.2339 (3.231 sec/step)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crdbDqlAWaip",
        "colab_type": "code",
        "outputId": "f2a71943-52cf-4a8f-d3c7-6ca9567d8c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "%ls -a training/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/                                           model.ckpt-938.data-00000-of-00001\n",
            "\u001b[01;34m..\u001b[0m/                                          model.ckpt-938.index\n",
            "checkpoint                                   model.ckpt-938.meta\n",
            "events.out.tfevents.1560958220.5e5e0d53e5c9  model.ckpt-962.data-00000-of-00001\n",
            "graph.pbtxt                                  model.ckpt-962.index\n",
            "model.ckpt-1009.data-00000-of-00001          model.ckpt-962.meta\n",
            "model.ckpt-1009.index                        model.ckpt-985.data-00000-of-00001\n",
            "model.ckpt-1009.meta                         model.ckpt-985.index\n",
            "model.ckpt-1032.data-00000-of-00001          model.ckpt-985.meta\n",
            "model.ckpt-1032.index                        pipeline.config\n",
            "model.ckpt-1032.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8EscylA6SqX",
        "colab_type": "code",
        "outputId": "9c80ab8c-8f88-45bc-fbe9-4184e07be6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python export_tflite_ssd_graph.py \\\n",
        "--pipeline_config_path=samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config \\\n",
        "--trained_checkpoint_prefix=training/model.ckpt-1032 \\\n",
        "--output_directory=tflite_graph \\\n",
        "--add_postprocessing_op=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'export_tflite_ssd_graph.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwTtzUyc6o07",
        "colab_type": "code",
        "outputId": "ae80695e-7367-4e90-cf68-3ea486a7eaed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls -a tflite_graph/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  tflite_graph.pb  tflite_graph.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttq98ROF_430",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r tflite_graph.zip tflite_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB6wxySR7cR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"tflite_graph.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bovxh6RB7Qyz",
        "colab_type": "code",
        "outputId": "03524a2c-4019-4d35-ad63-720e674881ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12133
        }
      },
      "source": [
        "!python export_inference_graph.py \\\n",
        "--pipeline_config_path=samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config \\\n",
        "--trained_checkpoint_prefix=training/model.ckpt-1032 \\\n",
        "--output_directory=inf_graph \\\n",
        "--add_postprocessing_op=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 22:48:53.179795 139683813046144 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0619 22:48:53.190540 139683813046144 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0619 22:48:53.201375 139683813046144 deprecation_wrapper.py:119] From export_inference_graph.py:156: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0619 22:48:53.202026 139683813046144 deprecation_wrapper.py:119] From export_inference_graph.py:139: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0619 22:48:53.208605 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:367: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0619 22:48:53.208906 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:110: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 22:48:53.244663 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2515: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0619 22:48:53.409043 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:566: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0619 22:48:55.435981 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/anchor_generator.py:149: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0619 22:48:55.449105 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0619 22:48:55.449284 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 22:48:55.494360 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 22:48:55.537578 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 22:48:55.582417 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 22:48:55.626574 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0619 22:48:55.672942 139683813046144 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0619 22:48:55.943550 139683813046144 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0619 22:48:57.636280 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:246: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0619 22:48:57.636593 139683813046144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:348: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "I0619 22:48:58.934019 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/add_fold\n",
            "I0619 22:48:58.934436 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/add_fold\n",
            "I0619 22:48:58.934644 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/add_fold\n",
            "I0619 22:48:58.934843 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/add_fold\n",
            "I0619 22:48:58.935044 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/add_fold\n",
            "I0619 22:48:58.935254 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/add_fold\n",
            "I0619 22:48:58.935446 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/add_fold\n",
            "I0619 22:48:58.935650 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/add_fold\n",
            "I0619 22:48:58.935848 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/add_fold\n",
            "I0619 22:48:58.936071 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/add_fold\n",
            "I0619 22:48:58.936259 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/add_fold\n",
            "I0619 22:48:58.936451 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/add_fold\n",
            "I0619 22:48:58.936629 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/add_fold\n",
            "I0619 22:48:58.936824 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/add_fold\n",
            "I0619 22:48:58.937029 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/add_fold\n",
            "I0619 22:48:58.937244 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/add_fold\n",
            "I0619 22:48:58.937424 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/add_fold\n",
            "I0619 22:48:58.937611 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/add_fold\n",
            "I0619 22:48:58.937786 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/add_fold\n",
            "I0619 22:48:58.937983 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/add_fold\n",
            "I0619 22:48:58.938168 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/add_fold\n",
            "I0619 22:48:58.938383 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/add_fold\n",
            "I0619 22:48:58.938575 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/add_fold\n",
            "I0619 22:48:58.938782 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/add_fold\n",
            "I0619 22:48:58.938987 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/add_fold\n",
            "I0619 22:48:58.939201 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/add_fold\n",
            "I0619 22:48:58.939381 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/add_fold\n",
            "I0619 22:48:58.939550 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/add_fold\n",
            "I0619 22:48:58.939729 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/add_fold\n",
            "I0619 22:48:58.939904 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/add_fold\n",
            "I0619 22:48:58.940097 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/add_fold\n",
            "I0619 22:48:58.940297 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/add_fold\n",
            "I0619 22:48:58.940493 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/add_fold\n",
            "I0619 22:48:58.940685 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/add_fold\n",
            "I0619 22:48:58.940876 139683813046144 quantize.py:298] Skipping quant after FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/add_fold\n",
            "W0619 22:48:58.943584 139683813046144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:504: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0619 22:48:58.944673 139683813046144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "289 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/3.51m params)\n",
            "  BoxPredictor_0 (--/48.51k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/4.62k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x384x12, 4.61k/4.61k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/43.89k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (114, 114/114 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x384x114, 43.78k/43.78k params)\n",
            "  BoxPredictor_1 (--/193.79k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/18.46k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x768x24, 18.43k/18.43k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/175.33k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (228, 228/228 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x768x228, 175.10k/175.10k params)\n",
            "  BoxPredictor_2 (--/97.02k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/9.24k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x384x24, 9.22k/9.22k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/87.78k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (228, 228/228 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x384x228, 87.55k/87.55k params)\n",
            "  BoxPredictor_3 (--/48.64k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/4.63k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x192x24, 4.61k/4.61k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/44.00k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (228, 228/228 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x192x228, 43.78k/43.78k params)\n",
            "  BoxPredictor_4 (--/48.64k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/4.63k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x192x24, 4.61k/4.61k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/44.00k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (228, 228/228 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x192x228, 43.78k/43.78k params)\n",
            "  BoxPredictor_5 (--/24.44k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/2.33k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/22.12k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (228, 228/228 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x96x228, 21.89k/21.89k params)\n",
            "  FeatureExtractor (--/3.05m params)\n",
            "    FeatureExtractor/MobilenetV1 (--/3.05m params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_0 (--/648 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_0/weights (3x3x3x24, 648/648 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_10_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_10_pointwise (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights (1x1x384x384, 147.46k/147.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_11_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_11_pointwise (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights (1x1x384x384, 147.46k/147.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_12_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_12_pointwise (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights (1x1x384x768, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_depthwise (--/6.91k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights (3x3x768x1, 6.91k/6.91k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise (--/589.82k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights (1x1x768x768, 589.82k/589.82k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192 (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/weights (1x1x768x192, 147.46k/147.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96 (--/36.86k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96 (--/18.43k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/weights (1x1x192x96, 18.43k/18.43k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48 (--/9.22k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/weights (1x1x192x48, 9.22k/9.22k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384 (--/663.55k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/weights (3x3x192x384, 663.55k/663.55k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192 (--/165.89k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/weights (3x3x96x192, 165.89k/165.89k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192 (--/165.89k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/weights (3x3x96x192, 165.89k/165.89k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96 (--/41.47k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/weights (3x3x48x96, 41.47k/41.47k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_1_depthwise (--/216 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights (3x3x24x1, 216/216 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_1_pointwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights (1x1x24x48, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_2_depthwise (--/432 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights (3x3x48x1, 432/432 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_2_pointwise (--/4.61k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights (1x1x48x96, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_3_depthwise (--/864 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_3_pointwise (--/9.22k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights (1x1x96x96, 9.22k/9.22k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_4_depthwise (--/864 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_4_pointwise (--/18.43k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights (1x1x96x192, 18.43k/18.43k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_5_depthwise (--/1.73k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_5_pointwise (--/36.86k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights (1x1x192x192, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_6_depthwise (--/1.73k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_6_pointwise (--/73.73k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights (1x1x192x384, 73.73k/73.73k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_7_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_7_pointwise (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights (1x1x384x384, 147.46k/147.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_8_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_8_pointwise (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights (1x1x384x384, 147.46k/147.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_9_depthwise (--/3.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV1/Conv2d_9_pointwise (--/147.46k params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights (1x1x384x384, 147.46k/147.46k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "289 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/3.07m flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_384/mul_fold (663.55k/663.55k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/mul_fold (589.82k/589.82k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/mul_fold (294.91k/294.91k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_192/mul_fold (165.89k/165.89k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_192/mul_fold (165.89k/165.89k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_192/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/mul_fold (147.46k/147.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/mul_fold (73.73k/73.73k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_96/mul_fold (41.47k/41.47k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_96/mul_fold (36.86k/36.86k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/mul_fold (36.86k/36.86k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/mul_fold (18.43k/18.43k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_96/mul_fold (18.43k/18.43k flops)\n",
            "  FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_48/mul_fold (9.22k/9.22k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/mul_fold (9.22k/9.22k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/mul_fold (6.91k/6.91k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/mul_fold (4.61k/4.61k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/mul_fold (3.46k/3.46k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/mul_fold (1.73k/1.73k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/mul_fold (1.73k/1.73k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/mul_fold (1.15k/1.15k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/mul_fold (864/864 flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/mul_fold (864/864 flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/mul_fold (648/648 flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/mul_fold (432/432 flops)\n",
            "  MultipleGridAnchorGenerator/add_8 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/mul_fold (216/216 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/add_11 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/add (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_1 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_3 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_4 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_7 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_6 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_9 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_10 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_29 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_20 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_21 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_22 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_23 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_24 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_25 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_26 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_27 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_28 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_32 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_56 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_34 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_36 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_38 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_40 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_42 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_44 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_46 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_48 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_50 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_52 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_54 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_58 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_60 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_62 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_64 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_66 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_68 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_70 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_72 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_30 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_31 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_32 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_33 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_34 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_35 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_36 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_35 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_36 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_37 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_34 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_20 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_22 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_24 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_26 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_28 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_23 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_20 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_21 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_22 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add_30 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_24 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_25 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_26 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_27 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_28 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_29 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_30 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_31 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_32 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_33 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "W0619 22:49:00.670408 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:397: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-06-19 22:49:02.071050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-19 22:49:02.114759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.115224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-19 22:49:02.115536: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 22:49:02.117072: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-19 22:49:02.118467: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-19 22:49:02.118866: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-19 22:49:02.120663: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-19 22:49:02.122090: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-19 22:49:02.126294: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-19 22:49:02.126432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.126845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.127213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-19 22:49:02.133767: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-19 22:49:02.133983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d15a40 executing computations on platform Host. Devices:\n",
            "2019-06-19 22:49:02.134011: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-19 22:49:02.282000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.282554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d164c0 executing computations on platform CUDA. Devices:\n",
            "2019-06-19 22:49:02.282583: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-19 22:49:02.282832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.283218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-19 22:49:02.283301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 22:49:02.283324: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-19 22:49:02.283346: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-19 22:49:02.283367: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-19 22:49:02.283391: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-19 22:49:02.283410: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-19 22:49:02.283432: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-19 22:49:02.283508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.283878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.284266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-19 22:49:02.284353: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 22:49:02.285393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-19 22:49:02.285420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-19 22:49:02.285440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-19 22:49:02.285742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.286150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:02.286500: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-19 22:49:02.286551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0619 22:49:02.287452 139683813046144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0619 22:49:02.288825 139683813046144 saver.py:1280] Restoring parameters from training/model.ckpt-1032\n",
            "2019-06-19 22:49:05.925907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:05.926458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-19 22:49:05.926582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 22:49:05.926607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-19 22:49:05.926628: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-19 22:49:05.926652: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-19 22:49:05.926678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-19 22:49:05.926699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-19 22:49:05.926720: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-19 22:49:05.926819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:05.927279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:05.927601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-19 22:49:05.927653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-19 22:49:05.927667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-19 22:49:05.927678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-19 22:49:05.927935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:05.928330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:05.928668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0619 22:49:05.930308 139683813046144 saver.py:1280] Restoring parameters from training/model.ckpt-1032\n",
            "W0619 22:49:06.795261 139683813046144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0619 22:49:06.795575 139683813046144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "I0619 22:49:07.279481 139683813046144 graph_util_impl.py:311] Froze 387 variables.\n",
            "I0619 22:49:07.386746 139683813046144 graph_util_impl.py:364] Converted 387 variables to const ops.\n",
            "2019-06-19 22:49:07.608586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:07.609079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-19 22:49:07.609188: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-19 22:49:07.609215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-19 22:49:07.609237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-19 22:49:07.609258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-19 22:49:07.609279: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-19 22:49:07.609298: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-19 22:49:07.609319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-19 22:49:07.609413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:07.609793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:07.610124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-19 22:49:07.610175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-19 22:49:07.610191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-19 22:49:07.610202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-19 22:49:07.610469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:07.610849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-19 22:49:07.611208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0619 22:49:08.267854 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:274: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0619 22:49:08.268439 139683813046144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:277: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0619 22:49:08.269008 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:283: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0619 22:49:08.269174 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:286: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0619 22:49:08.269379 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:291: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0619 22:49:08.269496 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:293: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0619 22:49:08.269803 139683813046144 builder_impl.py:636] No assets to save.\n",
            "I0619 22:49:08.269880 139683813046144 builder_impl.py:456] No assets to write.\n",
            "I0619 22:49:08.717237 139683813046144 builder_impl.py:421] SavedModel written to: inf_graph/saved_model/saved_model.pb\n",
            "W0619 22:49:08.774128 139683813046144 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:184: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0619 22:49:08.774395 139683813046144 config_util.py:186] Writing pipeline config file to inf_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QA5nlAU_WGe",
        "colab_type": "code",
        "outputId": "71fb7d5f-2fba-4d78-d55e-ef1d306f1cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%ls -a inf_graph/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/          frozen_inference_graph.pb       model.ckpt.meta\n",
            "\u001b[01;34m..\u001b[0m/         model.ckpt.data-00000-of-00001  pipeline.config\n",
            "checkpoint  model.ckpt.index                \u001b[01;34msaved_model\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygJPCdx9_bZA",
        "colab_type": "code",
        "outputId": "56d6c77b-0a20-4a0f-cba1-f7f9dfb5c1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!zip -r inf_graph.zip inf_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: inf_graph/ (stored 0%)\n",
            "  adding: inf_graph/saved_model/ (stored 0%)\n",
            "  adding: inf_graph/saved_model/saved_model.pb (deflated 14%)\n",
            "  adding: inf_graph/saved_model/variables/ (stored 0%)\n",
            "  adding: inf_graph/checkpoint (deflated 42%)\n",
            "  adding: inf_graph/model.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: inf_graph/pipeline.config (deflated 68%)\n",
            "  adding: inf_graph/frozen_inference_graph.pb (deflated 13%)\n",
            "  adding: inf_graph/model.ckpt.index (deflated 68%)\n",
            "  adding: inf_graph/model.ckpt.meta (deflated 95%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1waD2Pf7aE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"inf_graph.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58StD2C4_J3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}